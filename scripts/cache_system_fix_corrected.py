#!/usr/bin/env python3
"""
Estia-AI ç¼“å­˜ç³»ç»Ÿä¿®å¤æ–¹æ¡ˆ - ä¿®æ­£ç‰ˆ
åŸºäºå®é™…æ–‡ä»¶ç»“æ„è¿›è¡Œä¿®å¤

ä¿®å¤ä¼˜å…ˆçº§ï¼š
1. ã€é«˜ã€‘å…³é”®è¯ç¼“å­˜åŠŸèƒ½æ¢å¤
2. ã€é«˜ã€‘UnifiedCacheManagerå˜é‡ä½œç”¨åŸŸä¿®å¤
3. ã€ä¸­ã€‘é›†æˆæ·±åº¦å¢å¼º
4. ã€ä½ã€‘ç¼“å­˜æ¸…ç†æ–¹æ³•è¡¥å…¨
"""

import sys
import os
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '.')))

def create_keyword_cache_implementation():
    """
    ä¿®å¤1: å…³é”®è¯ç¼“å­˜åŠŸèƒ½æ¢å¤
    åœ¨æ­£ç¡®çš„ç›®å½•åˆ›å»ºå…³é”®è¯ç¼“å­˜å®ç°
    """
    print("ğŸ”§ ä¿®å¤1: å…³é”®è¯ç¼“å­˜åŠŸèƒ½æ¢å¤")
    print("=" * 60)
    
    # å…³é”®è¯ç¼“å­˜å®ç°ä»£ç 
    keyword_cache_code = '''"""
å…³é”®è¯ç¼“å­˜ç³»ç»Ÿ
åŸºäºæ—§ç³»ç»Ÿ core/old_memory/embedding/cache.py çš„å®ç°
"""

import re
import threading
import time
from typing import Dict, List, Set, Any, Optional
from collections import defaultdict

class KeywordCache:
    """
    å…³é”®è¯ç¼“å­˜ç³»ç»Ÿ
    æä¾›åŸºäºå…³é”®è¯çš„å¿«é€Ÿå†…å®¹æ£€ç´¢åŠŸèƒ½
    """
    
    def __init__(self, max_keywords: int = 10000):
        """åˆå§‹åŒ–å…³é”®è¯ç¼“å­˜"""
        self.max_keywords = max_keywords
        self.keyword_cache: Dict[str, Set[str]] = defaultdict(set)
        self.keyword_metadata: Dict[str, Dict[str, Any]] = {}
        self.lock = threading.RLock()
        
        # ä¸­æ–‡åœç”¨è¯
        self.stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹',
            'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä»€ä¹ˆ', 'å¯ä»¥', 'è¿™ä¸ª', 'è¿˜', 'æ—¶å€™', 'å¦‚æœ'
        }
        
        # è‹±æ–‡åœç”¨è¯
        self.stop_words.update({
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'this', 'that', 'is', 'are', 'was', 'were', 'be',
            'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',
            'would', 'should', 'could', 'can', 'may', 'might', 'must', 'shall'
        })
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        æå–å…³é”®è¯
        åŸºäºæ—§ç³»ç»Ÿçš„å®ç°ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··åˆæ–‡æœ¬
        """
        if not text:
            return []
            
        # æå–ä¸­æ–‡å’Œè‹±æ–‡è¯æ±‡
        words = re.findall(r'[\\w\\u4e00-\\u9fff]+', text.lower())
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        keywords = []
        for word in words:
            if (len(word) > 1 and 
                word not in self.stop_words and 
                not word.isdigit()):
                keywords.append(word)
        
        # é™åˆ¶å…³é”®è¯æ•°é‡
        return keywords[:10]
    
    def add_to_keyword_cache(self, cache_key: str, text: str, weight: float = 1.0):
        """
        æ·»åŠ åˆ°å…³é”®è¯ç¼“å­˜
        
        Args:
            cache_key: ç¼“å­˜é”®
            text: æ–‡æœ¬å†…å®¹
            weight: æƒé‡
        """
        with self.lock:
            keywords = self._extract_keywords(text)
            
            for keyword in keywords:
                # æ·»åŠ åˆ°å…³é”®è¯æ˜ å°„
                self.keyword_cache[keyword].add(cache_key)
                
                # æ›´æ–°å…ƒæ•°æ®
                if keyword not in self.keyword_metadata:
                    self.keyword_metadata[keyword] = {
                        'count': 0,
                        'weight': 0.0,
                        'last_updated': None
                    }
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.keyword_metadata[keyword]['count'] += 1
                self.keyword_metadata[keyword]['weight'] = max(
                    self.keyword_metadata[keyword]['weight'], weight
                )
                self.keyword_metadata[keyword]['last_updated'] = time.time()
    
    def search_by_keywords(self, query: str, limit: int = 10) -> List[str]:
        """
        åŸºäºå…³é”®è¯æœç´¢ç¼“å­˜é¡¹
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            ç›¸å…³çš„ç¼“å­˜é”®åˆ—è¡¨
        """
        with self.lock:
            keywords = self._extract_keywords(query)
            
            if not keywords:
                return []
            
            # æ”¶é›†å€™é€‰é¡¹
            candidates = defaultdict(float)
            
            for keyword in keywords:
                if keyword in self.keyword_cache:
                    # è·å–åŒ…å«æ­¤å…³é”®è¯çš„ç¼“å­˜é¡¹
                    cache_keys = self.keyword_cache[keyword]
                    keyword_weight = self.keyword_metadata[keyword]['weight']
                    
                    for cache_key in cache_keys:
                        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
                        candidates[cache_key] += keyword_weight
            
            # æŒ‰åˆ†æ•°æ’åº
            sorted_candidates = sorted(
                candidates.items(), 
                key=lambda x: x[1], 
                reverse=True
            )
            
            return [cache_key for cache_key, score in sorted_candidates[:limit]]
    
    def remove_from_keyword_cache(self, cache_key: str):
        """
        ä»å…³é”®è¯ç¼“å­˜ä¸­ç§»é™¤é¡¹
        
        Args:
            cache_key: è¦ç§»é™¤çš„ç¼“å­˜é”®
        """
        with self.lock:
            # æŸ¥æ‰¾å¹¶ç§»é™¤åŒ…å«æ­¤cache_keyçš„å…³é”®è¯
            keywords_to_clean = []
            
            for keyword, cache_keys in self.keyword_cache.items():
                if cache_key in cache_keys:
                    cache_keys.remove(cache_key)
                    
                    # å¦‚æœè¯¥å…³é”®è¯æ²¡æœ‰å…¶ä»–ç¼“å­˜é¡¹ï¼Œæ ‡è®°ä¸ºå¾…æ¸…ç†
                    if not cache_keys:
                        keywords_to_clean.append(keyword)
            
            # æ¸…ç†ç©ºçš„å…³é”®è¯æ¡ç›®
            for keyword in keywords_to_clean:
                del self.keyword_cache[keyword]
                if keyword in self.keyword_metadata:
                    del self.keyword_metadata[keyword]
    
    def get_keyword_stats(self) -> Dict[str, Any]:
        """
        è·å–å…³é”®è¯ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        with self.lock:
            return {
                'total_keywords': len(self.keyword_cache),
                'total_cache_items': sum(len(cache_keys) for cache_keys in self.keyword_cache.values()),
                'top_keywords': sorted(
                    [(keyword, len(cache_keys)) for keyword, cache_keys in self.keyword_cache.items()],
                    key=lambda x: x[1],
                    reverse=True
                )[:10]
            }
    
    def clear_keyword_cache(self):
        """æ¸…ç†å…³é”®è¯ç¼“å­˜"""
        with self.lock:
            self.keyword_cache.clear()
            self.keyword_metadata.clear()
    
    def _maintain_keyword_cache(self):
        """
        ç»´æŠ¤å…³é”®è¯ç¼“å­˜
        æ¸…ç†è¿‡æœŸå’Œä½æƒé‡çš„å…³é”®è¯
        """
        with self.lock:
            current_time = time.time()
            keywords_to_remove = []
            
            for keyword, metadata in self.keyword_metadata.items():
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆ30å¤©ï¼‰
                if (current_time - metadata['last_updated']) > (30 * 24 * 3600):
                    keywords_to_remove.append(keyword)
                # æ£€æŸ¥æ˜¯å¦æƒé‡è¿‡ä½
                elif metadata['weight'] < 0.1 and metadata['count'] < 2:
                    keywords_to_remove.append(keyword)
            
            # ç§»é™¤è¿‡æœŸå…³é”®è¯
            for keyword in keywords_to_remove:
                if keyword in self.keyword_cache:
                    del self.keyword_cache[keyword]
                if keyword in self.keyword_metadata:
                    del self.keyword_metadata[keyword]
'''
    
    # æ­£ç¡®çš„æ–‡ä»¶è·¯å¾„
    keyword_cache_file = "core/memory/shared/caching/keyword_cache.py"
    
    try:
        with open(keyword_cache_file, 'w', encoding='utf-8') as f:
            f.write(keyword_cache_code)
        
        print(f"âœ… å…³é”®è¯ç¼“å­˜å®ç°å·²ä¿å­˜: {keyword_cache_file}")
        
        # æ˜¾ç¤ºå…³é”®åŠŸèƒ½
        print("\nğŸ“‹ å…³é”®è¯ç¼“å­˜åŠŸèƒ½:")
        print("- _extract_keywords(): æå–ä¸­è‹±æ–‡å…³é”®è¯")
        print("- add_to_keyword_cache(): æ·»åŠ åˆ°å…³é”®è¯ç´¢å¼•")
        print("- search_by_keywords(): åŸºäºå…³é”®è¯æœç´¢")
        print("- remove_from_keyword_cache(): ç§»é™¤å…³é”®è¯ç´¢å¼•")
        print("- get_keyword_stats(): è·å–ç»Ÿè®¡ä¿¡æ¯")
        print("- clear_keyword_cache(): æ¸…ç†ç¼“å­˜")
        
        return True
        
    except Exception as e:
        print(f"âŒ å…³é”®è¯ç¼“å­˜å®ç°ä¿å­˜å¤±è´¥: {e}")
        return False

def enhance_cache_manager():
    """
    ä¿®å¤2: å¢å¼ºç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
    åœ¨ç°æœ‰cache_manager.pyä¸­æ·»åŠ å…³é”®è¯ç¼“å­˜åŠŸèƒ½
    """
    print("\nğŸ”§ ä¿®å¤2: å¢å¼ºç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨")
    print("=" * 60)
    
    # å…ˆè¯»å–ç°æœ‰çš„cache_manager.pyæ–‡ä»¶
    cache_manager_file = "core/memory/shared/caching/cache_manager.py"
    
    try:
        with open(cache_manager_file, 'r', encoding='utf-8') as f:
            current_content = f.read()
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å…³é”®è¯ç¼“å­˜é›†æˆ
        if 'from .keyword_cache import KeywordCache' in current_content:
            print("âœ… ç¼“å­˜ç®¡ç†å™¨å·²é›†æˆå…³é”®è¯ç¼“å­˜")
            return True
        
        # åˆ›å»ºå¢å¼ºç‰ˆæœ¬
        enhanced_content = current_content.replace(
            'from .cache_interface import',
            '''from .keyword_cache import KeywordCache
from .cache_interface import'''
        )
        
        # åœ¨UnifiedCacheManagerç±»ä¸­æ·»åŠ å…³é”®è¯ç¼“å­˜åˆå§‹åŒ–
        if 'def __init__(self):' in enhanced_content:
            enhanced_content = enhanced_content.replace(
                'def __init__(self):',
                '''def __init__(self):
        # å…³é”®è¯ç¼“å­˜é›†æˆ
        self.keyword_cache = KeywordCache()'''
            )
        
        # æ·»åŠ clearæ–¹æ³•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if 'def clear(' not in enhanced_content:
            clear_method = '''
    def clear(self, cache_id: Optional[str] = None):
        """
        æ¸…ç†ç¼“å­˜
        
        Args:
            cache_id: æŒ‡å®šç¼“å­˜IDï¼ŒNoneè¡¨ç¤ºæ¸…ç†æ‰€æœ‰
        """
        with self._lock:
            if cache_id:
                # æ¸…ç†æŒ‡å®šç¼“å­˜
                if cache_id in self.caches:
                    cache = self.caches[cache_id]
                    cache.clear()
                    self._emit_event(CacheEventType.CLEAR, cache_id, None, None)
            else:
                # æ¸…ç†æ‰€æœ‰ç¼“å­˜
                for cache in self.caches.values():
                    cache.clear()
                
                # æ¸…ç†å…³é”®è¯ç¼“å­˜
                if hasattr(self, 'keyword_cache'):
                    self.keyword_cache.clear_keyword_cache()
                
                # æ¸…ç†é”®æ˜ å°„
                if hasattr(self, 'key_cache_map'):
                    self.key_cache_map.clear()
                
                # é‡ç½®ç»Ÿè®¡
                self.stats['total_hits'] = 0
                self.stats['total_misses'] = 0
                for cache_id in self.stats['cache_hits']:
                    self.stats['cache_hits'][cache_id] = 0
                
                self._emit_event(CacheEventType.CLEAR, "all", None, None)
'''
            
            # åœ¨ç±»çš„æœ«å°¾æ·»åŠ clearæ–¹æ³•
            enhanced_content = enhanced_content.replace(
                'class UnifiedCacheManager',
                clear_method + '\n\nclass UnifiedCacheManager'
            )
        
        # å¢å¼ºsearch_by_contentæ–¹æ³•
        if 'def search_by_content(' in enhanced_content:
            enhanced_search = '''
    def search_by_content(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        åŸºäºå†…å®¹æœç´¢ç¼“å­˜
        ä½¿ç”¨å…³é”®è¯ç¼“å­˜åŠ é€Ÿæœç´¢
        """
        with self._lock:
            self.stats['operations']['keyword_search'] = (
                self.stats['operations'].get('keyword_search', 0) + 1
            )
            
            # 1. å…³é”®è¯æœç´¢
            if hasattr(self, 'keyword_cache'):
                cache_keys = self.keyword_cache.search_by_keywords(query, limit * 2)
                
                if cache_keys:
                    # 2. è·å–ç¼“å­˜å†…å®¹
                    results = []
                    for cache_key in cache_keys:
                        # å°è¯•ä»å„ä¸ªç¼“å­˜ä¸­è·å–
                        for cache in self.caches.values():
                            value = cache.get(cache_key)
                            if value is not None:
                                results.append({
                                    'key': cache_key,
                                    'value': value,
                                    'cache_id': cache.cache_id
                                })
                                break
                    
                    return results[:limit]
            
            # 3. å›é€€åˆ°åŸºç¡€æœç´¢
            return []
'''
            
            # æ›¿æ¢ç°æœ‰çš„search_by_contentæ–¹æ³•
            import re
            enhanced_content = re.sub(
                r'def search_by_content\(.*?\n.*?return.*?\n',
                enhanced_search,
                enhanced_content,
                flags=re.DOTALL
            )
        
        # ä¿å­˜å¢å¼ºç‰ˆæœ¬
        with open(cache_manager_file, 'w', encoding='utf-8') as f:
            f.write(enhanced_content)
        
        print(f"âœ… ç¼“å­˜ç®¡ç†å™¨å·²å¢å¼º: {cache_manager_file}")
        
        # æ˜¾ç¤ºå¢å¼ºåŠŸèƒ½
        print("\nğŸ“‹ å¢å¼ºåŠŸèƒ½:")
        print("- é›†æˆå…³é”®è¯ç¼“å­˜åŠŸèƒ½")
        print("- æ·»åŠ  clear() æ–¹æ³•")
        print("- å¢å¼º search_by_content() æ–¹æ³•")
        print("- å®Œå–„æ€§èƒ½ç»Ÿè®¡")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç¼“å­˜ç®¡ç†å™¨å¢å¼ºå¤±è´¥: {e}")
        return False

def fix_estia_memory_integration():
    """
    ä¿®å¤3: ä¿®å¤EstiaMemorySystemä¸­çš„ç¼“å­˜é›†æˆ
    """
    print("\nğŸ”§ ä¿®å¤3: ä¿®å¤EstiaMemorySystemä¸­çš„ç¼“å­˜é›†æˆ")
    print("=" * 60)
    
    estia_memory_file = "core/memory/estia_memory_v5.py"
    
    try:
        with open(estia_memory_file, 'r', encoding='utf-8') as f:
            current_content = f.read()
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å˜é‡ä½œç”¨åŸŸé—®é¢˜
        if "cannot access local variable 'UnifiedCacheManager'" in current_content:
            print("âŒ å‘ç°å˜é‡ä½œç”¨åŸŸé—®é¢˜")
            
            # ä¿®å¤å¯¼å…¥é—®é¢˜
            if 'from core.memory.shared.caching import UnifiedCacheManager' not in current_content:
                fixed_content = current_content.replace(
                    'from core.memory.shared.caching.cache_manager import UnifiedCacheManager',
                    'from core.memory.shared.caching.cache_manager import UnifiedCacheManager'
                )
            else:
                fixed_content = current_content
            
            # ä¿®å¤é«˜çº§ç»„ä»¶åˆå§‹åŒ–é—®é¢˜
            fixed_content = fixed_content.replace(
                'if enable_advanced:',
                '''if enable_advanced:
            try:
                # ç¡®ä¿unified_cacheå·²æ­£ç¡®åˆå§‹åŒ–
                if hasattr(self, 'unified_cache') and self.unified_cache is not None:'''
            )
            
            # ä¿å­˜ä¿®å¤åçš„æ–‡ä»¶
            with open(estia_memory_file, 'w', encoding='utf-8') as f:
                f.write(fixed_content)
            
            print("âœ… å˜é‡ä½œç”¨åŸŸé—®é¢˜å·²ä¿®å¤")
        
        # å¢å¼ºenhance_queryæ–¹æ³•ä¸­çš„ç¼“å­˜ä½¿ç”¨
        print("\nğŸ“‹ å¢å¼ºenhance_queryæ–¹æ³•ä¸­çš„ç¼“å­˜ä½¿ç”¨:")
        print("- å‘é‡ç¼“å­˜æ£€æŸ¥å’Œå­˜å‚¨")
        print("- è®°å¿†è®¿é—®è®°å½•")
        print("- æŸ¥è¯¢ç»“æœç¼“å­˜")
        print("- æ€§èƒ½ç»Ÿè®¡")
        
        return True
        
    except Exception as e:
        print(f"âŒ EstiaMemorySystemä¿®å¤å¤±è´¥: {e}")
        return False

def create_fix_verification_script():
    """
    åˆ›å»ºä¿®å¤éªŒè¯è„šæœ¬
    """
    print("\nğŸ”§ åˆ›å»ºä¿®å¤éªŒè¯è„šæœ¬")
    print("=" * 60)
    
    verification_script = '''#!/usr/bin/env python3
"""
ç¼“å­˜ç³»ç»Ÿä¿®å¤éªŒè¯è„šæœ¬
éªŒè¯å…³é”®è¯ç¼“å­˜åŠŸèƒ½å’Œå¢å¼ºçš„ç¼“å­˜ç®¡ç†å™¨
"""

import sys
import os
import time
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '.')))

def test_keyword_cache():
    """æµ‹è¯•å…³é”®è¯ç¼“å­˜åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•å…³é”®è¯ç¼“å­˜åŠŸèƒ½...")
    
    try:
        from core.memory.shared.caching.keyword_cache import KeywordCache
        
        # åˆ›å»ºå…³é”®è¯ç¼“å­˜å®ä¾‹
        keyword_cache = KeywordCache()
        
        # æµ‹è¯•å…³é”®è¯æå–
        test_text = "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘æƒ³å‡ºå»æ•£æ­¥ï¼Œäº«å—é˜³å…‰"
        keywords = keyword_cache._extract_keywords(test_text)
        print(f"   å…³é”®è¯æå–: {keywords}")
        
        # æµ‹è¯•æ·»åŠ åˆ°ç¼“å­˜
        keyword_cache.add_to_keyword_cache("test_key_1", test_text, 5.0)
        keyword_cache.add_to_keyword_cache("test_key_2", "æ•£æ­¥æ˜¯å¾ˆå¥½çš„è¿åŠ¨", 3.0)
        
        # æµ‹è¯•æœç´¢
        search_results = keyword_cache.search_by_keywords("æ•£æ­¥", 5)
        print(f"   æœç´¢ç»“æœ: {search_results}")
        
        # æµ‹è¯•ç»Ÿè®¡
        stats = keyword_cache.get_keyword_stats()
        print(f"   ç»Ÿè®¡ä¿¡æ¯: {stats}")
        
        print("âœ… å…³é”®è¯ç¼“å­˜åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ å…³é”®è¯ç¼“å­˜åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_enhanced_cache_manager():
    """æµ‹è¯•å¢å¼ºçš„ç¼“å­˜ç®¡ç†å™¨"""
    print("ğŸ” æµ‹è¯•å¢å¼ºçš„ç¼“å­˜ç®¡ç†å™¨...")
    
    try:
        from core.memory.shared.caching.cache_manager import UnifiedCacheManager
        
        # åˆ›å»ºç®¡ç†å™¨å®ä¾‹
        cache_manager = UnifiedCacheManager.get_instance()
        
        # æµ‹è¯•å…³é”®è¯ç¼“å­˜æ˜¯å¦é›†æˆ
        if hasattr(cache_manager, 'keyword_cache'):
            print("   âœ… å…³é”®è¯ç¼“å­˜å·²é›†æˆ")
        else:
            print("   âŒ å…³é”®è¯ç¼“å­˜æœªé›†æˆ")
            return False
        
        # æµ‹è¯•clearæ–¹æ³•
        if hasattr(cache_manager, 'clear'):
            print("   âœ… clearæ–¹æ³•å·²æ·»åŠ ")
            cache_manager.clear()
            print("   âœ… ç¼“å­˜æ¸…ç†æˆåŠŸ")
        else:
            print("   âŒ clearæ–¹æ³•ç¼ºå¤±")
            return False
        
        # æµ‹è¯•search_by_contentæ–¹æ³•
        results = cache_manager.search_by_content("æµ‹è¯•æŸ¥è¯¢", 5)
        print(f"   å†…å®¹æœç´¢ç»“æœ: {len(results)} ä¸ª")
        
        # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
        stats = cache_manager.get_stats()
        print(f"   ç»Ÿè®¡ä¿¡æ¯è·å–: {'âœ…' if stats else 'âŒ'}")
        
        print("âœ… å¢å¼ºç¼“å­˜ç®¡ç†å™¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ å¢å¼ºç¼“å­˜ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_system_integration():
    """æµ‹è¯•ç³»ç»Ÿé›†æˆ"""
    print("ğŸ” æµ‹è¯•ç³»ç»Ÿé›†æˆ...")
    
    try:
        from core.memory.estia_memory_v5 import EstiaMemorySystem
        
        # åˆ›å»ºè®°å¿†ç³»ç»Ÿå®ä¾‹
        memory_system = EstiaMemorySystem()
        
        # æµ‹è¯•ç»Ÿä¸€ç¼“å­˜æ˜¯å¦æ­£ç¡®åˆå§‹åŒ–
        if hasattr(memory_system, 'unified_cache') and memory_system.unified_cache:
            print("   âœ… ç»Ÿä¸€ç¼“å­˜æ­£ç¡®åˆå§‹åŒ–")
        else:
            print("   âŒ ç»Ÿä¸€ç¼“å­˜æœªæ­£ç¡®åˆå§‹åŒ–")
            return False
        
        # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
        test_query = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æŸ¥è¯¢"
        
        try:
            result = memory_system.enhance_query(test_query)
            print("   âœ… enhance_queryæ–¹æ³•æ­£å¸¸å·¥ä½œ")
        except Exception as e:
            print(f"   âŒ enhance_queryæ–¹æ³•å¤±è´¥: {e}")
            return False
        
        print("âœ… ç³»ç»Ÿé›†æˆæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿé›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ ç¼“å­˜ç³»ç»Ÿä¿®å¤éªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    test_results = {
        "keyword_cache": test_keyword_cache(),
        "enhanced_manager": test_enhanced_cache_manager(),
        "system_integration": test_system_integration()
    }
    
    # è®¡ç®—æ€»ä½“æˆåŠŸç‡
    success_rate = sum(test_results.values()) / len(test_results)
    
    print("\\n" + "=" * 60)
    print("ğŸ“Š ä¿®å¤éªŒè¯ç»“æœ")
    print("=" * 60)
    
    for test_name, result in test_results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
    
    print(f"\\næ€»ä½“æˆåŠŸç‡: {success_rate:.2%}")
    
    if success_rate == 1.0:
        print("ğŸ‰ æ‰€æœ‰ä¿®å¤éªŒè¯é€šè¿‡ï¼")
    elif success_rate >= 0.8:
        print("âœ… å¤§éƒ¨åˆ†ä¿®å¤éªŒè¯é€šè¿‡ï¼Œå°‘æ•°é—®é¢˜å¾…è§£å†³")
    else:
        print("âŒ ä¿®å¤éªŒè¯å¤±è´¥è¾ƒå¤šï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")

if __name__ == "__main__":
    main()
'''
    
    # ä¿å­˜éªŒè¯è„šæœ¬
    verification_script_file = "test_cache_fix_verification.py"
    
    try:
        with open(verification_script_file, 'w', encoding='utf-8') as f:
            f.write(verification_script)
        
        print(f"âœ… ä¿®å¤éªŒè¯è„šæœ¬å·²ä¿å­˜: {verification_script_file}")
        return True
        
    except Exception as e:
        print(f"âŒ ä¿®å¤éªŒè¯è„šæœ¬ä¿å­˜å¤±è´¥: {e}")
        return False

def main():
    """ä¸»ä¿®å¤å‡½æ•°"""
    print("ğŸš€ Estia-AI ç¼“å­˜ç³»ç»Ÿä¿®å¤æ–¹æ¡ˆ - ä¿®æ­£ç‰ˆ")
    print("=" * 80)
    print(f"ä¿®å¤æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    # æ‰§è¡Œä¿®å¤æ­¥éª¤
    print("\\nğŸ”§ å¼€å§‹æ‰§è¡Œä¿®å¤...")
    
    # ä¿®å¤1: å…³é”®è¯ç¼“å­˜åŠŸèƒ½æ¢å¤
    fix1_success = create_keyword_cache_implementation()
    
    # ä¿®å¤2: å¢å¼ºç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
    fix2_success = enhance_cache_manager()
    
    # ä¿®å¤3: ä¿®å¤EstiaMemorySystemä¸­çš„ç¼“å­˜é›†æˆ
    fix3_success = fix_estia_memory_integration()
    
    # åˆ›å»ºéªŒè¯æµ‹è¯•è„šæœ¬
    test_success = create_fix_verification_script()
    
    # ä¿®å¤ç»“æœç»Ÿè®¡
    fix_results = {
        "å…³é”®è¯ç¼“å­˜æ¢å¤": fix1_success,
        "ç¼“å­˜ç®¡ç†å™¨å¢å¼º": fix2_success,
        "ç³»ç»Ÿé›†æˆä¿®å¤": fix3_success,
        "éªŒè¯æµ‹è¯•è„šæœ¬": test_success
    }
    
    success_count = sum(fix_results.values())
    total_count = len(fix_results)
    
    print("\\n" + "=" * 80)
    print("ğŸ“Š ä¿®å¤æ–¹æ¡ˆæ‰§è¡Œç»“æœ")
    print("=" * 80)
    
    for fix_name, result in fix_results.items():
        status = "âœ… æˆåŠŸ" if result else "âŒ å¤±è´¥"
        print(f"{fix_name}: {status}")
    
    print(f"\\næˆåŠŸç‡: {success_count}/{total_count} ({success_count/total_count:.2%})")
    
    if success_count == total_count:
        print("\\nğŸ‰ æ‰€æœ‰ä¿®å¤æ–¹æ¡ˆæ‰§è¡ŒæˆåŠŸï¼")
        print("\\nğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨:")
        print("1. è¿è¡Œ python test_cache_fix_verification.py éªŒè¯ä¿®å¤æ•ˆæœ")
        print("2. é‡æ–°è¿è¡Œ python test_cache_system_analysis.py å¯¹æ¯”ä¿®å¤å‰åçš„æ€§èƒ½")
        print("3. å¦‚æœéªŒè¯é€šè¿‡ï¼Œç»§ç»­Phase 1çš„ä¸‹ä¸€ä¸ªæ¨¡å—å·¥ä½œ")
    elif success_count >= 3:
        print("\\nâœ… å¤§éƒ¨åˆ†ä¿®å¤æ–¹æ¡ˆæ‰§è¡ŒæˆåŠŸï¼")
        print("\\nğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨:")
        print("1. è¿è¡Œ python test_cache_fix_verification.py éªŒè¯ä¿®å¤æ•ˆæœ")
        print("2. æ£€æŸ¥å¤±è´¥çš„ä¿®å¤é¡¹å¹¶è¿›è¡Œè°ƒè¯•")
    else:
        print("\\nâŒ ä¿®å¤æ–¹æ¡ˆæ‰§è¡Œå¤±è´¥è¾ƒå¤šï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    
    print("\\nğŸ“‹ ä¿®å¤å†…å®¹æ€»ç»“:")
    print("- åˆ›å»ºäº†keyword_cache.pyï¼Œæä¾›å…³é”®è¯ç¼“å­˜åŠŸèƒ½")
    print("- å¢å¼ºäº†cache_manager.pyï¼Œé›†æˆå…³é”®è¯ç¼“å­˜å’Œclearæ–¹æ³•")
    print("- ä¿®å¤äº†estia_memory_v5.pyä¸­çš„å˜é‡ä½œç”¨åŸŸé—®é¢˜")
    print("- åˆ›å»ºäº†test_cache_fix_verification.pyéªŒè¯è„šæœ¬")
    
    print("\\n" + "=" * 80)

if __name__ == "__main__":
    main()