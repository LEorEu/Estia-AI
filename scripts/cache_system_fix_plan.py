#!/usr/bin/env python3
"""
Estia-AI ç¼“å­˜ç³»ç»Ÿä¿®å¤æ–¹æ¡ˆ
åŸºäºæµ‹è¯•ç»“æœåˆ†æï¼Œé’ˆå¯¹å…³é”®é—®é¢˜è¿›è¡Œä¿®å¤

ä¿®å¤ä¼˜å…ˆçº§ï¼š
1. ã€é«˜ã€‘å…³é”®è¯ç¼“å­˜åŠŸèƒ½æ¢å¤
2. ã€é«˜ã€‘UnifiedCacheManagerå˜é‡ä½œç”¨åŸŸä¿®å¤
3. ã€ä¸­ã€‘é›†æˆæ·±åº¦å¢å¼º
4. ã€ä½ã€‘ç¼“å­˜æ¸…ç†æ–¹æ³•è¡¥å…¨

ä¿®å¤ç­–ç•¥ï¼š
- å‚è€ƒæ—§ç³»ç»Ÿcore/old_memoryçš„æˆåŠŸå®ç°
- ä¿æŒæ–°ç³»ç»Ÿçš„æ¶æ„ä¼˜åŠ¿
- ç¡®ä¿å‘åå…¼å®¹æ€§
"""

import sys
import os
import re
import time
import json
from typing import Dict, List, Set, Any, Optional
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

def fix_unified_cache_manager_scope():
    """
    ä¿®å¤1: UnifiedCacheManagerå˜é‡ä½œç”¨åŸŸé—®é¢˜
    
    é—®é¢˜ï¼šestia_memory_v5.pyä¸­å‡ºç° "cannot access local variable 'UnifiedCacheManager'"
    åŸå› ï¼šå˜é‡ä½œç”¨åŸŸé—®é¢˜
    è§£å†³ï¼šä¿®å¤å¯¼å…¥å’Œå˜é‡ä½¿ç”¨
    """
    print("ğŸ”§ ä¿®å¤1: UnifiedCacheManagerå˜é‡ä½œç”¨åŸŸé—®é¢˜")
    print("=" * 60)
    
    # å®šä½é—®é¢˜æ–‡ä»¶
    estia_memory_file = "/mnt/d/Estia-AI/core/memory/estia_memory_v5.py"
    
    try:
        with open(estia_memory_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æŸ¥æ‰¾é—®é¢˜ä»£ç æ®µ
        if "cannot access local variable 'UnifiedCacheManager'" in content:
            print("âŒ å‘ç°å˜é‡ä½œç”¨åŸŸé—®é¢˜")
        
        # åˆ†æä»£ç ç»“æ„
        lines = content.split('\n')
        problematic_lines = []
        
        for i, line in enumerate(lines):
            if 'UnifiedCacheManager' in line and ('import' not in line):
                problematic_lines.append((i+1, line.strip()))
        
        print(f"ğŸ“ å‘ç° {len(problematic_lines)} å¤„ UnifiedCacheManager ä½¿ç”¨")
        
        # æŸ¥æ‰¾å¯¼å…¥è¯­å¥
        import_lines = []
        for i, line in enumerate(lines):
            if 'UnifiedCacheManager' in line and 'import' in line:
                import_lines.append((i+1, line.strip()))
        
        print(f"ğŸ“ å‘ç° {len(import_lines)} å¤„ UnifiedCacheManager å¯¼å…¥")
        
        # æä¾›ä¿®å¤å»ºè®®
        print("\nğŸ”§ ä¿®å¤å»ºè®®:")
        print("1. ç¡®ä¿åœ¨æ–‡ä»¶é¡¶éƒ¨æ­£ç¡®å¯¼å…¥ UnifiedCacheManager")
        print("2. æ£€æŸ¥æ˜¯å¦å­˜åœ¨å±€éƒ¨å˜é‡é‡åé—®é¢˜")
        print("3. ä½¿ç”¨å®Œæ•´çš„æ¨¡å—è·¯å¾„é¿å…å‘½åå†²çª")
        
        # ç”Ÿæˆä¿®å¤ä»£ç 
        fix_code = '''
# ä¿®å¤æ–¹æ¡ˆï¼šåœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ æ­£ç¡®çš„å¯¼å…¥
from core.memory.shared.caching.cache_manager import UnifiedCacheManager

# åœ¨ EstiaMemorySystem ç±»çš„ __init__ æ–¹æ³•ä¸­ï¼š
def __init__(self, db_path: str = "assets/memory.db", enable_advanced: bool = True):
    """åˆå§‹åŒ–Estiaè®°å¿†ç³»ç»Ÿ"""
    try:
        # ç¡®ä¿ä½¿ç”¨å®Œæ•´çš„ç±»åï¼Œé¿å…ä½œç”¨åŸŸé—®é¢˜
        self.unified_cache = UnifiedCacheManager.get_instance()
        logger.info("âœ… ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # åœ¨é«˜çº§ç»„ä»¶åˆå§‹åŒ–ä¸­ä½¿ç”¨ self.unified_cache
        if enable_advanced:
            try:
                # ä½¿ç”¨å·²ç»åˆå§‹åŒ–çš„ unified_cache å®ä¾‹
                if hasattr(self, 'unified_cache') and self.unified_cache:
                    # é«˜çº§ç»„ä»¶åˆå§‹åŒ–ä»£ç 
                    pass
            except Exception as e:
                logger.warning(f"é«˜çº§ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
                
    except Exception as e:
        logger.error(f"ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        self.unified_cache = None
'''
        
        print(f"\nğŸ“ ä¿®å¤ä»£ç ç¤ºä¾‹:")
        print(fix_code)
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿®å¤å¤±è´¥: {e}")
        return False

def create_keyword_cache_implementation():
    """
    ä¿®å¤2: å…³é”®è¯ç¼“å­˜åŠŸèƒ½æ¢å¤
    
    é—®é¢˜ï¼šæ–°ç³»ç»Ÿç¼ºå°‘å…³é”®è¯ç¼“å­˜åŠŸèƒ½
    è§£å†³ï¼šå‚è€ƒæ—§ç³»ç»Ÿå®ç°ï¼Œæ·»åŠ å…³é”®è¯ç¼“å­˜åŠŸèƒ½
    """
    print("\nğŸ”§ ä¿®å¤2: å…³é”®è¯ç¼“å­˜åŠŸèƒ½æ¢å¤")
    print("=" * 60)
    
    # åŸºäºæ—§ç³»ç»Ÿçš„å…³é”®è¯ç¼“å­˜å®ç°
    keyword_cache_code = '''
import re
import threading
from typing import Dict, List, Set, Any, Optional
from collections import defaultdict, OrderedDict

class KeywordCache:
    """
    å…³é”®è¯ç¼“å­˜ç³»ç»Ÿ
    åŸºäºæ—§ç³»ç»Ÿ core/old_memory/embedding/cache.py çš„å®ç°
    """
    
    def __init__(self, max_keywords: int = 10000):
        """åˆå§‹åŒ–å…³é”®è¯ç¼“å­˜"""
        self.max_keywords = max_keywords
        self.keyword_cache: Dict[str, Set[str]] = defaultdict(set)
        self.keyword_metadata: Dict[str, Dict[str, Any]] = {}
        self.lock = threading.RLock()
        
        # ä¸­æ–‡åœç”¨è¯
        self.stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹',
            'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'ä»€ä¹ˆ', 'å¯ä»¥', 'è¿™ä¸ª', 'è¿˜', 'æ—¶å€™', 'å¦‚æœ'
        }
        
        # è‹±æ–‡åœç”¨è¯
        self.stop_words.update({
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'this', 'that', 'is', 'are', 'was', 'were', 'be',
            'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',
            'would', 'should', 'could', 'can', 'may', 'might', 'must', 'shall'
        })
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        æå–å…³é”®è¯
        åŸºäºæ—§ç³»ç»Ÿçš„å®ç°ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··åˆæ–‡æœ¬
        """
        if not text:
            return []
            
        # æå–ä¸­æ–‡å’Œè‹±æ–‡è¯æ±‡
        words = re.findall(r'[\\w\\u4e00-\\u9fff]+', text.lower())
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        keywords = []
        for word in words:
            if (len(word) > 1 and 
                word not in self.stop_words and 
                not word.isdigit()):
                keywords.append(word)
        
        # é™åˆ¶å…³é”®è¯æ•°é‡
        return keywords[:10]
    
    def add_to_keyword_cache(self, cache_key: str, text: str, weight: float = 1.0):
        """
        æ·»åŠ åˆ°å…³é”®è¯ç¼“å­˜
        
        Args:
            cache_key: ç¼“å­˜é”®
            text: æ–‡æœ¬å†…å®¹
            weight: æƒé‡
        """
        with self.lock:
            keywords = self._extract_keywords(text)
            
            for keyword in keywords:
                # æ·»åŠ åˆ°å…³é”®è¯æ˜ å°„
                self.keyword_cache[keyword].add(cache_key)
                
                # æ›´æ–°å…ƒæ•°æ®
                if keyword not in self.keyword_metadata:
                    self.keyword_metadata[keyword] = {
                        'count': 0,
                        'weight': 0.0,
                        'last_updated': None
                    }
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.keyword_metadata[keyword]['count'] += 1
                self.keyword_metadata[keyword]['weight'] = max(
                    self.keyword_metadata[keyword]['weight'], weight
                )
                self.keyword_metadata[keyword]['last_updated'] = time.time()
    
    def search_by_keywords(self, query: str, limit: int = 10) -> List[str]:
        """
        åŸºäºå…³é”®è¯æœç´¢ç¼“å­˜é¡¹
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            ç›¸å…³çš„ç¼“å­˜é”®åˆ—è¡¨
        """
        with self.lock:
            keywords = self._extract_keywords(query)
            
            if not keywords:
                return []
            
            # æ”¶é›†å€™é€‰é¡¹
            candidates = defaultdict(float)
            
            for keyword in keywords:
                if keyword in self.keyword_cache:
                    # è·å–åŒ…å«æ­¤å…³é”®è¯çš„ç¼“å­˜é¡¹
                    cache_keys = self.keyword_cache[keyword]
                    keyword_weight = self.keyword_metadata[keyword]['weight']
                    
                    for cache_key in cache_keys:
                        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
                        candidates[cache_key] += keyword_weight
            
            # æŒ‰åˆ†æ•°æ’åº
            sorted_candidates = sorted(
                candidates.items(), 
                key=lambda x: x[1], 
                reverse=True
            )
            
            return [cache_key for cache_key, score in sorted_candidates[:limit]]
    
    def remove_from_keyword_cache(self, cache_key: str):
        """
        ä»å…³é”®è¯ç¼“å­˜ä¸­ç§»é™¤é¡¹
        
        Args:
            cache_key: è¦ç§»é™¤çš„ç¼“å­˜é”®
        """
        with self.lock:
            # æŸ¥æ‰¾å¹¶ç§»é™¤åŒ…å«æ­¤cache_keyçš„å…³é”®è¯
            keywords_to_clean = []
            
            for keyword, cache_keys in self.keyword_cache.items():
                if cache_key in cache_keys:
                    cache_keys.remove(cache_key)
                    
                    # å¦‚æœè¯¥å…³é”®è¯æ²¡æœ‰å…¶ä»–ç¼“å­˜é¡¹ï¼Œæ ‡è®°ä¸ºå¾…æ¸…ç†
                    if not cache_keys:
                        keywords_to_clean.append(keyword)
            
            # æ¸…ç†ç©ºçš„å…³é”®è¯æ¡ç›®
            for keyword in keywords_to_clean:
                del self.keyword_cache[keyword]
                if keyword in self.keyword_metadata:
                    del self.keyword_metadata[keyword]
    
    def get_keyword_stats(self) -> Dict[str, Any]:
        """
        è·å–å…³é”®è¯ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        with self.lock:
            return {
                'total_keywords': len(self.keyword_cache),
                'total_cache_items': sum(len(cache_keys) for cache_keys in self.keyword_cache.values()),
                'top_keywords': sorted(
                    [(keyword, len(cache_keys)) for keyword, cache_keys in self.keyword_cache.items()],
                    key=lambda x: x[1],
                    reverse=True
                )[:10]
            }
    
    def clear_keyword_cache(self):
        """æ¸…ç†å…³é”®è¯ç¼“å­˜"""
        with self.lock:
            self.keyword_cache.clear()
            self.keyword_metadata.clear()
    
    def _maintain_keyword_cache(self):
        """
        ç»´æŠ¤å…³é”®è¯ç¼“å­˜
        æ¸…ç†è¿‡æœŸå’Œä½æƒé‡çš„å…³é”®è¯
        """
        with self.lock:
            current_time = time.time()
            keywords_to_remove = []
            
            for keyword, metadata in self.keyword_metadata.items():
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆ30å¤©ï¼‰
                if (current_time - metadata['last_updated']) > (30 * 24 * 3600):
                    keywords_to_remove.append(keyword)
                # æ£€æŸ¥æ˜¯å¦æƒé‡è¿‡ä½
                elif metadata['weight'] < 0.1 and metadata['count'] < 2:
                    keywords_to_remove.append(keyword)
            
            # ç§»é™¤è¿‡æœŸå…³é”®è¯
            for keyword in keywords_to_remove:
                if keyword in self.keyword_cache:
                    del self.keyword_cache[keyword]
                if keyword in self.keyword_metadata:
                    del self.keyword_metadata[keyword]
'''
    
    # ä¿å­˜å…³é”®è¯ç¼“å­˜å®ç°
    keyword_cache_file = "/mnt/d/Estia-AI/core/memory/shared/caching/keyword_cache.py"
    
    try:
        with open(keyword_cache_file, 'w', encoding='utf-8') as f:
            f.write(keyword_cache_code)
        
        print(f"âœ… å…³é”®è¯ç¼“å­˜å®ç°å·²ä¿å­˜: {keyword_cache_file}")
        
        # æ˜¾ç¤ºå…³é”®åŠŸèƒ½
        print("\nğŸ“‹ å…³é”®è¯ç¼“å­˜åŠŸèƒ½:")
        print("- _extract_keywords(): æå–ä¸­è‹±æ–‡å…³é”®è¯")
        print("- add_to_keyword_cache(): æ·»åŠ åˆ°å…³é”®è¯ç´¢å¼•")
        print("- search_by_keywords(): åŸºäºå…³é”®è¯æœç´¢")
        print("- remove_from_keyword_cache(): ç§»é™¤å…³é”®è¯ç´¢å¼•")
        print("- get_keyword_stats(): è·å–ç»Ÿè®¡ä¿¡æ¯")
        print("- clear_keyword_cache(): æ¸…ç†ç¼“å­˜")
        
        return True
        
    except Exception as e:
        print(f"âŒ å…³é”®è¯ç¼“å­˜å®ç°ä¿å­˜å¤±è´¥: {e}")
        return False

def create_enhanced_cache_manager():
    """
    ä¿®å¤3: å¢å¼ºç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
    
    é—®é¢˜ï¼šç¼ºå°‘clearæ–¹æ³•å’Œå…³é”®è¯ç¼“å­˜é›†æˆ
    è§£å†³ï¼šå¢å¼ºUnifiedCacheManagerï¼Œé›†æˆå…³é”®è¯ç¼“å­˜
    """
    print("\nğŸ”§ ä¿®å¤3: å¢å¼ºç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨")
    print("=" * 60)
    
    enhanced_manager_code = '''
# å¢å¼ºç‰ˆç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
# æ–‡ä»¶ä½ç½®: core/memory/shared/caching/cache_manager.py

import threading
import time
from typing import Dict, List, Set, Any, Optional, TypeVar, Generic
from .keyword_cache import KeywordCache
from .cache_interface import CacheInterface, CacheLevel, CacheEvent, CacheEventType

K = TypeVar('K')
V = TypeVar('V')
M = TypeVar('M')

class EnhancedUnifiedCacheManager(Generic[K, V, M]):
    """
    å¢å¼ºç‰ˆç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
    é›†æˆå…³é”®è¯ç¼“å­˜åŠŸèƒ½
    """
    
    _instance = None
    _lock = threading.RLock()
    
    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
            
        with self._lock:
            if not self._initialized:
                # åŸºç¡€ç¼“å­˜ç®¡ç†
                self.caches: Dict[str, CacheInterface] = {}
                self.level_caches: Dict[CacheLevel, List[CacheInterface]] = {
                    level: [] for level in CacheLevel
                }
                
                # å…³é”®è¯ç¼“å­˜é›†æˆ
                self.keyword_cache = KeywordCache()
                
                # æ€§èƒ½ç»Ÿè®¡
                self.stats = {
                    'total_hits': 0,
                    'total_misses': 0,
                    'cache_hits': {},
                    'level_hits': {},
                    'operations': {},
                    'keyword_searches': 0,
                    'keyword_hits': 0
                }
                
                # ç¼“å­˜é”®æ˜ å°„
                self.key_cache_map: Dict[K, Set[str]] = {}
                
                # äº‹ä»¶ç›‘å¬å™¨
                self.listeners: List[callable] = []
                
                # é…ç½®
                self.config = {
                    'maintenance_interval': 300,
                    'enable_keyword_cache': True,
                    'keyword_cache_threshold': 0.5
                }
                
                self._initialized = True
    
    @classmethod
    def get_instance(cls):
        """è·å–å•ä¾‹å®ä¾‹"""
        return cls()
    
    def register_cache(self, cache: CacheInterface, level: CacheLevel = CacheLevel.EXTERNAL):
        """
        æ³¨å†Œç¼“å­˜åˆ°ç®¡ç†å™¨
        
        Args:
            cache: ç¼“å­˜å®ä¾‹
            level: ç¼“å­˜çº§åˆ«
        """
        with self._lock:
            cache_id = cache.cache_id
            self.caches[cache_id] = cache
            self.level_caches[level].append(cache)
            
            # åˆå§‹åŒ–ç»Ÿè®¡
            self.stats['cache_hits'][cache_id] = 0
            self.stats['level_hits'][level.value] = 0
            
            self._emit_event(CacheEventType.INIT, cache_id, None, None)
    
    def get(self, key: K, default: Optional[V] = None) -> Optional[V]:
        """
        è·å–ç¼“å­˜å€¼
        æ”¯æŒå¤šçº§ç¼“å­˜æŸ¥æ‰¾å’Œå…³é”®è¯æœç´¢
        """
        with self._lock:
            # 1. æ£€æŸ¥é”®æ˜ å°„ä¸­çš„å·²çŸ¥ç¼“å­˜
            if key in self.key_cache_map:
                for cache_id in self.key_cache_map[key]:
                    if cache_id in self.caches:
                        cache = self.caches[cache_id]
                        value = cache.get(key)
                        if value is not None:
                            self._record_hit(cache_id)
                            return value
            
            # 2. æŒ‰ä¼˜å…ˆçº§é¡ºåºæŸ¥æ‰¾
            for level in [CacheLevel.HOT, CacheLevel.WARM, CacheLevel.COLD, CacheLevel.PERSISTENT]:
                for cache in self.level_caches[level]:
                    value = cache.get(key)
                    if value is not None:
                        self._record_hit(cache.cache_id)
                        self._update_key_mapping(key, cache.cache_id)
                        return value
            
            # 3. è®°å½•æœªå‘½ä¸­
            self._record_miss()
            return default
    
    def put(self, key: K, value: V, metadata: Optional[M] = None, 
            text_content: Optional[str] = None, weight: float = 1.0):
        """
        æ”¾å…¥ç¼“å­˜å€¼
        æ”¯æŒå…³é”®è¯ç¼“å­˜é›†æˆ
        """
        with self._lock:
            # 1. å­˜å‚¨åˆ°é€‚å½“çš„ç¼“å­˜çº§åˆ«
            target_level = self._determine_cache_level(weight)
            
            if self.level_caches[target_level]:
                cache = self.level_caches[target_level][0]
                cache.put(key, value, metadata)
                
                # 2. æ›´æ–°é”®æ˜ å°„
                self._update_key_mapping(key, cache.cache_id)
                
                # 3. æ·»åŠ åˆ°å…³é”®è¯ç¼“å­˜
                if (self.config['enable_keyword_cache'] and 
                    text_content and 
                    weight >= self.config['keyword_cache_threshold']):
                    self.keyword_cache.add_to_keyword_cache(
                        str(key), text_content, weight
                    )
                
                # 4. è§¦å‘äº‹ä»¶
                self._emit_event(CacheEventType.PUT, cache.cache_id, key, value)
    
    def search_by_content(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        åŸºäºå†…å®¹æœç´¢ç¼“å­˜
        ä½¿ç”¨å…³é”®è¯ç¼“å­˜åŠ é€Ÿæœç´¢
        """
        with self._lock:
            self.stats['keyword_searches'] += 1
            
            # 1. å…³é”®è¯æœç´¢
            if self.config['enable_keyword_cache']:
                cache_keys = self.keyword_cache.search_by_keywords(query, limit * 2)
                
                if cache_keys:
                    self.stats['keyword_hits'] += 1
                    
                    # 2. è·å–ç¼“å­˜å†…å®¹
                    results = []
                    for cache_key in cache_keys:
                        # å°è¯•ä»å„ä¸ªç¼“å­˜ä¸­è·å–
                        for cache in self.caches.values():
                            value = cache.get(cache_key)
                            if value is not None:
                                results.append({
                                    'key': cache_key,
                                    'value': value,
                                    'cache_id': cache.cache_id
                                })
                                break
                    
                    return results[:limit]
            
            # 3. å›é€€åˆ°éå†æœç´¢
            results = []
            for cache_id, cache in self.caches.items():
                # ç®€å•çš„éå†æœç´¢ï¼ˆå¯ä»¥æ ¹æ®å…·ä½“ç¼“å­˜ç±»å‹ä¼˜åŒ–ï¼‰
                if hasattr(cache, 'search'):
                    cache_results = cache.search(query, limit)
                    results.extend(cache_results)
            
            return results[:limit]
    
    def record_memory_access(self, memory_id: str, access_weight: float = 1.0):
        """
        è®°å½•è®°å¿†è®¿é—®
        æ›´æ–°ç¼“å­˜ä¼˜å…ˆçº§
        """
        with self._lock:
            # æ›´æ–°è®¿é—®ç»Ÿè®¡
            self.stats['operations']['memory_access'] = (
                self.stats['operations'].get('memory_access', 0) + 1
            )
            
            # å¯ä»¥æ ¹æ®è®¿é—®æƒé‡è°ƒæ•´ç¼“å­˜ä¼˜å…ˆçº§
            # è¿™é‡Œå¯ä»¥å®ç°æ™ºèƒ½æå‡é€»è¾‘
            pass
    
    def clear(self, cache_id: Optional[str] = None):
        """
        æ¸…ç†ç¼“å­˜
        
        Args:
            cache_id: æŒ‡å®šç¼“å­˜IDï¼ŒNoneè¡¨ç¤ºæ¸…ç†æ‰€æœ‰
        """
        with self._lock:
            if cache_id:
                # æ¸…ç†æŒ‡å®šç¼“å­˜
                if cache_id in self.caches:
                    cache = self.caches[cache_id]
                    cache.clear()
                    self._emit_event(CacheEventType.CLEAR, cache_id, None, None)
            else:
                # æ¸…ç†æ‰€æœ‰ç¼“å­˜
                for cache in self.caches.values():
                    cache.clear()
                
                # æ¸…ç†å…³é”®è¯ç¼“å­˜
                self.keyword_cache.clear_keyword_cache()
                
                # æ¸…ç†é”®æ˜ å°„
                self.key_cache_map.clear()
                
                # é‡ç½®ç»Ÿè®¡
                self.stats['total_hits'] = 0
                self.stats['total_misses'] = 0
                for cache_id in self.stats['cache_hits']:
                    self.stats['cache_hits'][cache_id] = 0
                
                self._emit_event(CacheEventType.CLEAR, "all", None, None)
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        åŒ…å«å…³é”®è¯ç¼“å­˜ç»Ÿè®¡
        """
        with self._lock:
            hit_ratio = 0.0
            total_operations = self.stats['total_hits'] + self.stats['total_misses']
            if total_operations > 0:
                hit_ratio = self.stats['total_hits'] / total_operations
            
            # è·å–å…³é”®è¯ç¼“å­˜ç»Ÿè®¡
            keyword_stats = self.keyword_cache.get_keyword_stats()
            
            return {
                'manager': {
                    'hit_ratio': hit_ratio,
                    'total_hits': self.stats['total_hits'],
                    'total_misses': self.stats['total_misses'],
                    'cache_hits': self.stats['cache_hits'].copy(),
                    'level_hits': self.stats['level_hits'].copy(),
                    'operations': self.stats['operations'].copy(),
                    'keyword_searches': self.stats['keyword_searches'],
                    'keyword_hits': self.stats['keyword_hits']
                },
                'caches': {
                    cache_id: cache.get_stats() if hasattr(cache, 'get_stats') else {}
                    for cache_id, cache in self.caches.items()
                },
                'keyword_cache': keyword_stats
            }
    
    def _determine_cache_level(self, weight: float) -> CacheLevel:
        """æ ¹æ®æƒé‡ç¡®å®šç¼“å­˜çº§åˆ«"""
        if weight >= 8.0:
            return CacheLevel.HOT
        elif weight >= 5.0:
            return CacheLevel.WARM
        elif weight >= 2.0:
            return CacheLevel.COLD
        else:
            return CacheLevel.PERSISTENT
    
    def _update_key_mapping(self, key: K, cache_id: str):
        """æ›´æ–°é”®æ˜ å°„"""
        if key not in self.key_cache_map:
            self.key_cache_map[key] = set()
        self.key_cache_map[key].add(cache_id)
    
    def _record_hit(self, cache_id: str):
        """è®°å½•ç¼“å­˜å‘½ä¸­"""
        self.stats['total_hits'] += 1
        self.stats['cache_hits'][cache_id] = self.stats['cache_hits'].get(cache_id, 0) + 1
    
    def _record_miss(self):
        """è®°å½•ç¼“å­˜æœªå‘½ä¸­"""
        self.stats['total_misses'] += 1
    
    def _emit_event(self, event_type: CacheEventType, cache_id: str, key: K, value: V):
        """è§¦å‘äº‹ä»¶"""
        event = CacheEvent(event_type, cache_id, key, value, time.time())
        for listener in self.listeners:
            try:
                listener(event)
            except Exception as e:
                # å¿½ç•¥ç›‘å¬å™¨å¼‚å¸¸
                pass

# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿æŒåŸæœ‰çš„ç±»å
UnifiedCacheManager = EnhancedUnifiedCacheManager
'''
    
    # ä¿å­˜å¢å¼ºç‰ˆç¼“å­˜ç®¡ç†å™¨
    enhanced_manager_file = "/mnt/d/Estia-AI/core/memory/shared/caching/enhanced_cache_manager.py"
    
    try:
        with open(enhanced_manager_file, 'w', encoding='utf-8') as f:
            f.write(enhanced_manager_code)
        
        print(f"âœ… å¢å¼ºç‰ˆç¼“å­˜ç®¡ç†å™¨å·²ä¿å­˜: {enhanced_manager_file}")
        
        # æ˜¾ç¤ºå¢å¼ºåŠŸèƒ½
        print("\nğŸ“‹ å¢å¼ºåŠŸèƒ½:")
        print("- é›†æˆå…³é”®è¯ç¼“å­˜åŠŸèƒ½")
        print("- æ·»åŠ  clear() æ–¹æ³•")
        print("- å¢å¼º search_by_content() æ–¹æ³•")
        print("- å®Œå–„æ€§èƒ½ç»Ÿè®¡")
        print("- æ·»åŠ  record_memory_access() æ–¹æ³•")
        print("- æ”¯æŒåŸºäºå†…å®¹çš„æ™ºèƒ½ç¼“å­˜")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¢å¼ºç‰ˆç¼“å­˜ç®¡ç†å™¨ä¿å­˜å¤±è´¥: {e}")
        return False

def create_integration_enhancement():
    """
    ä¿®å¤4: é›†æˆæ·±åº¦å¢å¼º
    
    é—®é¢˜ï¼šenhance_queryæ–¹æ³•ä¸­ç¼“å­˜ä½¿ç”¨ä¸è¶³
    è§£å†³ï¼šå¢å¼ºæ ¸å¿ƒæµç¨‹ä¸­çš„ç¼“å­˜é›†æˆ
    """
    print("\nğŸ”§ ä¿®å¤4: é›†æˆæ·±åº¦å¢å¼º")
    print("=" * 60)
    
    integration_code = '''
# é›†æˆæ·±åº¦å¢å¼ºæ–¹æ¡ˆ
# æ–‡ä»¶ä½ç½®: core/memory/estia_memory_v5.py

# åœ¨ EstiaMemorySystem ç±»ä¸­å¢å¼º enhance_query æ–¹æ³•

def enhance_query(self, user_input: str, context: dict = None) -> str:
    """
    å¢å¼ºæŸ¥è¯¢å¤„ç†
    æ·±åº¦é›†æˆç¼“å­˜ç³»ç»Ÿï¼Œå‚è€ƒæ—§ç³»ç»Ÿçš„3ä¸ªå…³é”®ç¼“å­˜ä½¿ç”¨ä½ç½®
    """
    start_time = time.time()
    
    try:
        # 1. å‘é‡ç¼“å­˜ä½¿ç”¨ï¼ˆç¬¬ä¸€ä¸ªå…³é”®ä½ç½®ï¼‰
        cache_key = f"vector:{user_input}"
        cached_vector = None
        
        if self.unified_cache:
            cached_vector = self.unified_cache.get(cache_key)
            
        if cached_vector is None:
            # å‘é‡åŒ–
            if self.vectorizer:
                cached_vector = self.vectorizer.encode(user_input)
                if self.unified_cache:
                    self.unified_cache.put(
                        cache_key, cached_vector, 
                        text_content=user_input, 
                        weight=1.0
                    )
        
        # 2. è®°å¿†è®¿é—®è®°å½•ï¼ˆç¬¬äºŒä¸ªå…³é”®ä½ç½®ï¼‰
        if self.unified_cache:
            self.unified_cache.record_memory_access(
                f"query:{user_input}", 
                access_weight=1.0
            )
        
        # 3. æŸ¥è¯¢ç»“æœç¼“å­˜ï¼ˆç¬¬ä¸‰ä¸ªå…³é”®ä½ç½®ï¼‰
        result_cache_key = f"result:{user_input}"
        cached_result = None
        
        if self.unified_cache:
            cached_result = self.unified_cache.get(result_cache_key)
            
        if cached_result is None:
            # æ‰§è¡Œå®é™…çš„æŸ¥è¯¢å¢å¼ºé€»è¾‘
            if hasattr(self, 'sync_flow_manager') and self.sync_flow_manager:
                enhanced_context = self.sync_flow_manager.enhance_query(
                    user_input, context or {}
                )
                
                # ç¼“å­˜ç»“æœ
                if self.unified_cache:
                    self.unified_cache.put(
                        result_cache_key, enhanced_context,
                        text_content=user_input,
                        weight=2.0
                    )
                
                cached_result = enhanced_context
            else:
                cached_result = user_input
        
        # 4. æ€§èƒ½ç»Ÿè®¡å’Œç›‘æ§
        end_time = time.time()
        processing_time = (end_time - start_time) * 1000  # ms
        
        if self.unified_cache:
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            self.unified_cache.record_memory_access(
                f"performance:{processing_time:.2f}ms",
                access_weight=0.5
            )
        
        logger.info(f"âœ… åŒæ­¥æµç¨‹å®Œæˆï¼Œæ€»è€—æ—¶: {processing_time:.2f}ms")
        
        return cached_result
        
    except Exception as e:
        logger.error(f"æŸ¥è¯¢å¢å¼ºå¤±è´¥: {e}")
        return user_input

# å¢å¼ºåˆå§‹åŒ–æ–¹æ³•
def __init__(self, db_path: str = "assets/memory.db", enable_advanced: bool = True):
    """
    åˆå§‹åŒ–Estiaè®°å¿†ç³»ç»Ÿ
    ç¡®ä¿ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨æ­£ç¡®åˆå§‹åŒ–
    """
    logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–Estiaè®°å¿†ç³»ç»Ÿ v5.0.0")
    
    try:
        # 1. æ•°æ®åº“ç®¡ç†å™¨
        self.db_manager = DatabaseManager(db_path)
        logger.info("âœ… æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # 2. ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨ - ä¿®å¤å˜é‡ä½œç”¨åŸŸé—®é¢˜
        from core.memory.shared.caching.enhanced_cache_manager import EnhancedUnifiedCacheManager
        self.unified_cache = EnhancedUnifiedCacheManager.get_instance()
        logger.info("âœ… ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # 3. å‘é‡åŒ–å™¨
        self.vectorizer = TextVectorizer()
        logger.info("âœ… ä½¿ç”¨TextVectorizerï¼ˆall-MiniLM-L6-v2ï¼‰")
        
        # 4. åŸºç¡€è®°å¿†å­˜å‚¨å™¨
        self.memory_store = MemoryStore(
            db_manager=self.db_manager,
            vectorizer=self.vectorizer
        )
        logger.info("âœ… åŸºç¡€è®°å¿†å­˜å‚¨å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # 5. é«˜çº§ç»„ä»¶åˆå§‹åŒ– - ä¿®å¤å˜é‡ä½œç”¨åŸŸé—®é¢˜
        if enable_advanced:
            try:
                # ç¡®ä¿unified_cacheå·²æ­£ç¡®åˆå§‹åŒ–
                if hasattr(self, 'unified_cache') and self.unified_cache is not None:
                    # åˆå§‹åŒ–é«˜çº§ç»„ä»¶
                    self.smart_retriever = SmartRetriever(
                        db_manager=self.db_manager,
                        vectorizer=self.vectorizer,
                        cache_manager=self.unified_cache
                    )
                    
                    self.faiss_retriever = FAISSSearchEngine(
                        db_manager=self.db_manager,
                        vectorizer=self.vectorizer
                    )
                    
                    # åˆå§‹åŒ–åŒæ­¥æµç¨‹ç®¡ç†å™¨
                    self.sync_flow_manager = SyncFlowManager(
                        db_manager=self.db_manager,
                        unified_cache=self.unified_cache,
                        vectorizer=self.vectorizer,
                        memory_store=self.memory_store
                    )
                    
                    logger.info("âœ… é«˜çº§ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
                else:
                    logger.warning("ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨æœªæ­£ç¡®åˆå§‹åŒ–ï¼Œè·³è¿‡é«˜çº§ç»„ä»¶")
                    
            except Exception as e:
                logger.warning(f"é«˜çº§ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
                # è®¾ç½®ä¸ºNoneä»¥ä¾¿åç»­æ£€æŸ¥
                self.smart_retriever = None
                self.faiss_retriever = None
                self.sync_flow_manager = None
        
        logger.info("âœ… Estiaè®°å¿†ç³»ç»Ÿ v5.0.0 åˆå§‹åŒ–å®Œæˆ")
        
    except Exception as e:
        logger.error(f"Estiaè®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        raise
'''
    
    print("ğŸ“ é›†æˆæ·±åº¦å¢å¼ºæ–¹æ¡ˆ:")
    print("1. åœ¨enhance_queryæ–¹æ³•ä¸­æ·»åŠ 3ä¸ªå…³é”®ç¼“å­˜ä½¿ç”¨ä½ç½®")
    print("2. ä¿®å¤UnifiedCacheManagerå˜é‡ä½œç”¨åŸŸé—®é¢˜")
    print("3. å¢å¼ºåˆå§‹åŒ–æ–¹æ³•çš„é”™è¯¯å¤„ç†")
    print("4. æ·»åŠ æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡")
    
    print(f"\nğŸ“‹ å¢å¼ºè¦ç‚¹:")
    print("- å‘é‡ç¼“å­˜ä½¿ç”¨ï¼ˆç¬¬ä¸€ä¸ªå…³é”®ä½ç½®ï¼‰")
    print("- è®°å¿†è®¿é—®è®°å½•ï¼ˆç¬¬äºŒä¸ªå…³é”®ä½ç½®ï¼‰") 
    print("- æŸ¥è¯¢ç»“æœç¼“å­˜ï¼ˆç¬¬ä¸‰ä¸ªå…³é”®ä½ç½®ï¼‰")
    print("- æ€§èƒ½ç»Ÿè®¡å’Œç›‘æ§")
    print("- å˜é‡ä½œç”¨åŸŸä¿®å¤")
    
    return True

def create_fix_test_script():
    """
    åˆ›å»ºä¿®å¤éªŒè¯æµ‹è¯•è„šæœ¬
    """
    print("\nğŸ”§ åˆ›å»ºä¿®å¤éªŒè¯æµ‹è¯•è„šæœ¬")
    print("=" * 60)
    
    test_script = '''
#!/usr/bin/env python3
"""
ç¼“å­˜ç³»ç»Ÿä¿®å¤éªŒè¯æµ‹è¯•è„šæœ¬
éªŒè¯ä¿®å¤æ•ˆæœå’Œæ€§èƒ½æå‡
"""

import sys
import os
import time
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

def test_keyword_cache_functionality():
    """æµ‹è¯•å…³é”®è¯ç¼“å­˜åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•å…³é”®è¯ç¼“å­˜åŠŸèƒ½...")
    
    try:
        from core.memory.shared.caching.keyword_cache import KeywordCache
        
        # åˆ›å»ºå…³é”®è¯ç¼“å­˜å®ä¾‹
        keyword_cache = KeywordCache()
        
        # æµ‹è¯•å…³é”®è¯æå–
        test_text = "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘æƒ³å‡ºå»æ•£æ­¥ï¼Œäº«å—é˜³å…‰"
        keywords = keyword_cache._extract_keywords(test_text)
        print(f"   å…³é”®è¯æå–: {keywords}")
        
        # æµ‹è¯•æ·»åŠ åˆ°ç¼“å­˜
        keyword_cache.add_to_keyword_cache("test_key_1", test_text, 5.0)
        keyword_cache.add_to_keyword_cache("test_key_2", "æ•£æ­¥æ˜¯å¾ˆå¥½çš„è¿åŠ¨", 3.0)
        
        # æµ‹è¯•æœç´¢
        search_results = keyword_cache.search_by_keywords("æ•£æ­¥", 5)
        print(f"   æœç´¢ç»“æœ: {search_results}")
        
        # æµ‹è¯•ç»Ÿè®¡
        stats = keyword_cache.get_keyword_stats()
        print(f"   ç»Ÿè®¡ä¿¡æ¯: {stats}")
        
        print("âœ… å…³é”®è¯ç¼“å­˜åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ å…³é”®è¯ç¼“å­˜åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_enhanced_cache_manager():
    """æµ‹è¯•å¢å¼ºç‰ˆç¼“å­˜ç®¡ç†å™¨"""
    print("ğŸ” æµ‹è¯•å¢å¼ºç‰ˆç¼“å­˜ç®¡ç†å™¨...")
    
    try:
        from core.memory.shared.caching.enhanced_cache_manager import EnhancedUnifiedCacheManager
        
        # åˆ›å»ºç®¡ç†å™¨å®ä¾‹
        cache_manager = EnhancedUnifiedCacheManager.get_instance()
        
        # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
        cache_manager.put("test_key", "test_value", text_content="è¿™æ˜¯æµ‹è¯•å†…å®¹", weight=5.0)
        value = cache_manager.get("test_key")
        print(f"   åŸºæœ¬ç¼“å­˜: {value}")
        
        # æµ‹è¯•å†…å®¹æœç´¢
        search_results = cache_manager.search_by_content("æµ‹è¯•å†…å®¹", 5)
        print(f"   å†…å®¹æœç´¢: {len(search_results)} ä¸ªç»“æœ")
        
        # æµ‹è¯•clearæ–¹æ³•
        cache_manager.clear()
        print("   ç¼“å­˜æ¸…ç†å®Œæˆ")
        
        # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
        stats = cache_manager.get_stats()
        print(f"   ç»Ÿè®¡ä¿¡æ¯: {stats['manager']['hit_ratio']:.2%}")
        
        print("âœ… å¢å¼ºç‰ˆç¼“å­˜ç®¡ç†å™¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ å¢å¼ºç‰ˆç¼“å­˜ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_integration_enhancement():
    """æµ‹è¯•é›†æˆæ·±åº¦å¢å¼º"""
    print("ğŸ” æµ‹è¯•é›†æˆæ·±åº¦å¢å¼º...")
    
    try:
        from core.memory.estia_memory_v5 import EstiaMemorySystem
        
        # åˆ›å»ºè®°å¿†ç³»ç»Ÿå®ä¾‹
        memory_system = EstiaMemorySystem()
        
        # æµ‹è¯•ç»Ÿä¸€ç¼“å­˜æ˜¯å¦æ­£ç¡®åˆå§‹åŒ–
        if hasattr(memory_system, 'unified_cache') and memory_system.unified_cache:
            print("   âœ… ç»Ÿä¸€ç¼“å­˜æ­£ç¡®åˆå§‹åŒ–")
        else:
            print("   âŒ ç»Ÿä¸€ç¼“å­˜æœªæ­£ç¡®åˆå§‹åŒ–")
            return False
        
        # æµ‹è¯•enhance_queryç¼“å­˜æ•ˆæœ
        test_query = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æŸ¥è¯¢"
        
        # ç¬¬ä¸€æ¬¡æŸ¥è¯¢
        start_time = time.time()
        result1 = memory_system.enhance_query(test_query)
        first_time = time.time() - start_time
        
        # ç¬¬äºŒæ¬¡æŸ¥è¯¢ï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰
        start_time = time.time()
        result2 = memory_system.enhance_query(test_query)
        second_time = time.time() - start_time
        
        print(f"   ç¬¬ä¸€æ¬¡æŸ¥è¯¢: {first_time:.4f}s")
        print(f"   ç¬¬äºŒæ¬¡æŸ¥è¯¢: {second_time:.4f}s")
        
        # æ£€æŸ¥åŠ é€Ÿæ¯”
        if first_time > 0 and second_time < first_time * 0.5:
            speedup = first_time / second_time if second_time > 0 else float('inf')
            print(f"   âœ… ç¼“å­˜åŠ é€Ÿæ¯”: {speedup:.2f}x")
        else:
            print("   âŒ ç¼“å­˜åŠ é€Ÿæ•ˆæœä¸æ˜æ˜¾")
            return False
        
        # æµ‹è¯•ç¼“å­˜ç»Ÿè®¡
        stats = memory_system.unified_cache.get_stats()
        print(f"   ç¼“å­˜å‘½ä¸­ç‡: {stats['manager']['hit_ratio']:.2%}")
        
        print("âœ… é›†æˆæ·±åº¦å¢å¼ºæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ é›†æˆæ·±åº¦å¢å¼ºæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ ç¼“å­˜ç³»ç»Ÿä¿®å¤éªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    test_results = {
        "keyword_cache": test_keyword_cache_functionality(),
        "enhanced_manager": test_enhanced_cache_manager(),
        "integration": test_integration_enhancement()
    }
    
    # è®¡ç®—æ€»ä½“æˆåŠŸç‡
    success_rate = sum(test_results.values()) / len(test_results)
    
    print("\\n" + "=" * 60)
    print("ğŸ“Š ä¿®å¤éªŒè¯ç»“æœ")
    print("=" * 60)
    
    for test_name, result in test_results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
    
    print(f"\\næ€»ä½“æˆåŠŸç‡: {success_rate:.2%}")
    
    if success_rate == 1.0:
        print("ğŸ‰ æ‰€æœ‰ä¿®å¤éªŒè¯é€šè¿‡ï¼")
    elif success_rate >= 0.8:
        print("âœ… å¤§éƒ¨åˆ†ä¿®å¤éªŒè¯é€šè¿‡ï¼Œå°‘æ•°é—®é¢˜å¾…è§£å†³")
    else:
        print("âŒ ä¿®å¤éªŒè¯å¤±è´¥è¾ƒå¤šï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")

if __name__ == "__main__":
    main()
'''
    
    # ä¿å­˜æµ‹è¯•è„šæœ¬
    test_script_file = "/mnt/d/Estia-AI/test_cache_fix_verification.py"
    
    try:
        with open(test_script_file, 'w', encoding='utf-8') as f:
            f.write(test_script)
        
        print(f"âœ… ä¿®å¤éªŒè¯æµ‹è¯•è„šæœ¬å·²ä¿å­˜: {test_script_file}")
        return True
        
    except Exception as e:
        print(f"âŒ ä¿®å¤éªŒè¯æµ‹è¯•è„šæœ¬ä¿å­˜å¤±è´¥: {e}")
        return False

def main():
    """ä¸»ä¿®å¤å‡½æ•°"""
    print("ğŸš€ Estia-AI ç¼“å­˜ç³»ç»Ÿä¿®å¤æ–¹æ¡ˆ")
    print("=" * 80)
    print(f"ä¿®å¤æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    # æ‰§è¡Œä¿®å¤æ­¥éª¤
    print("\nğŸ”§ å¼€å§‹æ‰§è¡Œä¿®å¤...")
    
    # ä¿®å¤1: UnifiedCacheManagerå˜é‡ä½œç”¨åŸŸé—®é¢˜
    fix1_success = fix_unified_cache_manager_scope()
    
    # ä¿®å¤2: å…³é”®è¯ç¼“å­˜åŠŸèƒ½æ¢å¤
    fix2_success = create_keyword_cache_implementation()
    
    # ä¿®å¤3: å¢å¼ºç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
    fix3_success = create_enhanced_cache_manager()
    
    # ä¿®å¤4: é›†æˆæ·±åº¦å¢å¼º
    fix4_success = create_integration_enhancement()
    
    # åˆ›å»ºéªŒè¯æµ‹è¯•è„šæœ¬
    test_success = create_fix_test_script()
    
    # ä¿®å¤ç»“æœç»Ÿè®¡
    fix_results = {
        "å˜é‡ä½œç”¨åŸŸä¿®å¤": fix1_success,
        "å…³é”®è¯ç¼“å­˜æ¢å¤": fix2_success,
        "ç¼“å­˜ç®¡ç†å™¨å¢å¼º": fix3_success,
        "é›†æˆæ·±åº¦å¢å¼º": fix4_success,
        "éªŒè¯æµ‹è¯•è„šæœ¬": test_success
    }
    
    success_count = sum(fix_results.values())
    total_count = len(fix_results)
    
    print("\n" + "=" * 80)
    print("ğŸ“Š ä¿®å¤æ–¹æ¡ˆæ‰§è¡Œç»“æœ")
    print("=" * 80)
    
    for fix_name, result in fix_results.items():
        status = "âœ… æˆåŠŸ" if result else "âŒ å¤±è´¥"
        print(f"{fix_name}: {status}")
    
    print(f"\næˆåŠŸç‡: {success_count}/{total_count} ({success_count/total_count:.2%})")
    
    if success_count == total_count:
        print("\nğŸ‰ æ‰€æœ‰ä¿®å¤æ–¹æ¡ˆæ‰§è¡ŒæˆåŠŸï¼")
        print("\nğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨:")
        print("1. æ‰‹åŠ¨åº”ç”¨ä¿®å¤ä»£ç åˆ°ç›¸åº”æ–‡ä»¶")
        print("2. è¿è¡Œ test_cache_fix_verification.py éªŒè¯ä¿®å¤æ•ˆæœ")
        print("3. é‡æ–°è¿è¡Œ test_cache_system_analysis.py å¯¹æ¯”ä¿®å¤å‰åçš„æ€§èƒ½")
    else:
        print("\nâŒ éƒ¨åˆ†ä¿®å¤æ–¹æ¡ˆæ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    
    print("\n" + "=" * 80)

if __name__ == "__main__":
    main()