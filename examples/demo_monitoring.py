#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®°å¿†ç›‘æ§ç³»ç»Ÿæ¼”ç¤ºè„šæœ¬
==================

å¿«é€Ÿæ¼”ç¤º13æ­¥è®°å¿†å¤„ç†æµç¨‹ç›‘æ§çš„åŠŸèƒ½å’Œæ•ˆæœã€‚
ç›´æ¥è¿è¡Œæ­¤è„šæœ¬å³å¯çœ‹åˆ°ç›‘æ§æ•°æ®çš„æ”¶é›†å’Œå±•ç¤ºã€‚
"""

import time
import random
import json
from datetime import datetime
from typing import List, Dict, Any

# å¯¼å…¥ç›‘æ§ç³»ç»Ÿ
from core.memory.monitoring import (
    MemoryPipelineMonitor,
    MemoryPipelineStep,
    StepStatus,
    MonitorAnalytics,
    monitor_step,
    StepMonitorContext
)


class DemoMemorySystem:
    """æ¼”ç¤ºç”¨çš„è®°å¿†ç³»ç»Ÿï¼Œæ¨¡æ‹ŸçœŸå®çš„å¤„ç†æµç¨‹"""
    
    def __init__(self):
        self.monitor = MemoryPipelineMonitor.get_instance()
        self.analytics = MonitorAnalytics(self.monitor)
        
        # æ¨¡æ‹Ÿæ•°æ®
        self.sample_queries = [
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
            "è¯·å¸®æˆ‘æ€»ç»“ä¸€ä¸‹å·¥ä½œè®¡åˆ’",
            "æˆ‘ä»¬æ¥èŠèŠäººå·¥æ™ºèƒ½çš„å‘å±•",
            "ä½ èƒ½æ¨èä¸€äº›å¥½çš„å­¦ä¹ èµ„æºå—ï¼Ÿ",
            "å¦‚ä½•æé«˜å·¥ä½œæ•ˆç‡ï¼Ÿ",
            "æœ€è¿‘æœ‰ä»€ä¹ˆæœ‰è¶£çš„ç§‘æŠ€æ–°é—»ï¼Ÿ",
            "å¸®æˆ‘åˆ†æä¸€ä¸‹è¿™ä¸ªé¡¹ç›®çš„é£é™©",
            "æˆ‘æƒ³äº†è§£ä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„åº”ç”¨"
        ]
        
        self.sample_memories = [
            "ç”¨æˆ·å–œæ¬¢è®¨è®ºæŠ€æœ¯è¯é¢˜",
            "å·¥ä½œä¸­é‡åˆ°é¡¹ç›®ç®¡ç†é—®é¢˜",
            "å¯¹AIå‘å±•å¾ˆæ„Ÿå…´è¶£",
            "ç»å¸¸è¯¢é—®å­¦ä¹ å»ºè®®",
            "æ³¨é‡å·¥ä½œæ•ˆç‡æå‡",
            "å…³æ³¨ç§‘æŠ€è¡Œä¸šåŠ¨æ€"
        ]
        
        print("ğŸš€ æ¼”ç¤ºè®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    @monitor_step(MemoryPipelineStep.STEP_4_CACHE_VECTORIZE, 
                  capture_input=True, capture_output=True)
    def vectorize_query(self, user_input: str) -> List[float]:
        """Step 4: æ¨¡æ‹Ÿå‘é‡åŒ–è¿‡ç¨‹"""
        print(f"  ğŸ”„ æ­£åœ¨å‘é‡åŒ–: {user_input[:30]}...")
        
        # æ¨¡æ‹Ÿå‘é‡åŒ–æ—¶é—´
        time.sleep(random.uniform(0.05, 0.15))
        
        # è¿”å›æ¨¡æ‹Ÿå‘é‡
        vector = [random.random() for _ in range(512)]
        print(f"  âœ… å‘é‡åŒ–å®Œæˆï¼Œç»´åº¦: {len(vector)}")
        return vector
    
    def faiss_search(self, query_vector: List[float], k: int = 5) -> List[Dict[str, Any]]:
        """Step 5: æ¨¡æ‹ŸFAISSæ£€ç´¢"""
        with StepMonitorContext(
            MemoryPipelineStep.STEP_5_FAISS_SEARCH,
            input_data={"vector_dim": len(query_vector), "k": k}
        ) as ctx:
            
            print(f"  ğŸ” FAISSæ£€ç´¢ Top-{k} ç›¸ä¼¼è®°å¿†...")
            
            # æ¨¡æ‹Ÿæ£€ç´¢æ—¶é—´
            time.sleep(random.uniform(0.03, 0.08))
            
            # æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ
            results = []
            for i in range(k):
                memory = random.choice(self.sample_memories)
                similarity = random.uniform(0.6, 0.95)
                results.append({
                    "memory_id": f"mem_{i+1}",
                    "content": memory,
                    "similarity": similarity,
                    "timestamp": datetime.now().isoformat()
                })
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            results.sort(key=lambda x: x["similarity"], reverse=True)
            
            # è®¾ç½®ç›‘æ§æ•°æ®
            ctx.set_output(results)
            ctx.set_metadata({
                "result_count": len(results),
                "avg_similarity": sum(r["similarity"] for r in results) / len(results),
                "max_similarity": max(r["similarity"] for r in results),
                "search_threshold": 0.5
            })
            
            print(f"  âœ… æ‰¾åˆ° {len(results)} æ¡ç›¸å…³è®°å¿†")
            return results
    
    def expand_associations(self, search_results: List[Dict[str, Any]]) -> List[str]:
        """Step 6: æ¨¡æ‹Ÿå…³è”æ‹“å±•"""
        self.monitor.start_step(MemoryPipelineStep.STEP_6_ASSOCIATION_EXPAND)
        
        try:
            print("  ğŸ”— æ‹“å±•å…³è”ç½‘ç»œ...")
            time.sleep(random.uniform(0.02, 0.06))
            
            # æ¨¡æ‹Ÿå…³è”æ‹“å±•
            associations = []
            for result in search_results[:3]:  # å–å‰3ä¸ªç»“æœè¿›è¡Œæ‹“å±•
                associations.extend([
                    f"å…³è”_{result['memory_id']}_1",
                    f"å…³è”_{result['memory_id']}_2"
                ])
            
            self.monitor.finish_step(
                MemoryPipelineStep.STEP_6_ASSOCIATION_EXPAND,
                status=StepStatus.SUCCESS,
                output_data=associations,
                metadata={
                    "expansion_count": len(associations),
                    "source_count": len(search_results)
                }
            )
            
            print(f"  âœ… æ‹“å±•å¾—åˆ° {len(associations)} ä¸ªå…³è”")
            return associations
            
        except Exception as e:
            self.monitor.finish_step(
                MemoryPipelineStep.STEP_6_ASSOCIATION_EXPAND,
                status=StepStatus.FAILED,
                error=e
            )
            raise
    
    def build_context(self, memories: List[Dict[str, Any]], 
                     associations: List[str], user_input: str) -> str:
        """Step 9: æ¨¡æ‹Ÿä¸Šä¸‹æ–‡æ„å»º"""
        with StepMonitorContext(
            MemoryPipelineStep.STEP_9_CONTEXT_BUILD,
            input_data={
                "memory_count": len(memories),
                "association_count": len(associations),
                "query_length": len(user_input)
            }
        ) as ctx:
            
            print("  ğŸ“ æ„å»ºå¢å¼ºä¸Šä¸‹æ–‡...")
            time.sleep(random.uniform(0.01, 0.04))
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context_parts = [
                f"ç”¨æˆ·æŸ¥è¯¢: {user_input}",
                f"ç›¸å…³è®°å¿† ({len(memories)}æ¡):",
            ]
            
            for memory in memories[:3]:  # æ˜¾ç¤ºå‰3æ¡è®°å¿†
                context_parts.append(f"  - {memory['content']} (ç›¸ä¼¼åº¦: {memory['similarity']:.2f})")
            
            if associations:
                context_parts.append(f"å…³è”ä¿¡æ¯ ({len(associations)}æ¡): {', '.join(associations[:3])}")
            
            enhanced_context = "\n".join(context_parts)
            
            # è®¾ç½®ç›‘æ§æ•°æ®
            ctx.set_output(enhanced_context)
            ctx.set_metadata({
                "context_length": len(enhanced_context),
                "memory_used": len(memories),
                "associations_used": len(associations),
                "enhancement_ratio": len(enhanced_context) / len(user_input)
            })
            
            print(f"  âœ… ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆï¼Œé•¿åº¦: {len(enhanced_context)} å­—ç¬¦")
            return enhanced_context
    
    def process_query(self, user_input: str) -> str:
        """å®Œæ•´çš„æŸ¥è¯¢å¤„ç†æµç¨‹"""
        print(f"\nğŸ¯ å¼€å§‹å¤„ç†æŸ¥è¯¢: {user_input}")
        print("="*60)
        
        # å¼€å§‹ç›‘æ§ä¼šè¯
        session_id = self.monitor.start_session(user_input=user_input)
        
        try:
            # Step 4: å‘é‡åŒ–
            query_vector = self.vectorize_query(user_input)
            
            # Step 5: FAISSæ£€ç´¢
            search_results = self.faiss_search(query_vector)
            
            # Step 6: å…³è”æ‹“å±•
            associations = self.expand_associations(search_results)
            
            # Step 9: æ„å»ºä¸Šä¸‹æ–‡
            enhanced_context = self.build_context(search_results, associations, user_input)
            
            print("\nğŸ’¡ å¤„ç†å®Œæˆï¼")
            return enhanced_context
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            return f"å¤„ç†å¤±è´¥: {str(e)}"
        finally:
            # å®Œæˆä¼šè¯
            self.monitor.finish_session(session_id)
    
    def print_live_monitoring(self):
        """æ‰“å°å®æ—¶ç›‘æ§ä¿¡æ¯"""
        status = self.analytics.get_real_time_status()
        
        print("\n" + "="*60)
        print("ğŸ“Š å®æ—¶ç›‘æ§çŠ¶æ€")
        print("="*60)
        
        if status.get('status') == 'running':
            print(f"ğŸŸ¢ çŠ¶æ€: è¿è¡Œä¸­")
            print(f"ğŸ“‹ ä¼šè¯ID: {status.get('session_id')}")
            print(f"â±ï¸  è¿è¡Œæ—¶é—´: {status.get('running_time', 0):.2f}s")
            print(f"ğŸ“ˆ è¿›åº¦: {status.get('progress_percentage', 0):.1f}%")
            print(f"ğŸ¯ å½“å‰é˜¶æ®µ: {status.get('current_phase')}")
            print(f"âš™ï¸  å½“å‰æ­¥éª¤: {status.get('current_step')}")
        else:
            print(f"âšª çŠ¶æ€: {status.get('status', 'idle')}")
            print(f"ğŸ’¬ ä¿¡æ¯: {status.get('message', 'æ— æ´»è·ƒæµç¨‹')}")
    
    def print_performance_summary(self):
        """æ‰“å°æ€§èƒ½æ‘˜è¦"""
        summary = self.monitor.get_performance_summary()
        
        print("\n" + "="*60)
        print("ğŸ“ˆ æ€§èƒ½æ‘˜è¦æŠ¥å‘Š")
        print("="*60)
        
        if summary.get('total_sessions', 0) > 0:
            print(f"ğŸ“Š æ€»ä¼šè¯æ•°: {summary.get('total_sessions')}")
            print(f"â±ï¸  å¹³å‡è€—æ—¶: {summary.get('average_duration', 0):.3f}s")
            print(f"âœ… æˆåŠŸç‡: {summary.get('success_rate', 0)*100:.1f}%")
            
            # æœ€æ…¢çš„æ­¥éª¤
            slowest = summary.get('slowest_step', {})
            if slowest.get('step'):
                print(f"ğŸŒ æœ€æ…¢æ­¥éª¤: {slowest['step']} ({slowest.get('avg_duration', 0):.3f}s)")
            
            # æœ€å¿«çš„æ­¥éª¤
            fastest = summary.get('fastest_step', {})
            if fastest.get('step'):
                print(f"âš¡ æœ€å¿«æ­¥éª¤: {fastest['step']} ({fastest.get('avg_duration', 0):.3f}s)")
                
        else:
            print("ğŸ“­ æš‚æ— å†å²æ•°æ®")
    
    def print_detailed_analytics(self):
        """æ‰“å°è¯¦ç»†åˆ†ææŠ¥å‘Š"""
        if len(self.monitor.completed_sessions) == 0:
            print("\nâš ï¸ æš‚æ— è¶³å¤Ÿæ•°æ®è¿›è¡Œè¯¦ç»†åˆ†æ")
            return
            
        print("\n" + "="*60)
        print("ğŸ” è¯¦ç»†æ€§èƒ½åˆ†æ")
        print("="*60)
        
        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        report = self.analytics.generate_performance_report()
        
        print(f"ğŸ“Š åˆ†æäº† {report.total_sessions} ä¸ªä¼šè¯")
        print(f"â±ï¸  æ€»è€—æ—¶: {report.total_duration:.3f}s")
        print(f"ğŸ“ˆ å¹³å‡è€—æ—¶: {report.average_duration:.3f}s")
        print(f"âœ… æˆåŠŸç‡: {report.success_rate*100:.1f}%")
        
        # æœ€æ…¢çš„æ­¥éª¤Top3
        if report.slowest_steps:
            print("\nğŸŒ æœ€è€—æ—¶æ­¥éª¤ (Top 3):")
            for i, (step, duration) in enumerate(report.slowest_steps[:3], 1):
                print(f"  {i}. {step}: {duration:.3f}s")
        
        # å¤±è´¥ç‡åˆ†æ
        high_failure_steps = [(step, rate) for step, rate in report.failure_rates.items() if rate > 0]
        if high_failure_steps:
            print("\nâš ï¸ å­˜åœ¨å¤±è´¥çš„æ­¥éª¤:")
            for step, rate in high_failure_steps:
                print(f"  - {step}: {rate*100:.1f}% å¤±è´¥ç‡")
        
        # ä¼˜åŒ–å»ºè®®
        if report.recommendations:
            print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for i, rec in enumerate(report.recommendations, 1):
                print(f"  {i}. {rec}")
        
        # ç“¶é¢ˆåˆ†æ
        bottlenecks = self.analytics.analyze_bottlenecks()
        if bottlenecks.primary_bottleneck:
            print(f"\nğŸ¯ ä¸»è¦ç“¶é¢ˆ: {bottlenecks.primary_bottleneck}")
            print(f"ğŸ“Š å½±å“ç¨‹åº¦: {bottlenecks.bottleneck_impact:.1f}%")
            
            if bottlenecks.bottleneck_steps:
                print("\nğŸ”§ ç“¶é¢ˆæ­¥éª¤è¯¦æƒ…:")
                for step, duration, impact in bottlenecks.bottleneck_steps:
                    print(f"  - {step}: {duration:.3f}s ({impact})")


def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ‰ æ¬¢è¿ä½¿ç”¨ Estia è®°å¿†ç›‘æ§ç³»ç»Ÿæ¼”ç¤ºï¼")
    print("="*60)
    print("è¿™ä¸ªæ¼”ç¤ºå°†å±•ç¤º13æ­¥è®°å¿†å¤„ç†æµç¨‹çš„ç›‘æ§åŠŸèƒ½")
    print("åŒ…æ‹¬å®æ—¶ç›‘æ§ã€æ€§èƒ½åˆ†æå’Œè¯¦ç»†æŠ¥å‘Š\n")
    
    # åˆ›å»ºæ¼”ç¤ºç³»ç»Ÿ
    demo_system = DemoMemorySystem()
    
    # å¤„ç†å¤šä¸ªæŸ¥è¯¢ä»¥ç”Ÿæˆç›‘æ§æ•°æ®
    print("ğŸ”„ å¼€å§‹å¤„ç†æ¼”ç¤ºæŸ¥è¯¢...")
    
    for i, query in enumerate(demo_system.sample_queries[:4], 1):
        print(f"\nç¬¬ {i} æ¬¡æŸ¥è¯¢ (å…±4æ¬¡)")
        result = demo_system.process_query(query)
        
        # æ˜¾ç¤ºå®æ—¶ç›‘æ§ï¼ˆåœ¨å¤„ç†è¿‡ç¨‹ä¸­è¿™ä¼šæ˜¾ç¤ºè¿è¡ŒçŠ¶æ€ï¼‰
        time.sleep(0.1)  # ç¨ä½œå»¶è¿Ÿä»¥ä¾¿è§‚å¯Ÿ
    
    # æ˜¾ç¤ºæœ€ç»ˆçš„ç›‘æ§æŠ¥å‘Š
    demo_system.print_performance_summary()
    demo_system.print_detailed_analytics()
    
    print(f"\n" + "="*60)
    print("ğŸ¯ æ¼”ç¤ºå®Œæˆï¼")
    print("="*60)
    print("ğŸ’¡ æ¥ä¸‹æ¥å¯ä»¥:")
    print("  1. æŸ¥çœ‹ç”Ÿæˆçš„ç›‘æ§æ•°æ®")
    print("  2. è¿è¡Œ Web å¯è§†åŒ–ç•Œé¢ (ç¨åæä¾›)")
    print("  3. é›†æˆåˆ°å®é™…çš„ EstiaMemorySystem ä¸­")
    print("  4. è¿è¡Œå®Œæ•´æµ‹è¯•: python tests/test_memory_monitoring.py")
    
    # è¿”å›ç›‘æ§æ•°æ®ä¾›è¿›ä¸€æ­¥åˆ†æ
    return demo_system.monitor, demo_system.analytics


if __name__ == "__main__":
    main() 