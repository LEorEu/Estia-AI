#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Estia è®°å¿†ç›‘æ§ Web ä»ªè¡¨æ¿
========================

åŸºäºFlaskçš„å®æ—¶ç›‘æ§å¯è§†åŒ–ç•Œé¢ï¼ŒåŒ…å«ï¼š
- å®æ—¶æµç¨‹ç›‘æ§
- æ€§èƒ½å›¾è¡¨å’Œåˆ†æ
- å…³é”®è¯äº‘å’Œè¶‹åŠ¿åˆ†æ
- è®°å¿†å†…å®¹å¯è§†åŒ–
"""

import json
import time
import re
from datetime import datetime, timedelta
from collections import Counter, defaultdict
from typing import Dict, List, Any, Optional
from functools import lru_cache
import asyncio
from concurrent.futures import ThreadPoolExecutor

from flask import Flask, render_template, jsonify, request, send_from_directory
from flask_socketio import SocketIO, emit
import threading

# å¯¼å…¥ç›‘æ§ç³»ç»Ÿ
from core.memory.managers.monitor_flow.monitoring import (
    MemoryPipelineMonitor,
    MemoryPipelineStep,
    StepStatus,
    MonitorAnalytics
)

# å¯¼å…¥å®æ—¶æ•°æ®è¿æ¥å™¨
from .live_data_connector import live_connector

app = Flask(__name__, template_folder='../templates')
app.config['SECRET_KEY'] = 'estia_monitoring_secret'
socketio = SocketIO(app, cors_allowed_origins="*")

# å…¨å±€ç›‘æ§å®ä¾‹
try:
    monitor = MemoryPipelineMonitor.get_instance()
    analytics = MonitorAnalytics(monitor)
    print("âœ… ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")

    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
    session_count = len(monitor.completed_sessions)
    print(f"ğŸ“Š å½“å‰ä¼šè¯æ•°é‡: {session_count}")

    if session_count == 0:
        print("âš ï¸ æš‚æ— ç›‘æ§æ•°æ®ï¼Œå°†æ˜¾ç¤ºç©ºçŠ¶æ€")

except Exception as e:
    print(f"âŒ ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
    # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„ç›‘æ§å™¨ç”¨äºæµ‹è¯•
    class MockMonitor:
        def __init__(self):
            self.completed_sessions = []

        def get_performance_summary(self):
            return {
                'total_sessions': 0,
                'average_duration': 0.0,
                'success_rate': 0.0,
                'slowest_step': None
            }

    class MockAnalytics:
        def __init__(self, monitor):
            self.monitor = monitor

        def get_real_time_status(self):
            return {
                'status': 'idle',
                'session_id': None,
                'running_time': 0,
                'progress_percentage': 0
            }

        def generate_performance_report(self):
            from dataclasses import dataclass
            @dataclass
            class MockReport:
                total_sessions: int = 0
                avg_duration: float = 0.0
                success_rate: float = 0.0
            return MockReport()

        def analyze_bottlenecks(self):
            from dataclasses import dataclass
            @dataclass
            class MockBottlenecks:
                slowest_steps: list = None
                avg_bottleneck_time: float = 0.0
            return MockBottlenecks()

    monitor = MockMonitor()
    analytics = MockAnalytics(monitor)
    print("ğŸ”„ ä½¿ç”¨æ¨¡æ‹Ÿç›‘æ§å™¨")

# å°è¯•è¿æ¥v6.0çš„MemoryFlowMonitorï¼ˆå¢å¼ºé”™è¯¯å¤„ç†ï¼‰
flow_monitor = None
flow_monitor_error = None

try:
    from core.memory.managers.monitor_flow import MemoryFlowMonitor

    # åˆ›å»ºæ¨¡æ‹Ÿçš„ç»„ä»¶å­—å…¸æ¥åˆå§‹åŒ–MemoryFlowMonitor
    # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™äº›ç»„ä»¶åº”è¯¥æ¥è‡ªçœŸå®çš„v6.0ç³»ç»Ÿ
    mock_components = {
        'db_manager': None,  # æ•°æ®åº“ç®¡ç†å™¨
        'unified_cache': None,  # ç»Ÿä¸€ç¼“å­˜
        'sync_flow_manager': None,  # åŒæ­¥æµç¨‹ç®¡ç†å™¨
        'async_flow_manager': None  # å¼‚æ­¥æµç¨‹ç®¡ç†å™¨
    }

    flow_monitor = MemoryFlowMonitor(mock_components)
    print("âœ… v6.0 MemoryFlowMonitor åˆå§‹åŒ–æˆåŠŸ")

    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    try:
        test_stats = flow_monitor.get_comprehensive_stats()
        if 'error' in test_stats:
            print(f"âš ï¸ v6.0ç›‘æ§å™¨åŠŸèƒ½æµ‹è¯•å¤±è´¥: {test_stats['error']}")
        else:
            print("âœ… v6.0ç›‘æ§å™¨åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    except Exception as test_error:
        print(f"âš ï¸ v6.0ç›‘æ§å™¨åŠŸèƒ½æµ‹è¯•å¼‚å¸¸: {test_error}")

except ImportError as e:
    flow_monitor_error = f"å¯¼å…¥å¤±è´¥: {e}"
    print(f"âš ï¸ v6.0 MemoryFlowMonitor å¯¼å…¥å¤±è´¥: {e}")
except Exception as e:
    flow_monitor_error = f"åˆå§‹åŒ–å¤±è´¥: {e}"
    print(f"âš ï¸ v6.0 MemoryFlowMonitor åˆå§‹åŒ–å¤±è´¥: {e}")


class FallbackMonitor:
    """é™çº§ç›‘æ§å™¨ï¼Œå½“v6.0ç›‘æ§å™¨ä¸å¯ç”¨æ—¶ä½¿ç”¨"""

    def __init__(self):
        self.start_time = time.time()

    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """æä¾›åŸºç¡€çš„ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'timestamp': time.time(),
            'monitor_status': 'fallback',
            'performance_summary': {
                'cache_hit_rate': 0.0,
                'cache_efficiency': 0.0,
                'system_health': 'unknown'
            },
            'memory_overview': {
                'total_memories': 0,
                'active_sessions': 0
            },
            'session_statistics': {
                'total_sessions': 0,
                'avg_duration': 0.0
            },
            'health_status': {
                'status': 'degraded',
                'message': 'v6.0ç›‘æ§å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é™çº§æ¨¡å¼'
            },
            'error': flow_monitor_error
        }

    def get_13_step_monitoring(self) -> Dict[str, Any]:
        """æä¾›åŸºç¡€çš„æ­¥éª¤ç›‘æ§ä¿¡æ¯"""
        return {
            'error': 'v6.0ç›‘æ§å™¨ä¸å¯ç”¨',
            'fallback_mode': True,
            'message': '14æ­¥æµç¨‹ç›‘æ§éœ€è¦v6.0ç›‘æ§å™¨æ”¯æŒ'
        }

    def get_real_time_metrics(self) -> Dict[str, Any]:
        """æä¾›åŸºç¡€çš„å®æ—¶æŒ‡æ ‡"""
        return {
            'timestamp': time.time(),
            'cache_performance': {'status': 'unavailable'},
            'database_performance': {'status': 'unavailable'},
            'queue_status': {'status': 'unavailable'},
            'memory_usage': {'status': 'unavailable'},
            'error': 'v6.0ç›‘æ§å™¨ä¸å¯ç”¨'
        }


# å¦‚æœv6.0ç›‘æ§å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é™çº§ç›‘æ§å™¨
if flow_monitor is None:
    flow_monitor = FallbackMonitor()
    print("ğŸ”„ å¯ç”¨é™çº§ç›‘æ§æ¨¡å¼")


class KeywordAnalyzer:
    """å…³é”®è¯åˆ†æå™¨"""
    
    def __init__(self):
        # ä¸­æ–‡åœç”¨è¯
        self.stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
            'è‡ªå·±', 'è¿™', 'é‚£', 'ä»€ä¹ˆ', 'æˆ‘ä»¬', 'ä»–ä»¬', 'å¥¹ä»¬', 'å®ƒä»¬', 'è¿™ä¸ª', 'é‚£ä¸ª',
            'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•', 'å—', 'å‘¢', 'å§', 'å•Š', 'å‘€'
        }
        
        # è‹±æ–‡åœç”¨è¯
        self.stop_words.update({
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have',
            'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should', 'could',
            'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we',
            'they', 'me', 'him', 'her', 'us', 'them'
        })
    
    def extract_keywords(self, text: str, min_length: int = 2) -> List[str]:
        """æå–å…³é”®è¯"""
        if not text:
            return []
        
        # æ¸…ç†æ–‡æœ¬
        text = text.lower()
        # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡å’Œæ•°å­—
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', ' ', text)
        
        # åˆ†è¯ï¼ˆç®€å•çš„åŸºäºç©ºæ ¼å’Œæ ‡ç‚¹çš„åˆ†è¯ï¼‰
        words = text.split()
        
        # è¿‡æ»¤å…³é”®è¯
        keywords = []
        for word in words:
            word = word.strip()
            if (len(word) >= min_length and 
                word not in self.stop_words and
                not word.isdigit()):
                keywords.append(word)
        
        return keywords
    
    def analyze_keyword_trends(self, sessions: List) -> Dict[str, Any]:
        """åˆ†æå…³é”®è¯è¶‹åŠ¿"""
        keyword_counts = Counter()
        time_series = defaultdict(list)
        
        for session in sessions:
            if hasattr(session, 'user_input') and session.user_input:
                keywords = self.extract_keywords(session.user_input)
                
                for keyword in keywords:
                    keyword_counts[keyword] += 1
                    time_series[keyword].append(session.start_time)
            
            # åˆ†æAIå›å¤ä¸­çš„å…³é”®è¯
            if hasattr(session, 'ai_response') and session.ai_response:
                response_keywords = self.extract_keywords(session.ai_response)
                for keyword in response_keywords[:5]:  # åªå–å‰5ä¸ªé¿å…è¿‡å¤š
                    keyword_counts[f"å›å¤_{keyword}"] += 1
        
        # è®¡ç®—è¶‹åŠ¿
        trending_keywords = []
        for keyword, count in keyword_counts.most_common(20):
            if count >= 2:  # è‡³å°‘å‡ºç°2æ¬¡
                trending_keywords.append({
                    'word': keyword,
                    'count': count,
                    'frequency': count / len(sessions) if sessions else 0
                })
        
        return {
            'top_keywords': trending_keywords,
            'total_unique_keywords': len(keyword_counts),
            'keyword_distribution': dict(keyword_counts.most_common(10))
        }


class V6DataAdapter:
    """v6.0æ•°æ®é€‚é…å™¨ï¼Œå°†v6.0ç›‘æ§æ•°æ®è½¬æ¢ä¸ºä»ªè¡¨æ¿æœŸæœ›çš„æ ¼å¼ï¼ˆå¢å¼ºç‰ˆï¼‰"""

    def __init__(self, flow_monitor):
        self.flow_monitor = flow_monitor
        self.is_fallback_mode = isinstance(flow_monitor, FallbackMonitor)

    def adapt_comprehensive_stats(self) -> Dict[str, Any]:
        """é€‚é…ç»¼åˆç»Ÿè®¡æ•°æ®ï¼ˆå¢å¼ºé”™è¯¯å¤„ç†ï¼‰"""
        if not self.flow_monitor:
            return {'error': 'v6.0ç›‘æ§å™¨ä¸å¯ç”¨'}

        try:
            stats = self.flow_monitor.get_comprehensive_stats()

            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯å“åº”
            if 'error' in stats:
                return {
                    'error': stats['error'],
                    'fallback_mode': self.is_fallback_mode,
                    'timestamp': stats.get('timestamp', time.time())
                }

            # è½¬æ¢ä¸ºä»ªè¡¨æ¿æœŸæœ›çš„æ ¼å¼
            adapted_stats = {
                'timestamp': stats.get('timestamp', time.time()),
                'monitor_status': stats.get('monitor_status', 'unknown'),
                'fallback_mode': self.is_fallback_mode,
                'performance_summary': {
                    'cache_hit_rate': stats.get('performance_metrics', {}).get('cache_hit_rate', 0),
                    'cache_efficiency': stats.get('performance_metrics', {}).get('cache_efficiency', 0),
                    'system_health': stats.get('health_status', {}).get('status', 'unknown')
                },
                'memory_overview': stats.get('memory_overview', {}),
                'session_stats': stats.get('session_statistics', {})
            }

            # å¦‚æœæ˜¯é™çº§æ¨¡å¼ï¼Œæ·»åŠ è­¦å‘Šä¿¡æ¯
            if self.is_fallback_mode:
                adapted_stats['warning'] = 'v6.0ç›‘æ§å™¨ä¸å¯ç”¨ï¼Œæ˜¾ç¤ºé™çº§æ•°æ®'

            return adapted_stats

        except Exception as e:
            return {
                'error': f'é€‚é…ç»¼åˆç»Ÿè®¡å¤±è´¥: {str(e)}',
                'fallback_mode': self.is_fallback_mode,
                'timestamp': time.time()
            }
    
    def adapt_13_step_monitoring(self) -> Dict[str, Any]:
        """é€‚é…13æ­¥æµç¨‹ç›‘æ§æ•°æ®ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        if not self.flow_monitor:
            return {'error': 'v6.0ç›‘æ§å™¨ä¸å¯ç”¨'}

        try:
            step_data = self.flow_monitor.get_13_step_monitoring()

            # è½¬æ¢ä¸ºä»ªè¡¨æ¿æœŸæœ›çš„æ ¼å¼
            if 'error' in step_data:
                return step_data

            # ä¿®æ­£æ­¥éª¤æ•°é‡ - å®é™…æ˜¯14æ­¥
            adapted_data = {
                'total_steps': step_data.get('total_steps', 14),
                'sync_steps': step_data.get('sync_steps', 9),  # Step 1-9
                'async_steps': step_data.get('async_steps', 5),  # Step 10-14
                'step_performance': step_data.get('overall_performance', {}),
                'step_details': step_data.get('step_details', {}),
                'timestamp': step_data.get('timestamp', 0)
            }

            # æ·»åŠ æ­¥éª¤åç§°æ˜ å°„
            step_mapping = {
                'step_01_database_initialization': 'DBåˆå§‹åŒ–',
                'step_02_component_initialization': 'ç»„ä»¶åˆå§‹åŒ–',
                'step_03_async_evaluator_initialization': 'å¼‚æ­¥è¯„ä¼°å™¨åˆå§‹åŒ–',
                'step_04_unified_cache_vectorization': 'ç¼“å­˜å‘é‡åŒ–',
                'step_05_faiss_vector_retrieval': 'FAISSæ£€ç´¢',
                'step_06_association_network_expansion': 'å…³è”æ‹“å±•',
                'step_07_history_dialogue_aggregation': 'å†å²èšåˆ',
                'step_08_weight_ranking_deduplication': 'æƒé‡æ’åº',
                'step_09_final_context_assembly': 'ä¸Šä¸‹æ–‡æ„å»º',
                'step_10_llm_response_generation': 'LLMç”Ÿæˆ',
                'step_11_immediate_dialogue_storage': 'å¯¹è¯å­˜å‚¨',
                'step_12_async_llm_evaluation': 'å¼‚æ­¥è¯„ä¼°',
                'step_13_save_evaluation_results': 'ä¿å­˜ç»“æœ',
                'step_14_auto_association_creation': 'å…³è”åˆ›å»º'
            }

            adapted_data['step_mapping'] = step_mapping

            return adapted_data

        except Exception as e:
            return {'error': f'é€‚é…13æ­¥ç›‘æ§å¤±è´¥: {str(e)}'}
    
    def adapt_real_time_metrics(self) -> Dict[str, Any]:
        """é€‚é…å®æ—¶æ€§èƒ½æŒ‡æ ‡"""
        if not self.flow_monitor:
            return {'error': 'v6.0ç›‘æ§å™¨ä¸å¯ç”¨'}
        
        try:
            metrics = self.flow_monitor.get_real_time_metrics()
            
            # è½¬æ¢ä¸ºä»ªè¡¨æ¿æœŸæœ›çš„æ ¼å¼
            if 'error' in metrics:
                return metrics
            
            adapted_metrics = {
                'cache_performance': metrics.get('cache_performance', {}),
                'database_performance': metrics.get('database_performance', {}),
                'queue_status': metrics.get('queue_status', {}),
                'memory_usage': metrics.get('memory_usage', {}),
                'timestamp': metrics.get('timestamp', 0)
            }
            
            return adapted_metrics
            
        except Exception as e:
            return {'error': f'é€‚é…å®æ—¶æŒ‡æ ‡å¤±è´¥: {str(e)}'}


class MemoryContentAnalyzer:
    """è®°å¿†å†…å®¹åˆ†æå™¨"""
    
    def analyze_memory_patterns(self, sessions: List) -> Dict[str, Any]:
        """åˆ†æè®°å¿†æ¨¡å¼"""
        memory_types = Counter()
        similarity_scores = []
        memory_usage = defaultdict(int)
        
        for session in sessions:
            for step, metrics in session.steps.items():
                if step == MemoryPipelineStep.STEP_5_FAISS_SEARCH:
                    # åˆ†ææ£€ç´¢ç»“æœ
                    if 'avg_similarity' in metrics.metadata:
                        similarity_scores.append(metrics.metadata['avg_similarity'])
                    
                    if 'result_count' in metrics.metadata:
                        memory_usage['retrieved'] += metrics.metadata['result_count']
                
                elif step == MemoryPipelineStep.STEP_6_ASSOCIATION_EXPAND:
                    # åˆ†æå…³è”æ‹“å±•
                    if 'expansion_count' in metrics.metadata:
                        memory_usage['associations'] += metrics.metadata['expansion_count']
                
                elif step == MemoryPipelineStep.STEP_9_CONTEXT_BUILD:
                    # åˆ†æä¸Šä¸‹æ–‡æ„å»º
                    if 'memory_used' in metrics.metadata:
                        memory_usage['context_memories'] += metrics.metadata['memory_used']
        
        avg_similarity = sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0
        
        return {
            'average_similarity': avg_similarity,
            'memory_usage_stats': dict(memory_usage),
            'total_retrievals': len(similarity_scores),
            'similarity_distribution': self._calculate_similarity_distribution(similarity_scores)
        }
    
    def _calculate_similarity_distribution(self, scores: List[float]) -> Dict[str, int]:
        """è®¡ç®—ç›¸ä¼¼åº¦åˆ†å¸ƒ"""
        if not scores:
            return {}
        
        bins = {'é«˜ (>0.8)': 0, 'ä¸­ (0.6-0.8)': 0, 'ä½ (<0.6)': 0}
        
        for score in scores:
            if score > 0.8:
                bins['é«˜ (>0.8)'] += 1
            elif score > 0.6:
                bins['ä¸­ (0.6-0.8)'] += 1
            else:
                bins['ä½ (<0.6)'] += 1
        
        return bins


# åˆå§‹åŒ–åˆ†æå™¨
keyword_analyzer = KeywordAnalyzer()
memory_analyzer = MemoryContentAnalyzer()

# åˆå§‹åŒ–v6.0æ•°æ®é€‚é…å™¨
v6_adapter = V6DataAdapter(flow_monitor) if flow_monitor else None


class DataCache:
    """æ•°æ®ç¼“å­˜ç®¡ç†å™¨ï¼Œå‡å°‘é‡å¤è®¡ç®—å’ŒAPIè°ƒç”¨"""

    def __init__(self, cache_ttl: int = 3):
        self.cache_ttl = cache_ttl  # ç¼“å­˜ç”Ÿå­˜æ—¶é—´ï¼ˆç§’ï¼‰
        self._cache = {}
        self._timestamps = {}

    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        if key not in self._cache:
            return None

        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        if time.time() - self._timestamps[key] > self.cache_ttl:
            del self._cache[key]
            del self._timestamps[key]
            return None

        return self._cache[key]

    def set(self, key: str, value: Any) -> None:
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        self._cache[key] = value
        self._timestamps[key] = time.time()

    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
        self._timestamps.clear()


class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.data_cache = DataCache()
        self.last_session_count = 0
        self.last_update_time = 0

    def should_update_data(self, data_type: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°æ•°æ®"""
        current_session_count = len(monitor.completed_sessions)
        current_time = time.time()

        # å¦‚æœä¼šè¯æ•°é‡æ²¡æœ‰å˜åŒ–ä¸”è·ç¦»ä¸Šæ¬¡æ›´æ–°ä¸åˆ°3ç§’ï¼Œè·³è¿‡æ›´æ–°
        if (data_type in ['sessions', 'keywords', 'memory'] and
            current_session_count == self.last_session_count and
            current_time - self.last_update_time < 3):
            return False

        return True

    def update_session_tracking(self):
        """æ›´æ–°ä¼šè¯è·Ÿè¸ªä¿¡æ¯"""
        self.last_session_count = len(monitor.completed_sessions)
        self.last_update_time = time.time()

    async def get_cached_or_compute(self, key: str, compute_func, *args, **kwargs):
        """è·å–ç¼“å­˜æ•°æ®æˆ–è®¡ç®—æ–°æ•°æ®"""
        cached_data = self.data_cache.get(key)
        if cached_data is not None:
            return cached_data

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œè®¡ç®—
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(self.executor, compute_func, *args, **kwargs)

        self.data_cache.set(key, result)
        return result


# åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–å™¨
performance_optimizer = PerformanceOptimizer()


@app.route('/')
def dashboard():
    """ä¸»ä»ªè¡¨æ¿é¡µé¢"""
    return render_template('dashboard.html')


@app.route('/simple')
def simple_dashboard():
    """ç®€åŒ–ç‰ˆä»ªè¡¨æ¿é¡µé¢"""
    return render_template('simple_dashboard.html')


@app.route('/fixed')
def fixed_dashboard():
    """ä¿®å¤ç‰ˆä»ªè¡¨æ¿é¡µé¢"""
    return render_template('dashboard_fixed.html')


@app.route('/api/status')
def get_status():
    """è·å–å®æ—¶çŠ¶æ€ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    # æ£€æŸ¥ç¼“å­˜
    cached_data = performance_optimizer.data_cache.get('status')
    if cached_data:
        return jsonify(cached_data)

    try:
        status = analytics.get_real_time_status()
        summary = monitor.get_performance_summary()

        result = {
            'status': status,
            'summary': summary,
            'timestamp': datetime.now().isoformat()
        }

        # ç¼“å­˜ç»“æœ
        performance_optimizer.data_cache.set('status', result)
        return jsonify(result)

    except Exception as e:
        return jsonify({
            'error': f'è·å–çŠ¶æ€å¤±è´¥: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })


@app.route('/api/performance')
def get_performance():
    """è·å–æ€§èƒ½æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    if len(monitor.completed_sessions) == 0:
        return jsonify({'error': 'æš‚æ— æ•°æ®'})

    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
    if not performance_optimizer.should_update_data('performance'):
        cached_data = performance_optimizer.data_cache.get('performance')
        if cached_data:
            return jsonify(cached_data)

    try:
        report = analytics.generate_performance_report()
        bottlenecks = analytics.analyze_bottlenecks()

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        import dataclasses
        result = {
            'report': dataclasses.asdict(report),
            'bottlenecks': dataclasses.asdict(bottlenecks),
            'timestamp': datetime.now().isoformat()
        }

        # ç¼“å­˜ç»“æœ
        performance_optimizer.data_cache.set('performance', result)
        performance_optimizer.update_session_tracking()

        return jsonify(result)

    except Exception as e:
        return jsonify({
            'error': f'è·å–æ€§èƒ½æ•°æ®å¤±è´¥: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })


@app.route('/api/dashboard_data')
def get_dashboard_data():
    """æ‰¹é‡è·å–ä»ªè¡¨æ¿æ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨å®æ—¶æ•°æ®ï¼‰"""
    try:
        # æ£€æŸ¥ç¼“å­˜
        cached_data = performance_optimizer.data_cache.get('dashboard_batch')
        if cached_data:
            return jsonify(cached_data)

        # é¦–å…ˆå°è¯•è·å–å®æ—¶æ•°æ®
        if live_connector.check_system_running():
            print("ğŸ”„ æ£€æµ‹åˆ°Estiaç³»ç»Ÿæ­£åœ¨è¿è¡Œï¼Œä½¿ç”¨å®æ—¶æ•°æ®")
            return get_live_data()

        print("âš ï¸ Estiaç³»ç»Ÿæœªè¿è¡Œï¼Œä½¿ç”¨æ¨¡æ‹Ÿç›‘æ§æ•°æ®")

        # æ‰¹é‡è·å–æ‰€æœ‰æ•°æ®
        sessions = monitor.completed_sessions if hasattr(monitor, 'completed_sessions') else []
        result = {
            'timestamp': datetime.now().isoformat(),
            'has_data': len(sessions) > 0,
            'data_source': 'mock_monitor'
        }

        # å³ä½¿æ²¡æœ‰æ•°æ®ä¹Ÿè¿”å›ç©ºç»“æ„ï¼Œè€Œä¸æ˜¯é”™è¯¯
        if len(sessions) == 0:
            result.update({
                'status': {
                    'status': analytics.get_real_time_status(),
                    'summary': monitor.get_performance_summary()
                },
                'keywords': {
                    'top_keywords': [],
                    'total_unique_keywords': 0,
                    'keyword_distribution': {}
                },
                'sessions': {
                    'sessions': [],
                    'total': 0
                },
                'memory': {
                    'average_similarity': 0,
                    'memory_usage_stats': {},
                    'total_retrievals': 0,
                    'similarity_distribution': {}
                }
            })
            return jsonify(result)

        # çŠ¶æ€æ•°æ®
        try:
            status = analytics.get_real_time_status()
            summary = monitor.get_performance_summary()
            result['status'] = {'status': status, 'summary': summary}
        except Exception as e:
            result['status'] = {'error': str(e)}

        # å…³é”®è¯æ•°æ®
        try:
            keyword_data = keyword_analyzer.analyze_keyword_trends(sessions)
            result['keywords'] = keyword_data
        except Exception as e:
            result['keywords'] = {'error': str(e)}

        # ä¼šè¯æ•°æ®ï¼ˆåªè¿”å›æœ€è¿‘20ä¸ªï¼‰
        try:
            recent_sessions = sessions[-20:]
            session_data = []
            for session in recent_sessions:
                session_info = {
                    'session_id': session.session_id,
                    'start_time': datetime.fromtimestamp(session.start_time).isoformat(),
                    'duration': session.total_duration or 0,
                    'success_count': session.success_count,
                    'failed_count': session.failed_count,
                    'user_input': session.user_input or '',
                    'ai_response': (session.ai_response or '')[:100] + '...' if session.ai_response and len(session.ai_response) > 100 else session.ai_response or ''
                }
                session_data.append(session_info)

            result['sessions'] = {
                'sessions': session_data,
                'total': len(sessions)
            }
        except Exception as e:
            result['sessions'] = {'error': str(e)}

        # è®°å¿†åˆ†ææ•°æ®
        try:
            memory_data = memory_analyzer.analyze_memory_patterns(sessions)
            result['memory'] = memory_data
        except Exception as e:
            result['memory'] = {'error': str(e)}

        # ç¼“å­˜ç»“æœ
        performance_optimizer.data_cache.set('dashboard_batch', result)
        performance_optimizer.update_session_tracking()

        return jsonify(result)

    except Exception as e:
        return jsonify({
            'error': f'æ‰¹é‡è·å–æ•°æ®å¤±è´¥: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })


@app.route('/api/keywords')
def get_keywords():
    """è·å–å…³é”®è¯åˆ†æï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    sessions = monitor.completed_sessions

    if not sessions:
        return jsonify({'error': 'æš‚æ— æ•°æ®'})

    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
    if not performance_optimizer.should_update_data('keywords'):
        cached_data = performance_optimizer.data_cache.get('keywords')
        if cached_data:
            return jsonify(cached_data)

    try:
        keyword_data = keyword_analyzer.analyze_keyword_trends(sessions)

        result = {
            'keywords': keyword_data,
            'timestamp': datetime.now().isoformat()
        }

        # ç¼“å­˜ç»“æœ
        performance_optimizer.data_cache.set('keywords', result)
        performance_optimizer.update_session_tracking()

        return jsonify(result)

    except Exception as e:
        return jsonify({
            'error': f'å…³é”®è¯åˆ†æå¤±è´¥: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })


@app.route('/api/memory_analysis')
def get_memory_analysis():
    """è·å–è®°å¿†åˆ†æ"""
    sessions = monitor.completed_sessions
    
    if not sessions:
        return jsonify({'error': 'æš‚æ— æ•°æ®'})
    
    memory_data = memory_analyzer.analyze_memory_patterns(sessions)
    
    return jsonify({
        'memory': memory_data,
        'timestamp': datetime.now().isoformat()
    })


@app.route('/api/sessions')
def get_sessions():
    """è·å–ä¼šè¯åˆ—è¡¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    sessions = monitor.completed_sessions

    if not sessions:
        return jsonify({
            'sessions': [],
            'total': 0,
            'timestamp': datetime.now().isoformat()
        })

    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
    if not performance_optimizer.should_update_data('sessions'):
        cached_data = performance_optimizer.data_cache.get('sessions')
        if cached_data:
            return jsonify(cached_data)

    try:
        recent_sessions = sessions[-20:]  # æœ€è¿‘20ä¸ªä¼šè¯

        session_data = []
        for session in recent_sessions:
            session_info = {
                'session_id': session.session_id,
                'start_time': datetime.fromtimestamp(session.start_time).isoformat(),
                'duration': session.total_duration or 0,
                'success_count': session.success_count,
                'failed_count': session.failed_count,
                'user_input': session.user_input or '',
                'ai_response': (session.ai_response or '')[:100] + '...' if session.ai_response and len(session.ai_response) > 100 else session.ai_response or ''
            }
            session_data.append(session_info)

        result = {
            'sessions': session_data,
            'total': len(sessions),
            'timestamp': datetime.now().isoformat()
        }

        # ç¼“å­˜ç»“æœ
        performance_optimizer.data_cache.set('sessions', result)
        performance_optimizer.update_session_tracking()

        return jsonify(result)

    except Exception as e:
        return jsonify({
            'error': f'è·å–ä¼šè¯åˆ—è¡¨å¤±è´¥: {str(e)}',
            'sessions': [],
            'total': 0,
            'timestamp': datetime.now().isoformat()
        })


@app.route('/api/step_details/<step_name>')
def get_step_details(step_name):
    """è·å–ç‰¹å®šæ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯"""
    step_data = []
    
    for session in monitor.completed_sessions:
        for step, metrics in session.steps.items():
            if step.value == step_name:
                step_info = {
                    'session_id': session.session_id,
                    'duration': metrics.duration or 0,
                    'status': metrics.status.value,
                    'input_size': metrics.input_size or 0,
                    'output_size': metrics.output_size or 0,
                    'metadata': metrics.metadata,
                    'timestamp': datetime.fromtimestamp(metrics.start_time).isoformat()
                }
                step_data.append(step_info)
    
    return jsonify({
        'step_name': step_name,
        'executions': step_data,
        'count': len(step_data),
        'timestamp': datetime.now().isoformat()
    })


@app.route('/api/v6_comprehensive_stats')
def get_v6_comprehensive_stats():
    """è·å–v6.0ç³»ç»Ÿçš„ç»¼åˆç»Ÿè®¡ä¿¡æ¯"""
    if v6_adapter is None:
        return jsonify({'error': 'v6.0æ•°æ®é€‚é…å™¨ä¸å¯ç”¨'})
    
    try:
        adapted_stats = v6_adapter.adapt_comprehensive_stats()
        return jsonify({
            'v6_stats': adapted_stats,
            'timestamp': datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({'error': f'è·å–v6.0ç»Ÿè®¡å¤±è´¥: {str(e)}'})


@app.route('/api/13_step_monitoring')
def get_13_step_monitoring():
    """è·å–13æ­¥æµç¨‹ç›‘æ§è¯¦æƒ…"""
    if v6_adapter is None:
        return jsonify({'error': 'v6.0æ•°æ®é€‚é…å™¨ä¸å¯ç”¨'})
    
    try:
        adapted_data = v6_adapter.adapt_13_step_monitoring()
        return jsonify({
            '13_step_data': adapted_data,
            'timestamp': datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({'error': f'è·å–13æ­¥ç›‘æ§å¤±è´¥: {str(e)}'})


@socketio.on('connect')
def handle_connect():
    """WebSocketè¿æ¥å¤„ç†"""
    print('å®¢æˆ·ç«¯å·²è¿æ¥')
    emit('message', {'data': 'ç›‘æ§è¿æ¥å·²å»ºç«‹'})


@socketio.on('start_monitoring')
def handle_start_monitoring():
    """å¼€å§‹å®æ—¶ç›‘æ§ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    print('å¼€å§‹å®æ—¶ç›‘æ§')

    def monitoring_loop():
        error_count = 0
        max_errors = 5
        base_interval = 3  # åŸºç¡€é—´éš”3ç§’

        while error_count < max_errors:
            try:
                # æ£€æŸ¥ç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—
                cached_status = performance_optimizer.data_cache.get('websocket_status')
                if cached_status:
                    socketio.emit('status_update', cached_status)
                else:
                    # è·å–å®æ—¶çŠ¶æ€
                    status = analytics.get_real_time_status()
                    summary = monitor.get_performance_summary()

                    status_data = {
                        'status': status,
                        'summary': summary,
                        'timestamp': datetime.now().isoformat()
                    }

                    # ç¼“å­˜çŠ¶æ€æ•°æ®ï¼ˆçŸ­æ—¶é—´ç¼“å­˜ï¼‰
                    performance_optimizer.data_cache.set('websocket_status', status_data)

                    # å‘é€å®æ—¶æ•°æ®
                    socketio.emit('status_update', status_data)

                # é‡ç½®é”™è¯¯è®¡æ•°
                error_count = 0

                # åŠ¨æ€è°ƒæ•´æ›´æ–°é—´éš”
                session_count = len(monitor.completed_sessions)
                if session_count == 0:
                    interval = base_interval * 2  # æ— æ•°æ®æ—¶é™ä½é¢‘ç‡
                else:
                    interval = base_interval

                time.sleep(interval)

            except Exception as e:
                error_count += 1
                print(f"ç›‘æ§å¾ªç¯é”™è¯¯ ({error_count}/{max_errors}): {e}")

                # å‘é€é”™è¯¯çŠ¶æ€
                socketio.emit('monitoring_error', {
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })

                # æŒ‡æ•°é€€é¿
                time.sleep(min(base_interval * (2 ** error_count), 30))

        print("ç›‘æ§å¾ªç¯å› é”™è¯¯è¿‡å¤šè€Œåœæ­¢")

    # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œç›‘æ§
    monitoring_thread = threading.Thread(target=monitoring_loop)
    monitoring_thread.daemon = True
    monitoring_thread.start()


# æ³¨æ„ï¼šç°åœ¨ä½¿ç”¨ç‹¬ç«‹çš„æ¨¡æ¿æ–‡ä»¶ templates/dashboard.html


@app.route('/api/live_data')
def get_live_data():
    """è·å–å®æ—¶ç³»ç»Ÿæ•°æ®"""
    try:
        # è·å–å®æ—¶æ•°æ®
        live_data = live_connector.get_comprehensive_data()

        if not live_data['system_running']:
            return jsonify({
                'error': 'Estiaç³»ç»Ÿæœªè¿è¡Œæˆ–æ•°æ®åº“ä¸å¯è®¿é—®',
                'system_status': 'offline',
                'timestamp': datetime.now().isoformat()
            })

        # è½¬æ¢ä¸ºä»ªè¡¨æ¿æ ¼å¼
        memory_stats = live_data.get('memory_stats', {})
        session_stats = live_data.get('session_stats', {})
        health = live_data.get('system_health', {})

        # æ„å»ºä»ªè¡¨æ¿æ•°æ®
        dashboard_data = {
            'timestamp': datetime.now().isoformat(),
            'has_data': True,
            'live_mode': True,
            'status': {
                'status': {
                    'status': 'running' if health.get('system_status') == 'active' else 'idle',
                    'session_id': session_stats.get('recent_sessions', [{}])[0].get('session_id', 'æ— ') if session_stats.get('recent_sessions') else 'æ— ',
                    'running_time': 0,  # æ— æ³•ä»æ•°æ®åº“è·å–è¿è¡Œæ—¶é—´
                    'progress_percentage': 0
                },
                'summary': {
                    'total_sessions': session_stats.get('total_sessions', 0),
                    'average_duration': 0,  # æ— æ³•ä»å½“å‰æ•°æ®è®¡ç®—
                    'success_rate': 1.0,  # å‡è®¾æˆåŠŸç‡
                    'slowest_step': None
                }
            },
            'keywords': extract_keywords_from_memories(memory_stats.get('recent_memories', [])),
            'sessions': {
                'sessions': format_sessions_for_display(session_stats.get('recent_dialogues', [])),
                'total': session_stats.get('total_sessions', 0)
            },
            'memory': {
                'average_similarity': 0.75,  # ä¼°ç®—å€¼
                'memory_usage_stats': {
                    'retrieved': memory_stats.get('total_memories', 0),
                    'associations': memory_stats.get('today_memories', 0)
                },
                'total_retrievals': memory_stats.get('total_memories', 0),
                'similarity_distribution': {
                    'é«˜ (>0.8)': memory_stats.get('weight_distribution', {}).get('high_weight', 0),
                    'ä¸­ (0.6-0.8)': memory_stats.get('weight_distribution', {}).get('medium_weight', 0),
                    'ä½ (<0.6)': memory_stats.get('weight_distribution', {}).get('low_weight', 0)
                }
            }
        }

        return jsonify(dashboard_data)

    except Exception as e:
        return jsonify({
            'error': f'è·å–å®æ—¶æ•°æ®å¤±è´¥: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })


def extract_keywords_from_memories(memories: List[Dict]) -> Dict[str, Any]:
    """ä»è®°å¿†ä¸­æå–å…³é”®è¯"""
    try:
        import re
        from collections import Counter

        # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
        all_text = ' '.join([mem.get('content', '') for mem in memories])

        # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆæå–2-4å­—çš„è¯æ±‡ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,4}', all_text)

        # ç»Ÿè®¡è¯é¢‘
        word_counts = Counter(chinese_words)

        # è¿‡æ»¤å¸¸è§åœç”¨è¯
        stop_words = {'è¿™ä¸ª', 'é‚£ä¸ª', 'å¯ä»¥', 'éœ€è¦', 'åº”è¯¥', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å', 'ç°åœ¨', 'æ—¶å€™', 'é—®é¢˜', 'æ–¹æ³•', 'ç³»ç»Ÿ', 'åŠŸèƒ½'}
        filtered_words = {word: count for word, count in word_counts.items() if word not in stop_words and count > 1}

        # è·å–å‰10ä¸ªå…³é”®è¯
        top_words = sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)[:10]

        return {
            'top_keywords': [
                {'word': word, 'count': count, 'frequency': count / len(memories) if memories else 0}
                for word, count in top_words
            ],
            'total_unique_keywords': len(filtered_words),
            'keyword_distribution': dict(top_words)
        }

    except Exception as e:
        return {
            'top_keywords': [],
            'total_unique_keywords': 0,
            'keyword_distribution': {},
            'error': str(e)
        }


def format_sessions_for_display(dialogues: List[Dict]) -> List[Dict]:
    """æ ¼å¼åŒ–ä¼šè¯æ•°æ®ç”¨äºæ˜¾ç¤º"""
    try:
        formatted_sessions = []

        # æŒ‰ä¼šè¯IDåˆ†ç»„å¯¹è¯
        session_groups = {}
        for dialogue in dialogues:
            session_id = dialogue.get('session_id', 'unknown')
            if session_id not in session_groups:
                session_groups[session_id] = []
            session_groups[session_id].append(dialogue)

        # ä¸ºæ¯ä¸ªä¼šè¯åˆ›å»ºæ˜¾ç¤ºæ¡ç›®
        for session_id, session_dialogues in list(session_groups.items())[:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ªä¼šè¯
            # æŒ‰æ—¶é—´æ’åº
            session_dialogues.sort(key=lambda x: x.get('timestamp', 0))

            # æ‰¾åˆ°ç”¨æˆ·è¾“å…¥å’ŒAIå›å¤
            user_inputs = [d for d in session_dialogues if d.get('type') == 'user_input']
            ai_responses = [d for d in session_dialogues if d.get('type') == 'assistant_reply']

            # è·å–æœ€æ–°çš„ç”¨æˆ·è¾“å…¥å’ŒAIå›å¤
            latest_user_input = user_inputs[-1] if user_inputs else None
            latest_ai_response = ai_responses[-1] if ai_responses else None

            user_content = latest_user_input.get('content', 'æ— ç”¨æˆ·è¾“å…¥') if latest_user_input else 'æ— ç”¨æˆ·è¾“å…¥'
            ai_content = latest_ai_response.get('content', 'æ— AIå›å¤') if latest_ai_response else 'æ— AIå›å¤'

            # è®¡ç®—ä¼šè¯æŒç»­æ—¶é—´
            if session_dialogues:
                start_time = min(d.get('timestamp', 0) for d in session_dialogues)
                end_time = max(d.get('timestamp', 0) for d in session_dialogues)
                duration = max(0.1, end_time - start_time)  # è‡³å°‘0.1ç§’
                start_time_str = datetime.fromtimestamp(start_time).isoformat()
            else:
                duration = 0.1
                start_time_str = datetime.now().isoformat()

            formatted_sessions.append({
                'session_id': session_id,
                'start_time': start_time_str,
                'duration': duration,
                'success_count': len(ai_responses),
                'failed_count': 0,  # å‡è®¾æ²¡æœ‰å¤±è´¥
                'user_input': user_content[:100] + '...' if len(user_content) > 100 else user_content,
                'ai_response': ai_content[:100] + '...' if len(ai_content) > 100 else ai_content
            })

        return formatted_sessions

    except Exception as e:
        print(f"æ ¼å¼åŒ–ä¼šè¯æ•°æ®å¤±è´¥: {e}")
        return []


@app.route('/api/generate_test_data')
def generate_test_data():
    """ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰"""
    try:
        import random

        # ç”Ÿæˆæ¨¡æ‹Ÿä¼šè¯æ•°æ®
        test_sessions = []
        for i in range(10):
            session_time = datetime.now() - timedelta(minutes=random.randint(1, 60))
            test_sessions.append({
                'session_id': f'test_session_{i+1}',
                'start_time': session_time.isoformat(),
                'duration': random.uniform(0.5, 3.0),
                'success_count': random.randint(8, 14),
                'failed_count': random.randint(0, 2),
                'user_input': f'æµ‹è¯•æŸ¥è¯¢ {i+1}: è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æŸ¥è¯¢',
                'ai_response': f'è¿™æ˜¯æµ‹è¯•å›å¤ {i+1}...'
            })

        # ç”Ÿæˆæ¨¡æ‹Ÿå…³é”®è¯æ•°æ®
        test_keywords = {
            'top_keywords': [
                {'word': 'æµ‹è¯•', 'count': 15, 'frequency': 0.8},
                {'word': 'æŸ¥è¯¢', 'count': 12, 'frequency': 0.6},
                {'word': 'æ•°æ®', 'count': 10, 'frequency': 0.5},
                {'word': 'ç›‘æ§', 'count': 8, 'frequency': 0.4},
                {'word': 'ç³»ç»Ÿ', 'count': 6, 'frequency': 0.3}
            ],
            'total_unique_keywords': 25,
            'keyword_distribution': {
                'æµ‹è¯•': 15, 'æŸ¥è¯¢': 12, 'æ•°æ®': 10, 'ç›‘æ§': 8, 'ç³»ç»Ÿ': 6
            }
        }

        # ç”Ÿæˆæ¨¡æ‹Ÿè®°å¿†åˆ†ææ•°æ®
        test_memory = {
            'average_similarity': 0.75,
            'memory_usage_stats': {
                'retrieved': 45,
                'associations': 23,
                'context_memories': 12
            },
            'total_retrievals': 45,
            'similarity_distribution': {
                'é«˜ (>0.8)': 15,
                'ä¸­ (0.6-0.8)': 20,
                'ä½ (<0.6)': 10
            }
        }

        # ç”Ÿæˆæ¨¡æ‹ŸçŠ¶æ€æ•°æ®
        test_status = {
            'status': {
                'status': 'idle',
                'session_id': None,
                'running_time': random.uniform(10, 100),
                'progress_percentage': 0
            },
            'summary': {
                'total_sessions': 10,
                'average_duration': 1.5,
                'success_rate': 0.92,
                'slowest_step': {
                    'step': 'step_5_faiss_search',
                    'avg_duration': 0.234
                }
            }
        }

        result = {
            'timestamp': datetime.now().isoformat(),
            'has_data': True,
            'test_mode': True,
            'status': test_status,
            'keywords': test_keywords,
            'sessions': {
                'sessions': test_sessions,
                'total': len(test_sessions)
            },
            'memory': test_memory
        }

        return jsonify(result)

    except Exception as e:
        return jsonify({
            'error': f'ç”Ÿæˆæµ‹è¯•æ•°æ®å¤±è´¥: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })


def run_dashboard(host='127.0.0.1', port=5000, debug=True):
    """è¿è¡ŒWebä»ªè¡¨æ¿"""
    print(f"ğŸš€ å¯åŠ¨ Estia è®°å¿†ç›‘æ§ä»ªè¡¨æ¿")
    print(f"ğŸ“Š è®¿é—®åœ°å€: http://{host}:{port}")
    print(f"ğŸ”„ å®æ—¶ç›‘æ§: WebSocket è¿æ¥å·²å¯ç”¨")
    print(f"ğŸ§ª æµ‹è¯•æ•°æ®: http://{host}:{port}/api/generate_test_data")
    print("="*60)

    # å¯åŠ¨Flaskåº”ç”¨
    socketio.run(app, host=host, port=port, debug=debug)


if __name__ == '__main__':
    run_dashboard()