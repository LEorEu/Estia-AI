#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Estia è®°å¿†ç›‘æ§ Web ä»ªè¡¨æ¿
========================

åŸºäºFlaskçš„å®æ—¶ç›‘æ§å¯è§†åŒ–ç•Œé¢ï¼ŒåŒ…å«ï¼š
- å®æ—¶æµç¨‹ç›‘æ§
- æ€§èƒ½å›¾è¡¨å’Œåˆ†æ
- å…³é”®è¯äº‘å’Œè¶‹åŠ¿åˆ†æ
- è®°å¿†å†…å®¹å¯è§†åŒ–
"""

import json
import time
import re
from datetime import datetime, timedelta
from collections import Counter, defaultdict
from typing import Dict, List, Any, Optional
from functools import lru_cache
import asyncio
from concurrent.futures import ThreadPoolExecutor

from flask import Flask, render_template, jsonify, request, send_from_directory, send_file
from flask_socketio import SocketIO, emit
import threading
import os

# å¯¼å…¥ç›‘æ§ç³»ç»Ÿ
from core.memory.managers.monitor_flow.monitoring import (
    MemoryPipelineMonitor,
    MemoryPipelineStep,
    StepStatus,
    MonitorAnalytics
)

# å¯¼å…¥å®æ—¶æ•°æ®è¿æ¥å™¨
from .live_data_connector import live_connector

# å¯¼å…¥æ–°çš„ç›‘æ§ç³»ç»Ÿé›†æˆ
from .monitoring_integration import (
    initialize_monitoring_system, 
    get_monitoring_system,
    enhance_dashboard_data,
    register_monitoring_routes,
    monitoring_bp
)

# é…ç½®Flaskåº”ç”¨åŒæ—¶æœåŠ¡Vueå‰ç«¯
vue_dist_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'web-vue', 'dist')
app = Flask(__name__, 
           template_folder=vue_dist_path,
           static_folder=vue_dist_path,
           static_url_path='')
app.config['SECRET_KEY'] = 'estia_monitoring_secret'
socketio = SocketIO(app, cors_allowed_origins="*")

print(f"ğŸ“ Vueå‰ç«¯èµ„æºè·¯å¾„: {vue_dist_path}")
print(f"ğŸ“ Vueå‰ç«¯èµ„æºå­˜åœ¨: {os.path.exists(vue_dist_path)}")

# å…¨å±€ç›‘æ§å®ä¾‹
try:
    monitor = MemoryPipelineMonitor.get_instance()
    analytics = MonitorAnalytics(monitor)
    print("âœ… ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")

    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
    session_count = len(monitor.completed_sessions)
    print(f"ğŸ“Š å½“å‰ä¼šè¯æ•°é‡: {session_count}")

    if session_count == 0:
        print("âš ï¸ æš‚æ— ç›‘æ§æ•°æ®ï¼Œå°†æ˜¾ç¤ºç©ºçŠ¶æ€")

except Exception as e:
    print(f"âŒ ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
    # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„ç›‘æ§å™¨ç”¨äºæµ‹è¯•
    class MockMonitor:
        def __init__(self):
            self.completed_sessions = []

        def get_performance_summary(self):
            return {
                'total_sessions': 0,
                'average_duration': 0.0,
                'success_rate': 0.0,
                'slowest_step': None
            }

    class MockAnalytics:
        def __init__(self, monitor):
            self.monitor = monitor

        def get_real_time_status(self):
            return {
                'status': 'idle',
                'session_id': None,
                'running_time': 0,
                'progress_percentage': 0
            }

        def generate_performance_report(self):
            from dataclasses import dataclass
            @dataclass
            class MockReport:
                total_sessions: int = 0
                avg_duration: float = 0.0
                success_rate: float = 0.0
            return MockReport()

        def analyze_bottlenecks(self):
            from dataclasses import dataclass
            @dataclass
            class MockBottlenecks:
                slowest_steps: list = None
                avg_bottleneck_time: float = 0.0
            return MockBottlenecks()

    monitor = MockMonitor()
    analytics = MockAnalytics(monitor)
    print("ğŸ”„ ä½¿ç”¨æ¨¡æ‹Ÿç›‘æ§å™¨")

# å°è¯•è¿æ¥v6.0çš„MemoryFlowMonitorï¼ˆå¢å¼ºé”™è¯¯å¤„ç†ï¼‰
flow_monitor = None
flow_monitor_error = None

try:
    from core.memory.managers.monitor_flow import MemoryFlowMonitor

    # åˆ›å»ºæ¨¡æ‹Ÿçš„ç»„ä»¶å­—å…¸æ¥åˆå§‹åŒ–MemoryFlowMonitor
    # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™äº›ç»„ä»¶åº”è¯¥æ¥è‡ªçœŸå®çš„v6.0ç³»ç»Ÿ
    mock_components = {
        'db_manager': None,  # æ•°æ®åº“ç®¡ç†å™¨
        'unified_cache': None,  # ç»Ÿä¸€ç¼“å­˜
        'sync_flow_manager': None,  # åŒæ­¥æµç¨‹ç®¡ç†å™¨
        'async_flow_manager': None  # å¼‚æ­¥æµç¨‹ç®¡ç†å™¨
    }

    flow_monitor = MemoryFlowMonitor(mock_components)
    print("âœ… v6.0 MemoryFlowMonitor åˆå§‹åŒ–æˆåŠŸ")

    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    try:
        test_stats = flow_monitor.get_comprehensive_stats()
        if 'error' in test_stats:
            print(f"âš ï¸ v6.0ç›‘æ§å™¨åŠŸèƒ½æµ‹è¯•å¤±è´¥: {test_stats['error']}")
        else:
            print("âœ… v6.0ç›‘æ§å™¨åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    except Exception as test_error:
        print(f"âš ï¸ v6.0ç›‘æ§å™¨åŠŸèƒ½æµ‹è¯•å¼‚å¸¸: {test_error}")

except ImportError as e:
    flow_monitor_error = f"å¯¼å…¥å¤±è´¥: {e}"
    print(f"âš ï¸ v6.0 MemoryFlowMonitor å¯¼å…¥å¤±è´¥: {e}")
except Exception as e:
    flow_monitor_error = f"åˆå§‹åŒ–å¤±è´¥: {e}"
    print(f"âš ï¸ v6.0 MemoryFlowMonitor åˆå§‹åŒ–å¤±è´¥: {e}")


class FallbackMonitor:
    """é™çº§ç›‘æ§å™¨ï¼Œå½“v6.0ç›‘æ§å™¨ä¸å¯ç”¨æ—¶ä½¿ç”¨"""

    def __init__(self):
        self.start_time = time.time()

    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """æä¾›åŸºç¡€çš„ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'timestamp': time.time(),
            'monitor_status': 'fallback',
            'performance_summary': {
                'cache_hit_rate': 0.0,
                'cache_efficiency': 0.0,
                'system_health': 'unknown'
            },
            'memory_overview': {
                'total_memories': 0,
                'active_sessions': 0
            },
            'session_statistics': {
                'total_sessions': 0,
                'avg_duration': 0.0
            },
            'health_status': {
                'status': 'degraded',
                'message': 'v6.0ç›‘æ§å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é™çº§æ¨¡å¼'
            },
            'error': flow_monitor_error
        }

    def get_13_step_monitoring(self) -> Dict[str, Any]:
        """æä¾›åŸºç¡€çš„æ­¥éª¤ç›‘æ§ä¿¡æ¯"""
        return {
            'error': 'v6.0ç›‘æ§å™¨ä¸å¯ç”¨',
            'fallback_mode': True,
            'message': '14æ­¥æµç¨‹ç›‘æ§éœ€è¦v6.0ç›‘æ§å™¨æ”¯æŒ'
        }

    def get_real_time_metrics(self) -> Dict[str, Any]:
        """æä¾›åŸºç¡€çš„å®æ—¶æŒ‡æ ‡"""
        return {
            'timestamp': time.time(),
            'cache_performance': {'status': 'unavailable'},
            'database_performance': {'status': 'unavailable'},
            'queue_status': {'status': 'unavailable'},
            'memory_usage': {'status': 'unavailable'},
            'error': 'v6.0ç›‘æ§å™¨ä¸å¯ç”¨'
        }


# å¦‚æœv6.0ç›‘æ§å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é™çº§ç›‘æ§å™¨
if flow_monitor is None:
    flow_monitor = FallbackMonitor()
    print("ğŸ”„ å¯ç”¨é™çº§ç›‘æ§æ¨¡å¼")

# åˆå§‹åŒ–æ–°çš„ç›‘æ§ç³»ç»Ÿé›†æˆ
enhanced_monitor = None
try:
    enhanced_monitor = initialize_monitoring_system()
    if enhanced_monitor:
        print("âœ… å¢å¼ºç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        # æ³¨å†Œç›‘æ§APIè·¯ç”±
        register_monitoring_routes(app)
    else:
        print("âš ï¸ å¢å¼ºç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
except Exception as e:
    print(f"âŒ å¢å¼ºç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–å¼‚å¸¸: {e}")
    enhanced_monitor = None


class KeywordAnalyzer:
    """å…³é”®è¯åˆ†æå™¨"""
    
    def __init__(self):
        # ä¸­æ–‡åœç”¨è¯
        self.stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
            'è‡ªå·±', 'è¿™', 'é‚£', 'ä»€ä¹ˆ', 'æˆ‘ä»¬', 'ä»–ä»¬', 'å¥¹ä»¬', 'å®ƒä»¬', 'è¿™ä¸ª', 'é‚£ä¸ª',
            'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•', 'å—', 'å‘¢', 'å§', 'å•Š', 'å‘€'
        }
        
        # è‹±æ–‡åœç”¨è¯
        self.stop_words.update({
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have',
            'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should', 'could',
            'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we',
            'they', 'me', 'him', 'her', 'us', 'them'
        })
    
    def extract_keywords(self, text: str, min_length: int = 2) -> List[str]:
        """æå–å…³é”®è¯"""
        if not text:
            return []
        
        # æ¸…ç†æ–‡æœ¬
        text = text.lower()
        # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡å’Œæ•°å­—
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', ' ', text)
        
        # åˆ†è¯ï¼ˆç®€å•çš„åŸºäºç©ºæ ¼å’Œæ ‡ç‚¹çš„åˆ†è¯ï¼‰
        words = text.split()
        
        # è¿‡æ»¤å…³é”®è¯
        keywords = []
        for word in words:
            word = word.strip()
            if (len(word) >= min_length and 
                word not in self.stop_words and
                not word.isdigit()):
                keywords.append(word)
        
        return keywords
    
    def analyze_keyword_trends(self, sessions: List) -> Dict[str, Any]:
        """åˆ†æå…³é”®è¯è¶‹åŠ¿"""
        keyword_counts = Counter()
        time_series = defaultdict(list)
        
        for session in sessions:
            if hasattr(session, 'user_input') and session.user_input:
                keywords = self.extract_keywords(session.user_input)
                
                for keyword in keywords:
                    keyword_counts[keyword] += 1
                    time_series[keyword].append(session.start_time)
            
            # åˆ†æAIå›å¤ä¸­çš„å…³é”®è¯
            if hasattr(session, 'ai_response') and session.ai_response:
                response_keywords = self.extract_keywords(session.ai_response)
                for keyword in response_keywords[:5]:  # åªå–å‰5ä¸ªé¿å…è¿‡å¤š
                    keyword_counts[f"å›å¤_{keyword}"] += 1
        
        # è®¡ç®—è¶‹åŠ¿
        trending_keywords = []
        for keyword, count in keyword_counts.most_common(20):
            if count >= 2:  # è‡³å°‘å‡ºç°2æ¬¡
                trending_keywords.append({
                    'word': keyword,
                    'count': count,
                    'frequency': count / len(sessions) if sessions else 0
                })
        
        return {
            'top_keywords': trending_keywords,
            'total_unique_keywords': len(keyword_counts),
            'keyword_distribution': dict(keyword_counts.most_common(10))
        }


class V6DataAdapter:
    """v6.0æ•°æ®é€‚é…å™¨ï¼Œå°†v6.0ç›‘æ§æ•°æ®è½¬æ¢ä¸ºä»ªè¡¨æ¿æœŸæœ›çš„æ ¼å¼ï¼ˆå¢å¼ºç‰ˆï¼‰"""

    def __init__(self, flow_monitor):
        self.flow_monitor = flow_monitor
        self.is_fallback_mode = isinstance(flow_monitor, FallbackMonitor)

    def adapt_comprehensive_stats(self) -> Dict[str, Any]:
        """é€‚é…ç»¼åˆç»Ÿè®¡æ•°æ®ï¼ˆå¢å¼ºé”™è¯¯å¤„ç†ï¼‰"""
        if not self.flow_monitor:
            return {'error': 'v6.0ç›‘æ§å™¨ä¸å¯ç”¨'}

        try:
            stats = self.flow_monitor.get_comprehensive_stats()

            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯å“åº”
            if 'error' in stats:
                return {
                    'error': stats['error'],
                    'fallback_mode': self.is_fallback_mode,
                    'timestamp': stats.get('timestamp', time.time())
                }

            # è½¬æ¢ä¸ºä»ªè¡¨æ¿æœŸæœ›çš„æ ¼å¼
            adapted_stats = {
                'timestamp': stats.get('timestamp', time.time()),
                'monitor_status': stats.get('monitor_status', 'unknown'),
                'fallback_mode': self.is_fallback_mode,
                'performance_summary': {
                    'cache_hit_rate': stats.get('performance_metrics', {}).get('cache_hit_rate', 0),
                    'cache_efficiency': stats.get('performance_metrics', {}).get('cache_efficiency', 0),
                    'system_health': stats.get('health_status', {}).get('status', 'unknown')
                },
                'memory_overview': stats.get('memory_overview', {}),
                'session_stats': stats.get('session_statistics', {})
            }

            # å¦‚æœæ˜¯é™çº§æ¨¡å¼ï¼Œæ·»åŠ è­¦å‘Šä¿¡æ¯
            if self.is_fallback_mode:
                adapted_stats['warning'] = 'v6.0ç›‘æ§å™¨ä¸å¯ç”¨ï¼Œæ˜¾ç¤ºé™çº§æ•°æ®'

            return adapted_stats

        except Exception as e:
            return {
                'error': f'é€‚é…ç»¼åˆç»Ÿè®¡å¤±è´¥: {str(e)}',
                'fallback_mode': self.is_fallback_mode,
                'timestamp': time.time()
            }
    
    def adapt_13_step_monitoring(self) -> Dict[str, Any]:
        """é€‚é…13æ­¥æµç¨‹ç›‘æ§æ•°æ®ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        if not self.flow_monitor:
            return {'error': 'v6.0ç›‘æ§å™¨ä¸å¯ç”¨'}

        try:
            step_data = self.flow_monitor.get_13_step_monitoring()

            # è½¬æ¢ä¸ºä»ªè¡¨æ¿æœŸæœ›çš„æ ¼å¼
            if 'error' in step_data:
                return step_data

            # ä¿®æ­£æ­¥éª¤æ•°é‡ - å®é™…æ˜¯14æ­¥
            adapted_data = {
                'total_steps': step_data.get('total_steps', 14),
                'sync_steps': step_data.get('sync_steps', 9),  # Step 1-9
                'async_steps': step_data.get('async_steps', 5),  # Step 10-14
                'step_performance': step_data.get('overall_performance', {}),
                'step_details': step_data.get('step_details', {}),
                'timestamp': step_data.get('timestamp', 0)
            }

            # æ·»åŠ æ­¥éª¤åç§°æ˜ å°„
            step_mapping = {
                'step_01_database_initialization': 'DBåˆå§‹åŒ–',
                'step_02_component_initialization': 'ç»„ä»¶åˆå§‹åŒ–',
                'step_03_async_evaluator_initialization': 'å¼‚æ­¥è¯„ä¼°å™¨åˆå§‹åŒ–',
                'step_04_unified_cache_vectorization': 'ç¼“å­˜å‘é‡åŒ–',
                'step_05_faiss_vector_retrieval': 'FAISSæ£€ç´¢',
                'step_06_association_network_expansion': 'å…³è”æ‹“å±•',
                'step_07_history_dialogue_aggregation': 'å†å²èšåˆ',
                'step_08_weight_ranking_deduplication': 'æƒé‡æ’åº',
                'step_09_final_context_assembly': 'ä¸Šä¸‹æ–‡æ„å»º',
                'step_10_llm_response_generation': 'LLMç”Ÿæˆ',
                'step_11_immediate_dialogue_storage': 'å¯¹è¯å­˜å‚¨',
                'step_12_async_llm_evaluation': 'å¼‚æ­¥è¯„ä¼°',
                'step_13_save_evaluation_results': 'ä¿å­˜ç»“æœ',
                'step_14_auto_association_creation': 'å…³è”åˆ›å»º'
            }

            adapted_data['step_mapping'] = step_mapping

            return adapted_data

        except Exception as e:
            return {'error': f'é€‚é…13æ­¥ç›‘æ§å¤±è´¥: {str(e)}'}
    
    def adapt_real_time_metrics(self) -> Dict[str, Any]:
        """é€‚é…å®æ—¶æ€§èƒ½æŒ‡æ ‡"""
        if not self.flow_monitor:
            return {'error': 'v6.0ç›‘æ§å™¨ä¸å¯ç”¨'}
        
        try:
            metrics = self.flow_monitor.get_real_time_metrics()
            
            # è½¬æ¢ä¸ºä»ªè¡¨æ¿æœŸæœ›çš„æ ¼å¼
            if 'error' in metrics:
                return metrics
            
            adapted_metrics = {
                'cache_performance': metrics.get('cache_performance', {}),
                'database_performance': metrics.get('database_performance', {}),
                'queue_status': metrics.get('queue_status', {}),
                'memory_usage': metrics.get('memory_usage', {}),
                'timestamp': metrics.get('timestamp', 0)
            }
            
            return adapted_metrics
            
        except Exception as e:
            return {'error': f'é€‚é…å®æ—¶æŒ‡æ ‡å¤±è´¥: {str(e)}'}


class MemoryContentAnalyzer:
    """è®°å¿†å†…å®¹åˆ†æå™¨"""
    
    def analyze_memory_patterns(self, sessions: List) -> Dict[str, Any]:
        """åˆ†æè®°å¿†æ¨¡å¼"""
        memory_types = Counter()
        similarity_scores = []
        memory_usage = defaultdict(int)
        
        for session in sessions:
            for step, metrics in session.steps.items():
                if step == MemoryPipelineStep.STEP_5_FAISS_SEARCH:
                    # åˆ†ææ£€ç´¢ç»“æœ
                    if 'avg_similarity' in metrics.metadata:
                        similarity_scores.append(metrics.metadata['avg_similarity'])
                    
                    if 'result_count' in metrics.metadata:
                        memory_usage['retrieved'] += metrics.metadata['result_count']
                
                elif step == MemoryPipelineStep.STEP_6_ASSOCIATION_EXPAND:
                    # åˆ†æå…³è”æ‹“å±•
                    if 'expansion_count' in metrics.metadata:
                        memory_usage['associations'] += metrics.metadata['expansion_count']
                
                elif step == MemoryPipelineStep.STEP_9_CONTEXT_BUILD:
                    # åˆ†æä¸Šä¸‹æ–‡æ„å»º
                    if 'memory_used' in metrics.metadata:
                        memory_usage['context_memories'] += metrics.metadata['memory_used']
        
        avg_similarity = sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0
        
        return {
            'average_similarity': avg_similarity,
            'memory_usage_stats': dict(memory_usage),
            'total_retrievals': len(similarity_scores),
            'similarity_distribution': self._calculate_similarity_distribution(similarity_scores)
        }
    
    def _calculate_similarity_distribution(self, scores: List[float]) -> Dict[str, int]:
        """è®¡ç®—ç›¸ä¼¼åº¦åˆ†å¸ƒ"""
        if not scores:
            return {}
        
        bins = {'é«˜ (>0.8)': 0, 'ä¸­ (0.6-0.8)': 0, 'ä½ (<0.6)': 0}
        
        for score in scores:
            if score > 0.8:
                bins['é«˜ (>0.8)'] += 1
            elif score > 0.6:
                bins['ä¸­ (0.6-0.8)'] += 1
            else:
                bins['ä½ (<0.6)'] += 1
        
        return bins


# åˆå§‹åŒ–åˆ†æå™¨
keyword_analyzer = KeywordAnalyzer()
memory_analyzer = MemoryContentAnalyzer()

# åˆå§‹åŒ–v6.0æ•°æ®é€‚é…å™¨
v6_adapter = V6DataAdapter(flow_monitor) if flow_monitor else None


class DataCache:
    """æ•°æ®ç¼“å­˜ç®¡ç†å™¨ï¼Œå‡å°‘é‡å¤è®¡ç®—å’ŒAPIè°ƒç”¨"""

    def __init__(self, cache_ttl: int = 3):
        self.cache_ttl = cache_ttl  # ç¼“å­˜ç”Ÿå­˜æ—¶é—´ï¼ˆç§’ï¼‰
        self._cache = {}
        self._timestamps = {}

    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        if key not in self._cache:
            return None

        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        if time.time() - self._timestamps[key] > self.cache_ttl:
            del self._cache[key]
            del self._timestamps[key]
            return None

        return self._cache[key]

    def set(self, key: str, value: Any) -> None:
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        self._cache[key] = value
        self._timestamps[key] = time.time()

    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
        self._timestamps.clear()


class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.data_cache = DataCache()
        self.last_session_count = 0
        self.last_update_time = 0

    def should_update_data(self, data_type: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°æ•°æ®"""
        current_session_count = len(monitor.completed_sessions)
        current_time = time.time()

        # å¦‚æœä¼šè¯æ•°é‡æ²¡æœ‰å˜åŒ–ä¸”è·ç¦»ä¸Šæ¬¡æ›´æ–°ä¸åˆ°3ç§’ï¼Œè·³è¿‡æ›´æ–°
        if (data_type in ['sessions', 'keywords', 'memory'] and
            current_session_count == self.last_session_count and
            current_time - self.last_update_time < 3):
            return False

        return True

    def update_session_tracking(self):
        """æ›´æ–°ä¼šè¯è·Ÿè¸ªä¿¡æ¯"""
        self.last_session_count = len(monitor.completed_sessions)
        self.last_update_time = time.time()

    async def get_cached_or_compute(self, key: str, compute_func, *args, **kwargs):
        """è·å–ç¼“å­˜æ•°æ®æˆ–è®¡ç®—æ–°æ•°æ®"""
        cached_data = self.data_cache.get(key)
        if cached_data is not None:
            return cached_data

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œè®¡ç®—
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(self.executor, compute_func, *args, **kwargs)

        self.data_cache.set(key, result)
        return result


# åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–å™¨
performance_optimizer = PerformanceOptimizer()


# ä¸»é¡µè·¯ç”±å·²åœ¨æ–‡ä»¶æœ«å°¾å®šä¹‰ï¼ˆserve_vue_appå‡½æ•°ï¼‰


@app.route('/api/status')
def get_status():
    """è·å–å®æ—¶çŠ¶æ€ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    # æ£€æŸ¥ç¼“å­˜
    cached_data = performance_optimizer.data_cache.get('status')
    if cached_data:
        return jsonify(cached_data)

    try:
        status = analytics.get_real_time_status()
        summary = monitor.get_performance_summary()

        result = {
            'status': status,
            'summary': summary,
            'timestamp': datetime.now().isoformat()
        }

        # ç¼“å­˜ç»“æœ
        performance_optimizer.data_cache.set('status', result)
        return jsonify(result)

    except Exception as e:
        return jsonify({
            'error': f'è·å–çŠ¶æ€å¤±è´¥: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })


@app.route('/api/performance')
def get_performance():
    """è·å–æ€§èƒ½æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    if len(monitor.completed_sessions) == 0:
        return jsonify({'error': 'æš‚æ— æ•°æ®'})

    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
    if not performance_optimizer.should_update_data('performance'):
        cached_data = performance_optimizer.data_cache.get('performance')
        if cached_data:
            return jsonify(cached_data)

    try:
        report = analytics.generate_performance_report()
        bottlenecks = analytics.analyze_bottlenecks()

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        import dataclasses
        result = {
            'report': dataclasses.asdict(report),
            'bottlenecks': dataclasses.asdict(bottlenecks),
            'timestamp': datetime.now().isoformat()
        }

        # ç¼“å­˜ç»“æœ
        performance_optimizer.data_cache.set('performance', result)
        performance_optimizer.update_session_tracking()

        return jsonify(result)

    except Exception as e:
        return jsonify({
            'error': f'è·å–æ€§èƒ½æ•°æ®å¤±è´¥: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })


@app.route('/api/dashboard_data')
def get_dashboard_data():
    """æ‰¹é‡è·å–ä»ªè¡¨æ¿æ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨å®æ—¶æ•°æ®ï¼‰"""
    try:
        # æ£€æŸ¥ç¼“å­˜
        cached_data = performance_optimizer.data_cache.get('dashboard_batch')
        if cached_data:
            return jsonify(cached_data)

        # é¦–å…ˆå°è¯•è·å–å®æ—¶æ•°æ®
        if live_connector.check_system_running():
            print("ğŸ”„ æ£€æµ‹åˆ°Estiaç³»ç»Ÿæ­£åœ¨è¿è¡Œï¼Œä½¿ç”¨å®æ—¶æ•°æ®")
            return get_live_data()

        print("âš ï¸ Estiaç³»ç»Ÿæœªè¿è¡Œï¼Œä½¿ç”¨æ¨¡æ‹Ÿç›‘æ§æ•°æ®")

        # æ‰¹é‡è·å–æ‰€æœ‰æ•°æ®
        sessions = monitor.completed_sessions if hasattr(monitor, 'completed_sessions') else []
        result = {
            'timestamp': datetime.now().isoformat(),
            'has_data': len(sessions) > 0,
            'data_source': 'mock_monitor'
        }

        # å³ä½¿æ²¡æœ‰æ•°æ®ä¹Ÿè¿”å›ç©ºç»“æ„ï¼Œè€Œä¸æ˜¯é”™è¯¯
        if len(sessions) == 0:
            result.update({
                'status': {
                    'status': analytics.get_real_time_status(),
                    'summary': monitor.get_performance_summary()
                },
                'keywords': {
                    'top_keywords': [],
                    'total_unique_keywords': 0,
                    'keyword_distribution': {}
                },
                'sessions': {
                    'sessions': [],
                    'total': 0
                },
                'memory': {
                    'average_similarity': 0,
                    'memory_usage_stats': {},
                    'total_retrievals': 0,
                    'similarity_distribution': {}
                }
            })
            return jsonify(result)

        # çŠ¶æ€æ•°æ®
        try:
            status = analytics.get_real_time_status()
            summary = monitor.get_performance_summary()
            result['status'] = {'status': status, 'summary': summary}
        except Exception as e:
            result['status'] = {'error': str(e)}

        # å…³é”®è¯æ•°æ®
        try:
            keyword_data = keyword_analyzer.analyze_keyword_trends(sessions)
            result['keywords'] = keyword_data
        except Exception as e:
            result['keywords'] = {'error': str(e)}

        # ä¼šè¯æ•°æ®ï¼ˆåªè¿”å›æœ€è¿‘20ä¸ªï¼‰
        try:
            recent_sessions = sessions[-20:]
            session_data = []
            for session in recent_sessions:
                session_info = {
                    'session_id': session.session_id,
                    'start_time': datetime.fromtimestamp(session.start_time).isoformat(),
                    'duration': session.total_duration or 0,
                    'success_count': session.success_count,
                    'failed_count': session.failed_count,
                    'user_input': session.user_input or '',
                    'ai_response': (session.ai_response or '')[:100] + '...' if session.ai_response and len(session.ai_response) > 100 else session.ai_response or ''
                }
                session_data.append(session_info)

            result['sessions'] = {
                'sessions': session_data,
                'total': len(sessions)
            }
        except Exception as e:
            result['sessions'] = {'error': str(e)}

        # è®°å¿†åˆ†ææ•°æ®
        try:
            memory_data = memory_analyzer.analyze_memory_patterns(sessions)
            result['memory'] = memory_data
        except Exception as e:
            result['memory'] = {'error': str(e)}

        # ä½¿ç”¨å¢å¼ºç›‘æ§ç³»ç»Ÿå¢å¼ºæ•°æ®
        try:
            result = enhance_dashboard_data(result)
            print("âœ… ä»ªè¡¨æ¿æ•°æ®å·²å¢å¼º")
        except Exception as e:
            print(f"âš ï¸ æ•°æ®å¢å¼ºå¤±è´¥: {e}")

        # ç¼“å­˜ç»“æœ
        performance_optimizer.data_cache.set('dashboard_batch', result)
        performance_optimizer.update_session_tracking()

        return jsonify(result)

    except Exception as e:
        return jsonify({
            'error': f'æ‰¹é‡è·å–æ•°æ®å¤±è´¥: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })


@app.route('/api/keywords')
def get_keywords():
    """è·å–å…³é”®è¯åˆ†æï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    sessions = monitor.completed_sessions

    if not sessions:
        return jsonify({'error': 'æš‚æ— æ•°æ®'})

    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
    if not performance_optimizer.should_update_data('keywords'):
        cached_data = performance_optimizer.data_cache.get('keywords')
        if cached_data:
            return jsonify(cached_data)

    try:
        keyword_data = keyword_analyzer.analyze_keyword_trends(sessions)

        result = {
            'keywords': keyword_data,
            'timestamp': datetime.now().isoformat()
        }

        # ç¼“å­˜ç»“æœ
        performance_optimizer.data_cache.set('keywords', result)
        performance_optimizer.update_session_tracking()

        return jsonify(result)

    except Exception as e:
        return jsonify({
            'error': f'å…³é”®è¯åˆ†æå¤±è´¥: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })


@app.route('/api/memory_analysis')
def get_memory_analysis():
    """è·å–è®°å¿†åˆ†æ"""
    sessions = monitor.completed_sessions
    
    if not sessions:
        return jsonify({'error': 'æš‚æ— æ•°æ®'})
    
    memory_data = memory_analyzer.analyze_memory_patterns(sessions)
    
    return jsonify({
        'memory': memory_data,
        'timestamp': datetime.now().isoformat()
    })


@app.route('/api/sessions')
def get_sessions():
    """è·å–ä¼šè¯åˆ—è¡¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    sessions = monitor.completed_sessions

    if not sessions:
        return jsonify({
            'sessions': [],
            'total': 0,
            'timestamp': datetime.now().isoformat()
        })

    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
    if not performance_optimizer.should_update_data('sessions'):
        cached_data = performance_optimizer.data_cache.get('sessions')
        if cached_data:
            return jsonify(cached_data)

    try:
        recent_sessions = sessions[-20:]  # æœ€è¿‘20ä¸ªä¼šè¯

        session_data = []
        for session in recent_sessions:
            session_info = {
                'session_id': session.session_id,
                'start_time': datetime.fromtimestamp(session.start_time).isoformat(),
                'duration': session.total_duration or 0,
                'success_count': session.success_count,
                'failed_count': session.failed_count,
                'user_input': session.user_input or '',
                'ai_response': (session.ai_response or '')[:100] + '...' if session.ai_response and len(session.ai_response) > 100 else session.ai_response or ''
            }
            session_data.append(session_info)

        result = {
            'sessions': session_data,
            'total': len(sessions),
            'timestamp': datetime.now().isoformat()
        }

        # ç¼“å­˜ç»“æœ
        performance_optimizer.data_cache.set('sessions', result)
        performance_optimizer.update_session_tracking()

        return jsonify(result)

    except Exception as e:
        return jsonify({
            'error': f'è·å–ä¼šè¯åˆ—è¡¨å¤±è´¥: {str(e)}',
            'sessions': [],
            'total': 0,
            'timestamp': datetime.now().isoformat()
        })


@app.route('/api/step_details/<step_name>')
def get_step_details(step_name):
    """è·å–ç‰¹å®šæ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯"""
    step_data = []
    
    for session in monitor.completed_sessions:
        for step, metrics in session.steps.items():
            if step.value == step_name:
                step_info = {
                    'session_id': session.session_id,
                    'duration': metrics.duration or 0,
                    'status': metrics.status.value,
                    'input_size': metrics.input_size or 0,
                    'output_size': metrics.output_size or 0,
                    'metadata': metrics.metadata,
                    'timestamp': datetime.fromtimestamp(metrics.start_time).isoformat()
                }
                step_data.append(step_info)
    
    return jsonify({
        'step_name': step_name,
        'executions': step_data,
        'count': len(step_data),
        'timestamp': datetime.now().isoformat()
    })


@app.route('/api/v6_comprehensive_stats')
def get_v6_comprehensive_stats():
    """è·å–v6.0ç³»ç»Ÿçš„ç»¼åˆç»Ÿè®¡ä¿¡æ¯"""
    if v6_adapter is None:
        return jsonify({'error': 'v6.0æ•°æ®é€‚é…å™¨ä¸å¯ç”¨'})
    
    try:
        adapted_stats = v6_adapter.adapt_comprehensive_stats()
        return jsonify({
            'v6_stats': adapted_stats,
            'timestamp': datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({'error': f'è·å–v6.0ç»Ÿè®¡å¤±è´¥: {str(e)}'})


@app.route('/api/13_step_monitoring')
def get_13_step_monitoring():
    """è·å–13æ­¥æµç¨‹ç›‘æ§è¯¦æƒ…"""
    if v6_adapter is None:
        return jsonify({'error': 'v6.0æ•°æ®é€‚é…å™¨ä¸å¯ç”¨'})
    
    try:
        adapted_data = v6_adapter.adapt_13_step_monitoring()
        return jsonify({
            '13_step_data': adapted_data,
            'timestamp': datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({'error': f'è·å–13æ­¥ç›‘æ§å¤±è´¥: {str(e)}'})


@socketio.on('connect')
def handle_connect():
    """WebSocketè¿æ¥å¤„ç†"""
    print('å®¢æˆ·ç«¯å·²è¿æ¥')
    emit('message', {'data': 'ç›‘æ§è¿æ¥å·²å»ºç«‹'})


@socketio.on('start_monitoring')
def handle_start_monitoring():
    """å¼€å§‹å®æ—¶ç›‘æ§ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    print('å¼€å§‹å®æ—¶ç›‘æ§')

    def monitoring_loop():
        error_count = 0
        max_errors = 5
        base_interval = 3  # åŸºç¡€é—´éš”3ç§’

        while error_count < max_errors:
            try:
                # æ£€æŸ¥ç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—
                cached_status = performance_optimizer.data_cache.get('websocket_status')
                if cached_status:
                    socketio.emit('status_update', cached_status)
                else:
                    # è·å–å®æ—¶çŠ¶æ€
                    status = analytics.get_real_time_status()
                    summary = monitor.get_performance_summary()

                    status_data = {
                        'status': status,
                        'summary': summary,
                        'timestamp': datetime.now().isoformat()
                    }

                    # ç¼“å­˜çŠ¶æ€æ•°æ®ï¼ˆçŸ­æ—¶é—´ç¼“å­˜ï¼‰
                    performance_optimizer.data_cache.set('websocket_status', status_data)

                    # å‘é€å®æ—¶æ•°æ®
                    socketio.emit('status_update', status_data)

                # é‡ç½®é”™è¯¯è®¡æ•°
                error_count = 0

                # åŠ¨æ€è°ƒæ•´æ›´æ–°é—´éš”
                session_count = len(monitor.completed_sessions)
                if session_count == 0:
                    interval = base_interval * 2  # æ— æ•°æ®æ—¶é™ä½é¢‘ç‡
                else:
                    interval = base_interval

                time.sleep(interval)

            except Exception as e:
                error_count += 1
                print(f"ç›‘æ§å¾ªç¯é”™è¯¯ ({error_count}/{max_errors}): {e}")

                # å‘é€é”™è¯¯çŠ¶æ€
                socketio.emit('monitoring_error', {
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })

                # æŒ‡æ•°é€€é¿
                time.sleep(min(base_interval * (2 ** error_count), 30))

        print("ç›‘æ§å¾ªç¯å› é”™è¯¯è¿‡å¤šè€Œåœæ­¢")

    # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œç›‘æ§
    monitoring_thread = threading.Thread(target=monitoring_loop)
    monitoring_thread.daemon = True
    monitoring_thread.start()


@app.route('/api/live_data')
def get_live_data():
    """è·å–å®æ—¶ç³»ç»Ÿæ•°æ®"""
    try:
        # è·å–å®æ—¶æ•°æ®
        live_data = live_connector.get_comprehensive_data()

        if not live_data['system_running']:
            return jsonify({
                'error': 'Estiaç³»ç»Ÿæœªè¿è¡Œæˆ–æ•°æ®åº“ä¸å¯è®¿é—®',
                'system_status': 'offline',
                'timestamp': datetime.now().isoformat()
            })

        # è½¬æ¢ä¸ºä»ªè¡¨æ¿æ ¼å¼
        memory_stats = live_data.get('memory_stats', {})
        session_stats = live_data.get('session_stats', {})
        health = live_data.get('system_health', {})

        # æ„å»ºä»ªè¡¨æ¿æ•°æ®
        dashboard_data = {
            'timestamp': datetime.now().isoformat(),
            'has_data': True,
            'live_mode': True,
            'status': {
                'status': {
                    'status': 'running' if health.get('system_status') == 'active' else 'idle',
                    'session_id': session_stats.get('recent_sessions', [{}])[0].get('session_id', 'æ— ') if session_stats.get('recent_sessions') else 'æ— ',
                    'running_time': 0,  # æ— æ³•ä»æ•°æ®åº“è·å–è¿è¡Œæ—¶é—´
                    'progress_percentage': 0
                },
                'summary': {
                    'total_sessions': session_stats.get('total_sessions', 0),
                    'average_duration': 0,  # æ— æ³•ä»å½“å‰æ•°æ®è®¡ç®—
                    'success_rate': 1.0,  # å‡è®¾æˆåŠŸç‡
                    'slowest_step': None
                }
            },
            'keywords': extract_keywords_from_memories(memory_stats.get('recent_memories', [])),
            'sessions': {
                'sessions': format_sessions_for_display(session_stats.get('recent_dialogues', [])),
                'total': session_stats.get('total_sessions', 0)
            },
            'memory': {
                'average_similarity': 0.75,  # ä¼°ç®—å€¼
                'memory_usage_stats': {
                    'retrieved': memory_stats.get('total_memories', 0),
                    'associations': memory_stats.get('today_memories', 0)
                },
                'total_retrievals': memory_stats.get('total_memories', 0),
                'similarity_distribution': {
                    'é«˜ (>0.8)': memory_stats.get('weight_distribution', {}).get('high_weight', 0),
                    'ä¸­ (0.6-0.8)': memory_stats.get('weight_distribution', {}).get('medium_weight', 0),
                    'ä½ (<0.6)': memory_stats.get('weight_distribution', {}).get('low_weight', 0)
                }
            }
        }

        # ä½¿ç”¨å¢å¼ºç›‘æ§ç³»ç»Ÿå¢å¼ºå®æ—¶æ•°æ®
        try:
            dashboard_data = enhance_dashboard_data(dashboard_data)
            print("âœ… å®æ—¶æ•°æ®å·²å¢å¼º")
        except Exception as e:
            print(f"âš ï¸ å®æ—¶æ•°æ®å¢å¼ºå¤±è´¥: {e}")

        return jsonify(dashboard_data)

    except Exception as e:
        return jsonify({
            'error': f'è·å–å®æ—¶æ•°æ®å¤±è´¥: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })


def extract_keywords_from_memories(memories: List[Dict]) -> Dict[str, Any]:
    """ä»è®°å¿†ä¸­æå–å…³é”®è¯"""
    try:
        import re
        from collections import Counter

        # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
        all_text = ' '.join([mem.get('content', '') for mem in memories])

        # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆæå–2-4å­—çš„è¯æ±‡ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,4}', all_text)

        # ç»Ÿè®¡è¯é¢‘
        word_counts = Counter(chinese_words)

        # è¿‡æ»¤å¸¸è§åœç”¨è¯
        stop_words = {'è¿™ä¸ª', 'é‚£ä¸ª', 'å¯ä»¥', 'éœ€è¦', 'åº”è¯¥', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å', 'ç°åœ¨', 'æ—¶å€™', 'é—®é¢˜', 'æ–¹æ³•', 'ç³»ç»Ÿ', 'åŠŸèƒ½'}
        filtered_words = {word: count for word, count in word_counts.items() if word not in stop_words and count > 1}

        # è·å–å‰10ä¸ªå…³é”®è¯
        top_words = sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)[:10]

        return {
            'top_keywords': [
                {'word': word, 'count': count, 'frequency': count / len(memories) if memories else 0}
                for word, count in top_words
            ],
            'total_unique_keywords': len(filtered_words),
            'keyword_distribution': dict(top_words)
        }

    except Exception as e:
        return {
            'top_keywords': [],
            'total_unique_keywords': 0,
            'keyword_distribution': {},
            'error': str(e)
        }


def format_sessions_for_display(dialogues: List[Dict]) -> List[Dict]:
    """æ ¼å¼åŒ–ä¼šè¯æ•°æ®ç”¨äºæ˜¾ç¤º"""
    try:
        formatted_sessions = []

        # æŒ‰ä¼šè¯IDåˆ†ç»„å¯¹è¯
        session_groups = {}
        for dialogue in dialogues:
            session_id = dialogue.get('session_id', 'unknown')
            if session_id not in session_groups:
                session_groups[session_id] = []
            session_groups[session_id].append(dialogue)

        # ä¸ºæ¯ä¸ªä¼šè¯åˆ›å»ºæ˜¾ç¤ºæ¡ç›®
        for session_id, session_dialogues in list(session_groups.items())[:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ªä¼šè¯
            # æŒ‰æ—¶é—´æ’åº
            session_dialogues.sort(key=lambda x: x.get('timestamp', 0))

            # æ‰¾åˆ°ç”¨æˆ·è¾“å…¥å’ŒAIå›å¤
            user_inputs = [d for d in session_dialogues if d.get('type') == 'user_input']
            ai_responses = [d for d in session_dialogues if d.get('type') == 'assistant_reply']

            # è·å–æœ€æ–°çš„ç”¨æˆ·è¾“å…¥å’ŒAIå›å¤
            latest_user_input = user_inputs[-1] if user_inputs else None
            latest_ai_response = ai_responses[-1] if ai_responses else None

            user_content = latest_user_input.get('content', 'æ— ç”¨æˆ·è¾“å…¥') if latest_user_input else 'æ— ç”¨æˆ·è¾“å…¥'
            ai_content = latest_ai_response.get('content', 'æ— AIå›å¤') if latest_ai_response else 'æ— AIå›å¤'

            # è®¡ç®—ä¼šè¯æŒç»­æ—¶é—´
            if session_dialogues:
                start_time = min(d.get('timestamp', 0) for d in session_dialogues)
                end_time = max(d.get('timestamp', 0) for d in session_dialogues)
                duration = max(0.1, end_time - start_time)  # è‡³å°‘0.1ç§’
                start_time_str = datetime.fromtimestamp(start_time).isoformat()
            else:
                duration = 0.1
                start_time_str = datetime.now().isoformat()

            formatted_sessions.append({
                'session_id': session_id,
                'start_time': start_time_str,
                'duration': duration,
                'success_count': len(ai_responses),
                'failed_count': 0,  # å‡è®¾æ²¡æœ‰å¤±è´¥
                'user_input': user_content[:100] + '...' if len(user_content) > 100 else user_content,
                'ai_response': ai_content[:100] + '...' if len(ai_content) > 100 else ai_content
            })

        return formatted_sessions

    except Exception as e:
        print(f"æ ¼å¼åŒ–ä¼šè¯æ•°æ®å¤±è´¥: {e}")
        return []


@app.route('/api/generate_test_data')
def generate_test_data():
    """ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰"""
    try:
        import random

        # ç”Ÿæˆæ¨¡æ‹Ÿä¼šè¯æ•°æ®
        test_sessions = []
        for i in range(10):
            session_time = datetime.now() - timedelta(minutes=random.randint(1, 60))
            test_sessions.append({
                'session_id': f'test_session_{i+1}',
                'start_time': session_time.isoformat(),
                'duration': random.uniform(0.5, 3.0),
                'success_count': random.randint(8, 14),
                'failed_count': random.randint(0, 2),
                'user_input': f'æµ‹è¯•æŸ¥è¯¢ {i+1}: è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æŸ¥è¯¢',
                'ai_response': f'è¿™æ˜¯æµ‹è¯•å›å¤ {i+1}...'
            })

        # ç”Ÿæˆæ¨¡æ‹Ÿå…³é”®è¯æ•°æ®
        test_keywords = {
            'top_keywords': [
                {'word': 'æµ‹è¯•', 'count': 15, 'frequency': 0.8},
                {'word': 'æŸ¥è¯¢', 'count': 12, 'frequency': 0.6},
                {'word': 'æ•°æ®', 'count': 10, 'frequency': 0.5},
                {'word': 'ç›‘æ§', 'count': 8, 'frequency': 0.4},
                {'word': 'ç³»ç»Ÿ', 'count': 6, 'frequency': 0.3}
            ],
            'total_unique_keywords': 25,
            'keyword_distribution': {
                'æµ‹è¯•': 15, 'æŸ¥è¯¢': 12, 'æ•°æ®': 10, 'ç›‘æ§': 8, 'ç³»ç»Ÿ': 6
            }
        }

        # ç”Ÿæˆæ¨¡æ‹Ÿè®°å¿†åˆ†ææ•°æ®
        test_memory = {
            'average_similarity': 0.75,
            'memory_usage_stats': {
                'retrieved': 45,
                'associations': 23,
                'context_memories': 12
            },
            'total_retrievals': 45,
            'similarity_distribution': {
                'é«˜ (>0.8)': 15,
                'ä¸­ (0.6-0.8)': 20,
                'ä½ (<0.6)': 10
            }
        }

        # ç”Ÿæˆæ¨¡æ‹ŸçŠ¶æ€æ•°æ®
        test_status = {
            'status': {
                'status': 'idle',
                'session_id': None,
                'running_time': random.uniform(10, 100),
                'progress_percentage': 0
            },
            'summary': {
                'total_sessions': 10,
                'average_duration': 1.5,
                'success_rate': 0.92,
                'slowest_step': {
                    'step': 'step_5_faiss_search',
                    'avg_duration': 0.234
                }
            }
        }

        result = {
            'timestamp': datetime.now().isoformat(),
            'has_data': True,
            'test_mode': True,
            'status': test_status,
            'keywords': test_keywords,
            'sessions': {
                'sessions': test_sessions,
                'total': len(test_sessions)
            },
            'memory': test_memory
        }

        return jsonify(result)

    except Exception as e:
        return jsonify({
            'error': f'ç”Ÿæˆæµ‹è¯•æ•°æ®å¤±è´¥: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })


# =================================
# æ–°å¢: Webç›‘æ§é‡æ„APIç«¯ç‚¹ (éä¾µå…¥å¼)
# =================================

@app.route('/api/health')
def api_health_check():
    """APIå¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return jsonify({
        'status': 'ok',
        'timestamp': datetime.now().isoformat(),
        'monitor_available': monitor is not None,
        'monitor_type': type(monitor).__name__ if monitor else 'None'
    })

@app.route('/api/session/<session_id>/context')
def get_session_context(session_id: str):
    """è·å–æŒ‡å®šä¼šè¯çš„å®Œæ•´ä¸Šä¸‹æ–‡æ„å»ºè¿‡ç¨‹ï¼ˆéä¾µå…¥å¼è¯»å–ï¼‰"""
    try:
        # å®‰å…¨åœ°è·å–ç›‘æ§æ•°æ®
        if not monitor:
            return jsonify({
                'error': 'ç›‘æ§ç³»ç»Ÿæœªåˆå§‹åŒ–',
                'timestamp': datetime.now().isoformat()
            }), 503
            
        # ä»ç°æœ‰ç›‘æ§ç³»ç»Ÿä¸­è¯»å–æ•°æ®
        sessions = getattr(monitor, 'completed_sessions', [])
        
        # æŸ¥æ‰¾æŒ‡å®šä¼šè¯
        target_session = None
        for session in sessions:
            if hasattr(session, 'session_id') and session.session_id == session_id:
                target_session = session
                break
        
        if not target_session:
            return jsonify({
                'error': f'ä¼šè¯ {session_id} æœªæ‰¾åˆ°',
                'available_sessions': [s.session_id for s in sessions if hasattr(s, 'session_id')]
            }), 404
        
        # æå–ä¸Šä¸‹æ–‡æ„å»ºç›¸å…³çš„æ­¥éª¤æ•°æ®
        context_data = {
            'session_id': session_id,
            'timestamp': datetime.now().isoformat(),
            'preprocessing': {},
            'memory_retrieval': {},
            'history_aggregation': {},
            'final_context': {}
        }
        
        # ä»ç›‘æ§æ•°æ®ä¸­æå–å„æ­¥éª¤ä¿¡æ¯
        for step, metrics in target_session.steps.items():
            if step == MemoryPipelineStep.STEP_4_CACHE_VECTORIZE:
                context_data['preprocessing'] = {
                    'query_processed': metrics.metadata.get('query_text', ''),
                    'keywords_extracted': metrics.metadata.get('keywords', []),
                    'vector_dimension': metrics.metadata.get('vector_dim', 0),
                    'processing_time': metrics.duration
                }
            
            elif step == MemoryPipelineStep.STEP_5_FAISS_SEARCH:
                context_data['memory_retrieval'] = {
                    'retrieved_memories': metrics.metadata.get('memories', []),
                    'similarity_scores': metrics.metadata.get('similarities', []),
                    'retrieval_count': metrics.metadata.get('result_count', 0),
                    'search_time': metrics.duration,
                    'avg_similarity': metrics.metadata.get('avg_similarity', 0)
                }
            
            elif step == MemoryPipelineStep.STEP_6_ASSOCIATION_EXPAND:
                context_data['memory_retrieval']['associations'] = {
                    'expanded_memories': metrics.metadata.get('expanded_memories', []),
                    'association_count': metrics.metadata.get('expansion_count', 0),
                    'association_strength': metrics.metadata.get('avg_strength', 0)
                }
            
            elif step == MemoryPipelineStep.STEP_7_HISTORY_AGGREGATE:
                context_data['history_aggregation'] = {
                    'historical_dialogues': metrics.metadata.get('dialogues', []),
                    'dialogue_count': metrics.metadata.get('dialogue_count', 0),
                    'relevance_scores': metrics.metadata.get('relevance_scores', []),
                    'aggregation_time': metrics.duration
                }
            
            elif step == MemoryPipelineStep.STEP_9_CONTEXT_BUILD:
                context_data['final_context'] = {
                    'complete_context': metrics.metadata.get('final_context', ''),
                    'context_length': metrics.metadata.get('context_length', 0),
                    'token_count': metrics.metadata.get('token_count', 0),
                    'memory_count': metrics.metadata.get('memory_used', 0),
                    'build_time': metrics.duration,
                    'context_structure': {
                        'system_prompt': metrics.metadata.get('system_prompt', ''),
                        'retrieved_memories': metrics.metadata.get('formatted_memories', []),
                        'historical_context': metrics.metadata.get('historical_context', ''),
                        'user_input': metrics.metadata.get('user_input', '')
                    }
                }
        
        return jsonify(context_data)
        
    except Exception as e:
        return jsonify({
            'error': f'è·å–ä¼šè¯ä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}',
            'session_id': session_id,
            'timestamp': datetime.now().isoformat()
        }), 500


@app.route('/api/session/<session_id>/evaluation')
def get_session_evaluation(session_id: str):
    """è·å–æŒ‡å®šä¼šè¯çš„å¼‚æ­¥è¯„ä¼°ç»“æœï¼ˆéä¾µå…¥å¼è¯»å–ï¼‰"""
    try:
        # å®‰å…¨åœ°è·å–ç›‘æ§æ•°æ®
        if not monitor:
            return jsonify({
                'error': 'ç›‘æ§ç³»ç»Ÿæœªåˆå§‹åŒ–',
                'timestamp': datetime.now().isoformat()
            }), 503
            
        # ä»ç°æœ‰ç›‘æ§ç³»ç»Ÿä¸­è¯»å–è¯„ä¼°æ•°æ®
        sessions = getattr(monitor, 'completed_sessions', [])
        
        target_session = None
        for session in sessions:
            if hasattr(session, 'session_id') and session.session_id == session_id:
                target_session = session
                break
        
        if not target_session:
            return jsonify({'error': f'ä¼šè¯ {session_id} æœªæ‰¾åˆ°'}), 404
        
        evaluation_data = {
            'session_id': session_id,
            'timestamp': datetime.now().isoformat(),
            'evaluation_context': {},
            'evaluation_results': {},
            'association_creation': {}
        }
        
        # æå–å¼‚æ­¥è¯„ä¼°ç›¸å…³æ•°æ®
        for step, metrics in target_session.steps.items():
            if step == MemoryPipelineStep.STEP_12_ASYNC_EVALUATE:
                evaluation_data['evaluation_context'] = {
                    'user_input': metrics.metadata.get('user_input', ''),
                    'assistant_response': metrics.metadata.get('assistant_response', ''),
                    'evaluation_prompt': metrics.metadata.get('evaluation_prompt', ''),
                    'model_used': metrics.metadata.get('model_used', ''),
                    'evaluation_time': metrics.duration
                }
                
                evaluation_data['evaluation_results'] = {
                    'importance_score': metrics.metadata.get('importance_score', 0),
                    'importance_reason': metrics.metadata.get('importance_reason', ''),
                    'emotion_analysis': metrics.metadata.get('emotion_analysis', {}),
                    'topic_tags': metrics.metadata.get('topic_tags', []),
                    'knowledge_extracted': metrics.metadata.get('knowledge_extracted', []),
                    'association_suggestions': metrics.metadata.get('association_suggestions', [])
                }
            
            elif step == MemoryPipelineStep.STEP_14_CREATE_ASSOCIATIONS:
                evaluation_data['association_creation'] = {
                    'new_associations': metrics.metadata.get('new_associations', []),
                    'association_count': metrics.metadata.get('association_count', 0),
                    'association_types': metrics.metadata.get('association_types', []),
                    'creation_time': metrics.duration
                }
        
        return jsonify(evaluation_data)
        
    except Exception as e:
        return jsonify({
            'error': f'è·å–è¯„ä¼°æ•°æ®å¤±è´¥: {str(e)}',
            'session_id': session_id,
            'timestamp': datetime.now().isoformat()
        }), 500


@app.route('/api/pipeline/status')
def get_pipeline_status():
    """è·å–15æ­¥æµç¨‹çš„å®æ—¶çŠ¶æ€ï¼ˆéä¾µå…¥å¼è¯»å–ï¼‰"""
    try:
        # å®‰å…¨åœ°è·å–ç›‘æ§æ•°æ®
        if not monitor:
            return jsonify({
                'error': 'ç›‘æ§ç³»ç»Ÿæœªåˆå§‹åŒ–',
                'timestamp': datetime.now().isoformat()
            }), 503
            
        # è·å–å½“å‰æ­£åœ¨æ‰§è¡Œå’Œæœ€è¿‘å®Œæˆçš„ä¼šè¯
        active_sessions = getattr(monitor, 'active_sessions', {})
        completed_sessions = getattr(monitor, 'completed_sessions', [])
        recent_sessions = completed_sessions[-5:] if completed_sessions else []
        
        pipeline_status = {
            'timestamp': datetime.now().isoformat(),
            'active_sessions': len(active_sessions),
            'phase_status': {
                'initialization': {'status': 'completed', 'progress': 100},
                'query_enhancement': {'status': 'idle', 'progress': 0},
                'storage_evaluation': {'status': 'idle', 'progress': 0}
            },
            'step_status': {},
            'current_step': None,
            'performance_metrics': {}
        }
        
        # å¦‚æœæœ‰æ´»è·ƒä¼šè¯ï¼Œæ˜¾ç¤ºå®æ—¶çŠ¶æ€
        if active_sessions:
            current_session = next(iter(active_sessions.values()))
            current_step = getattr(current_session, 'current_step', None)
            pipeline_status['current_step'] = current_step.value if current_step else None
            
            # æ›´æ–°é˜¶æ®µçŠ¶æ€
            if current_step:
                if current_step.value.startswith('step_1') or current_step.value.startswith('step_2') or current_step.value.startswith('step_3'):
                    pipeline_status['phase_status']['initialization']['status'] = 'running'
                elif current_step.value.startswith('step_4') or current_step.value.startswith('step_5') or current_step.value.startswith('step_6') or current_step.value.startswith('step_7') or current_step.value.startswith('step_8') or current_step.value.startswith('step_9'):
                    pipeline_status['phase_status']['query_enhancement']['status'] = 'running'
                else:
                    pipeline_status['phase_status']['storage_evaluation']['status'] = 'running'
        
        # ä»æœ€è¿‘ä¼šè¯ä¸­æå–æ­¥éª¤çŠ¶æ€ç»Ÿè®¡
        step_stats = defaultdict(list)
        for session in recent_sessions:
            for step, metrics in session.steps.items():
                step_stats[step.value].append({
                    'duration': metrics.duration,
                    'status': metrics.status.value,
                    'timestamp': metrics.start_time
                })
        
        # è®¡ç®—æ¯ä¸ªæ­¥éª¤çš„å¹³å‡æ€§èƒ½
        for step_name, metrics_list in step_stats.items():
            successful_metrics = [m for m in metrics_list if m['status'] == 'success']
            if successful_metrics:
                avg_duration = sum(m['duration'] for m in successful_metrics) / len(successful_metrics)
                success_rate = len(successful_metrics) / len(metrics_list)
                
                pipeline_status['step_status'][step_name] = {
                    'avg_duration': round(avg_duration, 3),
                    'success_rate': round(success_rate, 2),
                    'total_executions': len(metrics_list),
                    'last_execution': max(m['timestamp'] for m in metrics_list)
                }
        
        return jsonify(pipeline_status)
        
    except Exception as e:
        return jsonify({
            'error': f'è·å–æµç¨‹çŠ¶æ€å¤±è´¥: {str(e)}',
            'timestamp': datetime.now().isoformat()
        }), 500


@app.route('/api/current_context')
def get_current_context():
    """è·å–å½“å‰æ­£åœ¨æ„å»ºçš„ä¸Šä¸‹æ–‡ï¼ˆéä¾µå…¥å¼è¯»å–ï¼‰"""
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨æ‰§è¡Œçš„ä¼šè¯
        active_sessions = getattr(monitor, 'active_sessions', {})
        
        if not active_sessions:
            return jsonify({
                'message': 'å½“å‰æ²¡æœ‰æ´»è·ƒçš„ä¸Šä¸‹æ–‡æ„å»ºè¿‡ç¨‹',
                'active': False,
                'timestamp': datetime.now().isoformat()
            })
        
        # è·å–ç¬¬ä¸€ä¸ªæ´»è·ƒä¼šè¯
        current_session = next(iter(active_sessions.values()))
        current_step = getattr(current_session, 'current_step', None)
        
        context_data = {
            'active': True,
            'session_id': current_session.session_id,
            'current_step': current_step.value if current_step else None,
            'step_progress': {},
            'partial_context': {},
            'timestamp': datetime.now().isoformat()
        }
        
        # æ ¹æ®å½“å‰æ­¥éª¤ï¼Œæä¾›å·²å®Œæˆçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        completed_steps = getattr(current_session, 'completed_steps', {})
        for step, metrics in completed_steps.items():
            step_name = step.value
            if step_name.startswith('step_4'):
                context_data['partial_context']['preprocessing'] = {
                    'status': 'completed',
                    'query_text': metrics.metadata.get('query_text', ''),
                    'processing_time': metrics.duration
                }
            elif step_name.startswith('step_5'):
                context_data['partial_context']['memory_retrieval'] = {
                    'status': 'completed',
                    'retrieved_count': metrics.metadata.get('result_count', 0),
                    'avg_similarity': metrics.metadata.get('avg_similarity', 0)
                }
        
        return jsonify(context_data)
        
    except Exception as e:
        return jsonify({
            'error': f'è·å–å½“å‰ä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}',
            'timestamp': datetime.now().isoformat()
        }), 500


# =================================
# WebSocketå®æ—¶æ›´æ–°äº‹ä»¶å¤„ç†
# =================================

@socketio.on('connect')
def handle_connect():
    """å¤„ç†WebSocketè¿æ¥"""
    print(f"ğŸ”— WebSocketå®¢æˆ·ç«¯è¿æ¥: {request.sid}")
    emit('connection_status', {
        'status': 'connected',
        'message': 'å·²è¿æ¥åˆ°Estiaç›‘æ§ç³»ç»Ÿ',
        'timestamp': datetime.now().isoformat()
    })

@socketio.on('disconnect')
def handle_disconnect():
    """å¤„ç†WebSocketæ–­å¼€è¿æ¥"""
    print(f"ğŸ”Œ WebSocketå®¢æˆ·ç«¯æ–­å¼€: {request.sid}")

@socketio.on('subscribe_pipeline')
def handle_subscribe_pipeline():
    """è®¢é˜…æµç¨‹çŠ¶æ€æ›´æ–°"""
    try:
        # è·å–å½“å‰æµç¨‹çŠ¶æ€
        if not monitor:
            emit('pipeline_error', {'error': 'ç›‘æ§ç³»ç»Ÿæœªåˆå§‹åŒ–'})
            return
            
        active_sessions = getattr(monitor, 'active_sessions', {})
        completed_sessions = getattr(monitor, 'completed_sessions', [])
        recent_sessions = completed_sessions[-5:] if completed_sessions else []
        
        pipeline_status = {
            'timestamp': datetime.now().isoformat(),
            'active_sessions': len(active_sessions),
            'phase_status': {
                'initialization': {'status': 'completed', 'progress': 100},
                'query_enhancement': {'status': 'idle', 'progress': 0},
                'storage_evaluation': {'status': 'idle', 'progress': 0}
            },
            'step_status': {},
            'current_step': None,
            'performance_metrics': {}
        }
        
        # å‘é€åˆå§‹çŠ¶æ€
        emit('pipeline_status_update', pipeline_status)
        print(f"ğŸ“Š å®¢æˆ·ç«¯ {request.sid} è®¢é˜…æµç¨‹çŠ¶æ€æ›´æ–°")
        
    except Exception as e:
        emit('pipeline_error', {
            'error': f'è®¢é˜…å¤±è´¥: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })

@socketio.on('subscribe_context_updates')
def handle_subscribe_context():
    """è®¢é˜…ä¸Šä¸‹æ–‡æ›´æ–°"""
    try:
        # æ£€æŸ¥å½“å‰æ´»è·ƒä¼šè¯
        if not monitor:
            emit('context_error', {'error': 'ç›‘æ§ç³»ç»Ÿæœªåˆå§‹åŒ–'})
            return
            
        active_sessions = getattr(monitor, 'active_sessions', {})
        
        if active_sessions:
            # æœ‰æ´»è·ƒä¼šè¯ï¼Œå‘é€å½“å‰ä¸Šä¸‹æ–‡
            current_session = next(iter(active_sessions.values()))
            context_update = {
                'active': True,
                'session_id': getattr(current_session, 'session_id', 'unknown'),
                'current_step': getattr(current_session, 'current_step', None),
                'timestamp': datetime.now().isoformat()
            }
        else:
            # æ— æ´»è·ƒä¼šè¯
            context_update = {
                'active': False,
                'message': 'å½“å‰æ²¡æœ‰æ´»è·ƒçš„ä¸Šä¸‹æ–‡æ„å»ºè¿‡ç¨‹',
                'timestamp': datetime.now().isoformat()
            }
        
        emit('context_status_update', context_update)
        print(f"ğŸ“ å®¢æˆ·ç«¯ {request.sid} è®¢é˜…ä¸Šä¸‹æ–‡æ›´æ–°")
        
    except Exception as e:
        emit('context_error', {
            'error': f'è®¢é˜…å¤±è´¥: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })

@socketio.on('get_real_time_metrics')
def handle_real_time_metrics():
    """è·å–å®æ—¶æ€§èƒ½æŒ‡æ ‡"""
    try:
        if not monitor:
            emit('metrics_error', {'error': 'ç›‘æ§ç³»ç»Ÿæœªåˆå§‹åŒ–'})
            return
            
        # è·å–ç³»ç»Ÿç»Ÿè®¡
        sessions = getattr(monitor, 'completed_sessions', [])
        active_sessions = getattr(monitor, 'active_sessions', {})
        
        # è®¡ç®—å…³é”®æŒ‡æ ‡
        total_sessions = len(sessions)
        active_count = len(active_sessions)
        
        # æ€§èƒ½æŒ‡æ ‡
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'session_metrics': {
                'total_sessions': total_sessions,
                'active_sessions': active_count,
                'success_rate': 0.95 if total_sessions > 0 else 0,  # æ¨¡æ‹ŸæˆåŠŸç‡
            },
            'performance_metrics': {
                'avg_response_time': 1.49,  # æ¯«ç§’ï¼Œæ¥è‡ªv6.0æ€§èƒ½æ•°æ®
                'qps': 671.60,  # æ¥è‡ªv6.0æ€§èƒ½æ•°æ®
                'cache_hit_rate': 1.0,  # 100%ç¼“å­˜å‘½ä¸­ç‡
                'cache_acceleration': 588  # 588xåŠ é€Ÿ
            },
            'system_health': {
                'memory_usage': 85.2,  # æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨ç‡
                'cpu_usage': 12.5,     # æ¨¡æ‹ŸCPUä½¿ç”¨ç‡
                'connection_count': 1   # å½“å‰è¿æ¥æ•°
            }
        }
        
        emit('real_time_metrics', metrics)
        
    except Exception as e:
        emit('metrics_error', {
            'error': f'è·å–æŒ‡æ ‡å¤±è´¥: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })

# åå°ä»»åŠ¡ï¼šå®šæœŸæ¨é€æ›´æ–°
import threading
import time

def background_monitoring():
    """åå°ç›‘æ§ä»»åŠ¡ï¼Œå®šæœŸæ¨é€æ›´æ–°"""
    while True:
        try:
            with app.app_context():
                if monitor:
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒä¼šè¯å˜åŒ–
                    active_sessions = getattr(monitor, 'active_sessions', {})
                    
                    # æ¨é€æµç¨‹çŠ¶æ€æ›´æ–°
                    pipeline_status = {
                        'timestamp': datetime.now().isoformat(),
                        'active_sessions': len(active_sessions),
                        'phase_status': {
                            'initialization': {'status': 'completed', 'progress': 100},
                            'query_enhancement': {'status': 'idle', 'progress': 0},
                            'storage_evaluation': {'status': 'idle', 'progress': 0}
                        }
                    }
                    
                    # å¹¿æ’­ç»™æ‰€æœ‰è®¢é˜…çš„å®¢æˆ·ç«¯
                    socketio.emit('pipeline_status_update', pipeline_status)
                    
        except Exception as e:
            print(f"âš ï¸ åå°ç›‘æ§ä»»åŠ¡é”™è¯¯: {e}")
        
        time.sleep(5)  # æ¯5ç§’æ¨é€ä¸€æ¬¡æ›´æ–°

# å¯åŠ¨åå°ç›‘æ§çº¿ç¨‹
def start_background_monitoring():
    """å¯åŠ¨åå°ç›‘æ§çº¿ç¨‹"""
    monitoring_thread = threading.Thread(target=background_monitoring, daemon=True)
    monitoring_thread.start()
    print("ğŸ”„ åå°ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")

def run_dashboard(host='127.0.0.1', port=5000, debug=True):
    """è¿è¡ŒWebä»ªè¡¨æ¿"""
    print("ğŸš€ å¯åŠ¨ Estia AI ä¸€ä½“åŒ–ç›‘æ§ä»ªè¡¨æ¿")
    print("="*60)
    print(f"ğŸŒ Vueå‰ç«¯ + Flaskåç«¯ é›†æˆæœåŠ¡")
    print(f"ğŸ“Š ä¸»ç•Œé¢: http://{host}:{port}")
    print(f"ğŸ” ç›‘æ§ç³»ç»Ÿ: å¢å¼ºæ€§èƒ½ç›‘æ§ + å‘Šè­¦ç®¡ç†")
    print(f"âš¡ Vueå‰ç«¯: å·²é›†æˆæ‰“åŒ…æ–‡ä»¶ï¼Œæ— éœ€å•ç‹¬å¯åŠ¨")
    print(f"ğŸ”„ å®æ—¶ç›‘æ§: WebSocket è¿æ¥å·²å¯ç”¨")
    print()
    print("ğŸ“¡ APIç«¯ç‚¹:")
    print(f"  â€¢ ä»ªè¡¨æ¿æ•°æ®: http://{host}:{port}/api/dashboard_data")
    print(f"  â€¢ ç›‘æ§çŠ¶æ€: http://{host}:{port}/api/monitoring/status")
    print(f"  â€¢ ç³»ç»Ÿå¥åº·: http://{host}:{port}/api/monitoring/health")
    print(f"  â€¢ æ´»è·ƒå‘Šè­¦: http://{host}:{port}/api/monitoring/alerts")
    print()
    print("ğŸ’¡ ç‰¹æ€§:")
    print("  âœ… å®æ—¶ç³»ç»Ÿæ€§èƒ½ç›‘æ§")
    print("  âœ… æ™ºèƒ½å‘Šè­¦ç®¡ç†ç³»ç»Ÿ")
    print("  âœ… ç³»ç»Ÿå¥åº·è¯„åˆ†")
    print("  âœ… Vue + Flask ä¸€ä½“åŒ–éƒ¨ç½²")
    print("="*60)

    # å¯åŠ¨åå°ç›‘æ§
    start_background_monitoring()
    
    # å¯åŠ¨Flaskåº”ç”¨
    socketio.run(app, host=host, port=port, debug=debug)


# Vueå‰ç«¯è·¯ç”±å¤„ç†
@app.route('/')
def serve_vue_app():
    """æœåŠ¡Vueåº”ç”¨çš„ä¸»é¡µ"""
    try:
        return send_file(os.path.join(vue_dist_path, 'index.html'))
    except Exception as e:
        return f"Vueå‰ç«¯ä¸å¯ç”¨: {e}", 404

@app.route('/<path:path>')
def serve_vue_static(path):
    """æœåŠ¡Vueåº”ç”¨çš„é™æ€èµ„æºå’Œè·¯ç”±"""
    try:
        # é¦–å…ˆå°è¯•ä½œä¸ºé™æ€æ–‡ä»¶
        file_path = os.path.join(vue_dist_path, path)
        if os.path.exists(file_path) and os.path.isfile(file_path):
            return send_file(file_path)
        
        # å¦‚æœä¸æ˜¯APIè·¯å¾„ä¸”ä¸æ˜¯é™æ€æ–‡ä»¶ï¼Œè¿”å›index.htmlï¼ˆç”¨äºVueè·¯ç”±ï¼‰
        if not path.startswith('api/') and not path.startswith('socket.io/'):
            return send_file(os.path.join(vue_dist_path, 'index.html'))
        
        # å…¶ä»–æƒ…å†µè¿”å›404
        return "Not Found", 404
        
    except Exception as e:
        return f"èµ„æºä¸å¯ç”¨: {e}", 404

if __name__ == '__main__':
    run_dashboard()