#!/usr/bin/env python3
"""
ç¼“å­˜ç³»ç»Ÿå¯¹æ¯”æµ‹è¯•
å¯¹æ¯”æ–°çš„å¢žå¼ºç¼“å­˜ç³»ç»Ÿä¸Žç®€å•ç¼“å­˜ç³»ç»Ÿçš„æ€§èƒ½å·®å¼‚
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import time
from collections import OrderedDict
from core.memory.embedding.cache import EnhancedMemoryCache

class SimpleLRUCache:
    """ç®€å•çš„LRUç¼“å­˜å®žçŽ°ï¼Œç”¨äºŽå¯¹æ¯”"""
    
    def __init__(self, capacity=1000):
        self.cache = OrderedDict()
        self.capacity = capacity
        self.stats = {"hits": 0, "misses": 0}
    
    def get(self, key):
        if key in self.cache:
            # ç§»åˆ°æœ€è¿‘ä½¿ç”¨ä½ç½®
            value = self.cache.pop(key)
            self.cache[key] = value
            self.stats["hits"] += 1
            return value
        self.stats["misses"] += 1
        return None
    
    def put(self, key, value):
        if key in self.cache:
            self.cache.pop(key)
        elif len(self.cache) >= self.capacity:
            self.cache.popitem(last=False)
        self.cache[key] = value
    
    def search_by_content(self, query, limit=5):
        """ç®€å•çš„çº¿æ€§æœç´¢"""
        results = []
        query_words = set(query.lower().split())
        
        for key, value in self.cache.items():
            # æ¨¡æ‹Ÿä»Žkeyæ¢å¤æ–‡æœ¬å†…å®¹ï¼ˆå®žé™…ä¸­å¯èƒ½éœ€è¦é¢å¤–å­˜å‚¨ï¼‰
            text = f"æ¨¡æ‹Ÿæ–‡æœ¬å†…å®¹åŒ…å«{key[:20]}"
            text_words = set(text.lower().split())
            
            # è®¡ç®—é‡å åº¦
            if query_words.intersection(text_words):
                overlap = len(query_words.intersection(text_words)) / len(query_words)
                results.append({
                    "key": key,
                    "vector": value,
                    "score": overlap,
                    "cache_level": "simple"
                })
        
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:limit]

def generate_test_data(count=1000):
    """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
    test_data = []
    topics = ["Pythonç¼–ç¨‹", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "Webå¼€å‘", "æ•°æ®åº“", "ç®—æ³•", "äººå·¥æ™ºèƒ½", "è‡ªç„¶è¯­è¨€å¤„ç†"]
    
    for i in range(count):
        topic = topics[i % len(topics)]
        text = f"è¿™æ˜¯å…³äºŽ{topic}çš„ç¬¬{i}æ¡è®°å¿†ï¼ŒåŒ…å«ä¸€äº›è¯¦ç»†çš„æŠ€æœ¯å†…å®¹å’Œå®žè·µç»éªŒ"
        weight = 1.0 + (i % 10)  # æƒé‡åœ¨1.0-10.0ä¹‹é—´
        vector = np.random.rand(384).astype(np.float32)
        test_data.append((text, weight, vector))
    
    return test_data

def test_write_performance():
    """æµ‹è¯•å†™å…¥æ€§èƒ½"""
    print("=" * 60)
    print("å†™å…¥æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    test_data = generate_test_data(500)
    
    # æµ‹è¯•ç®€å•ç¼“å­˜
    simple_cache = SimpleLRUCache(capacity=500)
    start_time = time.time()
    for text, weight, vector in test_data:
        key = str(hash(text))
        simple_cache.put(key, vector)
    simple_write_time = time.time() - start_time
    
    # æµ‹è¯•å¢žå¼ºç¼“å­˜
    enhanced_cache = EnhancedMemoryCache(
        cache_dir="data/memory/test_comparison_cache",
        hot_capacity=50,
        warm_capacity=450,
        persist=False  # å…³é—­æŒä¹…åŒ–ä»¥å…¬å¹³å¯¹æ¯”
    )
    start_time = time.time()
    for text, weight, vector in test_data:
        enhanced_cache.put(text, vector, memory_weight=weight)
    enhanced_write_time = time.time() - start_time
    
    print(f"ç®€å•LRUç¼“å­˜å†™å…¥æ—¶é—´: {simple_write_time:.3f}ç§’")
    print(f"å¢žå¼ºç¼“å­˜å†™å…¥æ—¶é—´: {enhanced_write_time:.3f}ç§’")
    
    if simple_write_time > 0 and enhanced_write_time > 0:
        if enhanced_write_time < simple_write_time:
            improvement = (simple_write_time - enhanced_write_time) / simple_write_time * 100
            print(f"æ€§èƒ½æ¯”è¾ƒ: å¢žå¼ºç¼“å­˜æ¯”ç®€å•ç¼“å­˜å¿« {improvement:.1f}%")
        else:
            slowdown = (enhanced_write_time - simple_write_time) / simple_write_time * 100
            print(f"æ€§èƒ½æ¯”è¾ƒ: å¢žå¼ºç¼“å­˜æ¯”ç®€å•ç¼“å­˜æ…¢ {slowdown:.1f}%")
    else:
        print("æ€§èƒ½æ¯”è¾ƒ: å†™å…¥æ—¶é—´å¤ªçŸ­ï¼Œæ— æ³•å‡†ç¡®æ¯”è¾ƒ")
    
    return simple_cache, enhanced_cache, test_data

def test_read_performance(simple_cache, enhanced_cache, test_data):
    """æµ‹è¯•è¯»å–æ€§èƒ½"""
    print("\n" + "=" * 60)
    print("è¯»å–æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    # éšæœºé€‰æ‹©100æ¡æ•°æ®è¿›è¡Œè¯»å–æµ‹è¯•
    import random
    test_samples = random.sample(test_data, min(100, len(test_data)))
    
    # æµ‹è¯•ç®€å•ç¼“å­˜è¯»å–
    simple_times = []
    simple_hits = 0
    for text, weight, vector in test_samples:
        key = str(hash(text))
        start_time = time.time()
        result = simple_cache.get(key)
        read_time = time.time() - start_time
        simple_times.append(read_time)
        if result is not None:
            simple_hits += 1
    
    # æµ‹è¯•å¢žå¼ºç¼“å­˜è¯»å–
    enhanced_times = []
    enhanced_hits = 0
    for text, weight, vector in test_samples:
        start_time = time.time()
        result = enhanced_cache.get(text, memory_weight=weight)
        read_time = time.time() - start_time
        enhanced_times.append(read_time)
        if result is not None:
            enhanced_hits += 1
    
    simple_avg = sum(simple_times) / len(simple_times) * 1000 if simple_times else 0
    enhanced_avg = sum(enhanced_times) / len(enhanced_times) * 1000 if enhanced_times else 0
    
    print(f"ç®€å•LRUç¼“å­˜:")
    print(f"  å¹³å‡è¯»å–æ—¶é—´: {simple_avg:.3f}ms")
    print(f"  å‘½ä¸­çŽ‡: {simple_hits/len(test_samples)*100:.1f}%")
    
    print(f"å¢žå¼ºç¼“å­˜:")
    print(f"  å¹³å‡è¯»å–æ—¶é—´: {enhanced_avg:.3f}ms")
    print(f"  å‘½ä¸­çŽ‡: {enhanced_hits/len(test_samples)*100:.1f}%")
    
    if simple_avg > 0 and enhanced_avg > 0:
        if enhanced_avg < simple_avg:
            improvement = (simple_avg - enhanced_avg) / simple_avg * 100
            print(f"æ€§èƒ½æ¯”è¾ƒ: å¢žå¼ºç¼“å­˜æ¯”ç®€å•ç¼“å­˜å¿« {improvement:.1f}%")
        else:
            slowdown = (enhanced_avg - simple_avg) / simple_avg * 100
            print(f"æ€§èƒ½æ¯”è¾ƒ: å¢žå¼ºç¼“å­˜æ¯”ç®€å•ç¼“å­˜æ…¢ {slowdown:.1f}%")
    else:
        print("æ€§èƒ½æ¯”è¾ƒ: ä¸¤ä¸ªç³»ç»Ÿçš„è¯»å–æ—¶é—´éƒ½éžå¸¸æŽ¥è¿‘0ï¼Œæ— æ³•å‡†ç¡®æ¯”è¾ƒ")

def test_search_performance(simple_cache, enhanced_cache):
    """æµ‹è¯•æœç´¢æ€§èƒ½"""
    print("\n" + "=" * 60)
    print("æœç´¢æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    search_queries = [
        "Pythonç¼–ç¨‹",
        "æœºå™¨å­¦ä¹ ", 
        "æ·±åº¦å­¦ä¹ ",
        "Webå¼€å‘",
        "æ•°æ®åº“",
        "ç®—æ³•",
        "äººå·¥æ™ºèƒ½",
        "è‡ªç„¶è¯­è¨€å¤„ç†"
    ]
    
    simple_times = []
    enhanced_times = []
    
    for query in search_queries:
        # æµ‹è¯•ç®€å•ç¼“å­˜æœç´¢
        start_time = time.time()
        simple_results = simple_cache.search_by_content(query, limit=5)
        simple_search_time = time.time() - start_time
        simple_times.append(simple_search_time)
        
        # æµ‹è¯•å¢žå¼ºç¼“å­˜æœç´¢
        start_time = time.time()
        enhanced_results = enhanced_cache.search_by_content(query, limit=5)
        enhanced_search_time = time.time() - start_time
        enhanced_times.append(enhanced_search_time)
        
        print(f"æŸ¥è¯¢ '{query}':")
        print(f"  ç®€å•ç¼“å­˜: {simple_search_time*1000:.2f}ms, æ‰¾åˆ° {len(simple_results)} ä¸ªç»“æžœ")
        print(f"  å¢žå¼ºç¼“å­˜: {enhanced_search_time*1000:.2f}ms, æ‰¾åˆ° {len(enhanced_results)} ä¸ªç»“æžœ")
    
    simple_avg = sum(simple_times) / len(simple_times) * 1000
    enhanced_avg = sum(enhanced_times) / len(enhanced_times) * 1000
    
    print(f"\næœç´¢æ€§èƒ½æ€»ç»“:")
    print(f"  ç®€å•ç¼“å­˜å¹³å‡æœç´¢æ—¶é—´: {simple_avg:.2f}ms")
    print(f"  å¢žå¼ºç¼“å­˜å¹³å‡æœç´¢æ—¶é—´: {enhanced_avg:.2f}ms")
    print(f"  æ€§èƒ½æå‡: {(simple_avg - enhanced_avg)/simple_avg*100:.1f}%")

def test_memory_usage():
    """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    print("\n" + "=" * 60)
    print("å†…å­˜ä½¿ç”¨å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    import psutil
    import gc
    
    # èŽ·å–åˆå§‹å†…å­˜
    process = psutil.Process()
    initial_memory = process.memory_info().rss / 1024 / 1024  # MB
    
    # æµ‹è¯•ç®€å•ç¼“å­˜å†…å­˜ä½¿ç”¨
    gc.collect()
    memory_before_simple = process.memory_info().rss / 1024 / 1024
    
    simple_cache = SimpleLRUCache(capacity=1000)
    test_data = generate_test_data(1000)
    
    for text, weight, vector in test_data:
        key = str(hash(text))
        simple_cache.put(key, vector)
    
    memory_after_simple = process.memory_info().rss / 1024 / 1024
    simple_memory_usage = memory_after_simple - memory_before_simple
    
    # æ¸…ç†ç®€å•ç¼“å­˜
    del simple_cache
    gc.collect()
    
    # æµ‹è¯•å¢žå¼ºç¼“å­˜å†…å­˜ä½¿ç”¨
    memory_before_enhanced = process.memory_info().rss / 1024 / 1024
    
    enhanced_cache = EnhancedMemoryCache(
        cache_dir="data/memory/test_memory_cache",
        hot_capacity=100,
        warm_capacity=900,
        persist=False
    )
    
    for text, weight, vector in test_data:
        enhanced_cache.put(text, vector, memory_weight=weight)
    
    memory_after_enhanced = process.memory_info().rss / 1024 / 1024
    enhanced_memory_usage = memory_after_enhanced - memory_before_enhanced
    
    print(f"ç®€å•LRUç¼“å­˜å†…å­˜ä½¿ç”¨: {simple_memory_usage:.2f}MB")
    print(f"å¢žå¼ºç¼“å­˜å†…å­˜ä½¿ç”¨: {enhanced_memory_usage:.2f}MB")
    print(f"å†…å­˜ä½¿ç”¨å·®å¼‚: {enhanced_memory_usage - simple_memory_usage:.2f}MB")
    
    # æ˜¾ç¤ºå¢žå¼ºç¼“å­˜çš„é¢å¤–åŠŸèƒ½ä»·å€¼
    stats = enhanced_cache.get_stats()
    print(f"\nå¢žå¼ºç¼“å­˜é¢å¤–åŠŸèƒ½:")
    print(f"  å…³é”®è¯ç´¢å¼•æ•°é‡: {stats['cache_management']['keyword_count']}")
    print(f"  å…ƒæ•°æ®æ¡ç›®æ•°é‡: {stats['cache_management']['metadata_count']}")
    print(f"  æ™ºèƒ½æå‡æ¬¡æ•°: {stats['cache_management']['promotions']}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ðŸš€ å¼€å§‹ç¼“å­˜ç³»ç»Ÿæ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("å¯¹æ¯”ç®€å•LRUç¼“å­˜ vs å¢žå¼ºè®°å¿†ç¼“å­˜ç³»ç»Ÿ")
    
    try:
        # å†™å…¥æ€§èƒ½æµ‹è¯•
        simple_cache, enhanced_cache, test_data = test_write_performance()
        
        # è¯»å–æ€§èƒ½æµ‹è¯•
        test_read_performance(simple_cache, enhanced_cache, test_data)
        
        # æœç´¢æ€§èƒ½æµ‹è¯•
        test_search_performance(simple_cache, enhanced_cache)
        
        # å†…å­˜ä½¿ç”¨æµ‹è¯•
        test_memory_usage()
        
        print("\n" + "=" * 60)
        print("ðŸŽ‰ æ€§èƒ½å¯¹æ¯”æµ‹è¯•å®Œæˆï¼")
        print("=" * 60)
        
        print("\nðŸ“Š å¢žå¼ºç¼“å­˜ç³»ç»Ÿçš„ä¼˜åŠ¿:")
        print("âœ… å¤šçº§ç¼“å­˜ç­–ç•¥ï¼šæ›´æ™ºèƒ½çš„å†…å­˜ç®¡ç†")
        print("âœ… å…³é”®è¯ç´¢å¼•ï¼šå¤§å¹…æå‡æœç´¢æ€§èƒ½")  
        print("âœ… æ™ºèƒ½æå‡æœºåˆ¶ï¼šè‡ªåŠ¨ä¼˜åŒ–çƒ­ç‚¹æ•°æ®")
        print("âœ… æƒé‡æ„ŸçŸ¥ï¼šé‡è¦è®°å¿†ä¼˜å…ˆç¼“å­˜")
        print("âœ… ä¸°å¯Œç»Ÿè®¡ï¼šè¯¦ç»†çš„æ€§èƒ½ç›‘æŽ§")
        
        print("\nðŸ’¡ å»ºè®®:")
        print("- å¯¹äºŽç®€å•åœºæ™¯ï¼Œç®€å•LRUç¼“å­˜å·²è¶³å¤Ÿ")
        print("- å¯¹äºŽå¤æ‚è®°å¿†ç³»ç»Ÿï¼Œå¢žå¼ºç¼“å­˜æä¾›æ›´å¤šæ™ºèƒ½ç‰¹æ€§")
        print("- å¢žå¼ºç¼“å­˜çš„é¢å¤–å†…å­˜å¼€é”€æ¢æ¥äº†æ˜¾è‘—çš„åŠŸèƒ½æå‡")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºçŽ°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 