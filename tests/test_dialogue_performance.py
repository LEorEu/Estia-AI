#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å¯¹è¯æ€§èƒ½æµ‹è¯•
æ¨¡æ‹Ÿå®é™…ç”¨æˆ·å¯¹è¯çš„å“åº”æ—¶é—´
"""

import os
import sys
import time
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def test_dialogue_response_time():
    """æµ‹è¯•å¯¹è¯å“åº”æ—¶é—´"""
    print("ğŸš€ Estiaå¯¹è¯æ€§èƒ½æµ‹è¯•")
    print("æ¨¡æ‹Ÿå®é™…ç”¨æˆ·å¯¹è¯çš„å“åº”æ—¶é—´")
    print("=" * 60)
    
    try:
        # å‡è®¾ç³»ç»Ÿå·²ç»åˆå§‹åŒ–å®Œæˆï¼Œç°åœ¨æµ‹è¯•å•æ¬¡å¯¹è¯çš„å“åº”æ—¶é—´
        print("ğŸ“‹ æµ‹è¯•åœºæ™¯ï¼šç³»ç»Ÿå·²åˆå§‹åŒ–ï¼Œç”¨æˆ·å‘é€æ–°å¯¹è¯")
        
        # Step 1: æ¨¡æ‹Ÿè¯­éŸ³è½¬æ–‡æœ¬ï¼ˆè¿™é‡Œç›´æ¥ç”¨æ–‡æœ¬ï¼‰
        user_input = "æˆ‘ä»Šå¤©å­¦ä¹ äº†æ·±åº¦å­¦ä¹ ï¼Œæ„Ÿè§‰å¾ˆæœ‰æ”¶è·"
        print(f"ğŸ‘¤ ç”¨æˆ·è¾“å…¥: {user_input}")
        
        # Step 2: æ–‡æœ¬å‘é‡åŒ–ï¼ˆä½¿ç”¨å·²åˆå§‹åŒ–çš„å‘é‡åŒ–å™¨ï¼‰
        print("\nğŸ” Step 1: æ–‡æœ¬å‘é‡åŒ–")
        start_time = time.time()
        
        from core.memory.embedding.vectorizer import TextVectorizer
        vectorizer = TextVectorizer(
            model_type="sentence-transformers",
            model_name="Qwen/Qwen3-Embedding-0.6B",
            cache_dir="data/memory/cache",
            use_cache=True
        )
        
        # å‘é‡åŒ–ç”¨æˆ·è¾“å…¥
        query_vector = vectorizer.encode(user_input)
        vectorization_time = time.time() - start_time
        print(f"   â±ï¸ å‘é‡åŒ–è€—æ—¶: {vectorization_time*1000:.2f}ms")
        
        # Step 3: FAISSæ£€ç´¢ç›¸å…³è®°å¿†
        print("\nğŸ” Step 2: FAISSå‘é‡æ£€ç´¢")
        start_time = time.time()
        
        from core.memory.retrieval.faiss_search import FAISSSearchEngine
        search_engine = FAISSSearchEngine(
            index_path="data/vectors/memory_index.bin",
            dimension=1024,
            cache_dir="data/memory/faiss_cache"
        )
        
        # æ£€ç´¢ç›¸å…³è®°å¿†
        faiss_results = search_engine.search(query_vector, k=5)
        faiss_time = time.time() - start_time
        print(f"   â±ï¸ FAISSæ£€ç´¢è€—æ—¶: {faiss_time*1000:.2f}ms")
        print(f"   ğŸ“„ æ‰¾åˆ° {len(faiss_results)} ä¸ªç›¸å…³è®°å¿†")
        
        # Step 4: å…³è”ç½‘ç»œæ‹“å±•
        print("\nğŸ” Step 3: å…³è”ç½‘ç»œæ‹“å±•")
        start_time = time.time()
        
        from core.memory.storage.memory_store import MemoryStore
        memory_store = MemoryStore(
            db_path="assets/memory.db",
            vector_dim=1024,
            model_name="Qwen/Qwen3-Embedding-0.6B"
        )
        
        from core.memory.association.network import AssociationNetwork
        association_network = AssociationNetwork(db_manager=memory_store.db_manager)
        
        # è·å–å…³è”è®°å¿†
        associated_memories = set()
        for memory_key, similarity in faiss_results:
            try:
                related = association_network.get_related_memories(
                    memory_key, depth=1, min_strength=0.3
                )
                if related:
                    associated_memories.update([r.get('memory_id', r) for r in related])
            except:
                pass
        
        association_time = time.time() - start_time
        print(f"   â±ï¸ å…³è”æ‹“å±•è€—æ—¶: {association_time*1000:.2f}ms")
        print(f"   ğŸ”— æ‰¾åˆ° {len(associated_memories)} ä¸ªå…³è”è®°å¿†")
        
        # Step 5: ä¸Šä¸‹æ–‡ç»„è£…
        print("\nğŸ” Step 4: ä¸Šä¸‹æ–‡ç»„è£…")
        start_time = time.time()
        
        # æ¨¡æ‹Ÿä»æ•°æ®åº“è·å–è®°å¿†å†…å®¹
        primary_memories = [memory_key for memory_key, _ in faiss_results]
        all_memory_keys = list(set(primary_memories + list(associated_memories)))
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = [
            "[ç³»ç»Ÿ] ä½ æ˜¯Estiaï¼Œä¸€ä¸ªå…·æœ‰è®°å¿†èƒ½åŠ›çš„AIåŠ©æ‰‹ã€‚",
            f"[ç›¸å…³è®°å¿†] æ‰¾åˆ° {len(all_memory_keys)} æ¡ç›¸å…³è®°å¿†ã€‚",
            f"[ç”¨æˆ·] {user_input}",
            "[æŒ‡ä»¤] åŸºäºç›¸å…³è®°å¿†ï¼Œç»™å‡ºæœ‰ç”¨çš„å›å¤ã€‚"
        ]
        
        final_context = "\n".join(context_parts)
        context_time = time.time() - start_time
        print(f"   â±ï¸ ä¸Šä¸‹æ–‡ç»„è£…è€—æ—¶: {context_time*1000:.2f}ms")
        
        # Step 6: ä¿å­˜æ–°è®°å¿†ï¼ˆå¼‚æ­¥ï¼‰
        print("\nğŸ” Step 5: ä¿å­˜ç”¨æˆ·è¾“å…¥")
        start_time = time.time()
        
        # æ¨¡æ‹Ÿä¿å­˜ç”¨æˆ·è¾“å…¥ä¸ºæ–°è®°å¿†
        memory_id = memory_store.add_memory(
            content=user_input,
            source="user",
            importance=0.7,
            metadata={
                "type": "user_input",
                "timestamp": time.time(),
                "session_id": "test_session"
            }
        )
        
        save_time = time.time() - start_time
        print(f"   â±ï¸ è®°å¿†ä¿å­˜è€—æ—¶: {save_time*1000:.2f}ms")
        
        # è®¡ç®—æ€»æ—¶é—´
        total_time = vectorization_time + faiss_time + association_time + context_time + save_time
        
        print("\n" + "=" * 60)
        print("ğŸ“Š å¯¹è¯å“åº”æ€§èƒ½åˆ†æ")
        print("=" * 60)
        
        print(f"ğŸ”¤ å‘é‡åŒ–è€—æ—¶:     {vectorization_time*1000:>8.2f}ms")
        print(f"ğŸ” FAISSæ£€ç´¢è€—æ—¶:  {faiss_time*1000:>8.2f}ms") 
        print(f"ğŸ•¸ï¸ å…³è”æ‹“å±•è€—æ—¶:    {association_time*1000:>8.2f}ms")
        print(f"ğŸ“ ä¸Šä¸‹æ–‡ç»„è£…è€—æ—¶:  {context_time*1000:>8.2f}ms")
        print(f"ğŸ’¾ è®°å¿†ä¿å­˜è€—æ—¶:    {save_time*1000:>8.2f}ms")
        print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print(f"âš¡ æ€»å“åº”æ—¶é—´:     {total_time*1000:>8.2f}ms")
        
        # æ€§èƒ½è¯„ä¼°
        print(f"\nğŸ¯ æ€§èƒ½è¯„ä¼°:")
        if total_time < 0.1:
            print("   âœ… ä¼˜ç§€ (<100ms) - å®æ—¶å“åº”")
        elif total_time < 0.5:
            print("   âœ… è‰¯å¥½ (<500ms) - æµç•…å¯¹è¯")
        elif total_time < 1.0:
            print("   âš ï¸ ä¸€èˆ¬ (<1s) - å¯æ¥å—å»¶è¿Ÿ")
        else:
            print("   âŒ è¾ƒæ…¢ (>1s) - éœ€è¦ä¼˜åŒ–")
        
        # ç“¶é¢ˆåˆ†æ
        times = {
            "å‘é‡åŒ–": vectorization_time,
            "FAISSæ£€ç´¢": faiss_time,
            "å…³è”æ‹“å±•": association_time,
            "ä¸Šä¸‹æ–‡ç»„è£…": context_time,
            "è®°å¿†ä¿å­˜": save_time
        }
        
        bottleneck = max(times, key=times.get)
        bottleneck_time = times[bottleneck]
        
        print(f"\nğŸ” æ€§èƒ½ç“¶é¢ˆåˆ†æ:")
        print(f"   æœ€æ…¢ç¯èŠ‚: {bottleneck} ({bottleneck_time*1000:.2f}ms)")
        print(f"   å æ€»æ—¶é—´: {bottleneck_time/total_time*100:.1f}%")
        
        # ä¼˜åŒ–å»ºè®®
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        if vectorization_time > 0.05:
            print("   ğŸ”¤ å‘é‡åŒ–: è€ƒè™‘ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–å¢åŠ ç¼“å­˜")
        if faiss_time > 0.02:
            print("   ğŸ” FAISS: è€ƒè™‘å‡å°‘æ£€ç´¢æ•°é‡æˆ–ä½¿ç”¨æ›´å¿«çš„ç´¢å¼•")
        if association_time > 0.05:
            print("   ğŸ•¸ï¸ å…³è”: è€ƒè™‘é™åˆ¶å…³è”æ·±åº¦æˆ–ä½¿ç”¨ç¼“å­˜")
        if save_time > 0.1:
            print("   ğŸ’¾ ä¿å­˜: è€ƒè™‘å¼‚æ­¥ä¿å­˜æˆ–æ‰¹é‡å¤„ç†")
        
        return total_time
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_multiple_queries():
    """æµ‹è¯•å¤šæ¬¡æŸ¥è¯¢çš„å¹³å‡æ€§èƒ½"""
    print("\n" + "=" * 60)
    print("ğŸ”„ å¤šæ¬¡æŸ¥è¯¢æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    test_queries = [
        "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "æˆ‘æƒ³å­¦ä¹ Pythonç¼–ç¨‹",
        "æ¨èä¸€äº›å¥½çœ‹çš„ç”µå½±",
        "å·¥ä½œå‹åŠ›å¾ˆå¤§æ€ä¹ˆåŠï¼Ÿ",
        "æ·±åº¦å­¦ä¹ çš„åŸºç¡€çŸ¥è¯†"
    ]
    
    times = []
    
    try:
        # é¢„çƒ­ç³»ç»Ÿ
        print("ğŸ”¥ ç³»ç»Ÿé¢„çƒ­ä¸­...")
        from core.memory.embedding.vectorizer import TextVectorizer
        vectorizer = TextVectorizer(
            model_type="sentence-transformers",
            model_name="Qwen/Qwen3-Embedding-0.6B",
            cache_dir="data/memory/cache",
            use_cache=True
        )
        vectorizer.encode("é¢„çƒ­æŸ¥è¯¢")  # é¢„çƒ­
        
        print("ğŸ“Š å¼€å§‹æ€§èƒ½æµ‹è¯•...")
        for i, query in enumerate(test_queries, 1):
            start_time = time.time()
            
            # æ¨¡æ‹Ÿå®Œæ•´çš„æŸ¥è¯¢æµç¨‹
            query_vector = vectorizer.encode(query)
            
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šæ­¥éª¤...
            
            query_time = time.time() - start_time
            times.append(query_time)
            
            print(f"   æŸ¥è¯¢ {i}: {query_time*1000:>6.2f}ms - {query}")
        
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        
        print(f"\nğŸ“ˆ å¤šæŸ¥è¯¢ç»Ÿè®¡:")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {avg_time*1000:.2f}ms")
        print(f"   æœ€å¿«å“åº”æ—¶é—´: {min_time*1000:.2f}ms")
        print(f"   æœ€æ…¢å“åº”æ—¶é—´: {max_time*1000:.2f}ms")
        print(f"   å“åº”æ—¶é—´ç¨³å®šæ€§: {'âœ… ç¨³å®š' if max_time/min_time < 2 else 'âš ï¸ ä¸ç¨³å®š'}")
        
        return avg_time
        
    except Exception as e:
        print(f"âŒ å¤šæŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    # å•æ¬¡å¯¹è¯æ€§èƒ½æµ‹è¯•
    single_time = test_dialogue_response_time()
    
    # å¤šæ¬¡æŸ¥è¯¢æ€§èƒ½æµ‹è¯•
    avg_time = test_multiple_queries()
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ¯ æ€§èƒ½æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    if single_time and avg_time:
        print(f"å•æ¬¡å¯¹è¯å“åº”æ—¶é—´: {single_time*1000:.2f}ms")
        print(f"å¹³å‡æŸ¥è¯¢å“åº”æ—¶é—´: {avg_time*1000:.2f}ms")
        
        if single_time < 0.5:
            print("âœ… ç³»ç»Ÿæ€§èƒ½è‰¯å¥½ï¼Œå¯ä»¥æ”¯æŒæµç•…å¯¹è¯")
        else:
            print("âš ï¸ ç³»ç»Ÿæ€§èƒ½éœ€è¦ä¼˜åŒ–ï¼Œå“åº”è¾ƒæ…¢")
    
    print("\nğŸ’¡ å®é™…å¯¹è¯ä¸­ï¼Œåˆå§‹åŒ–åªéœ€è¦ä¸€æ¬¡ï¼Œåç»­æ¯æ¬¡å¯¹è¯çš„å“åº”ä¼šæ›´å¿«ï¼")

if __name__ == "__main__":
    main() 