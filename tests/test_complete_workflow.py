#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯• - Step 1-6
æµ‹è¯•ä»æ•°æ®åº“åˆå§‹åŒ–åˆ°å†å²å¯¹è¯æ£€ç´¢çš„å®Œæ•´æµç¨‹
"""

import os
import sys
import time
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def test_complete_workflow():
    """æµ‹è¯•å®Œæ•´çš„Step 1-6å·¥ä½œæµç¨‹"""
    print("ğŸš€ Estiaå®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯• (Step 1-6)")
    print("="*60)
    
    try:
        # Step 1: æ•°æ®åº“åˆå§‹åŒ–å’Œå‘é‡ç´¢å¼•æ„å»º
        print("ğŸ”§ Step 1: æ•°æ®åº“åˆå§‹åŒ–å’Œå‘é‡ç´¢å¼•æ„å»º")
        
        from core.memory.init.db_manager import DatabaseManager
        from core.memory.init.vector_index import VectorIndexManager
        
        db_manager = DatabaseManager("assets/memory.db")
        vector_manager = VectorIndexManager("data/vectors/memory_index.bin", vector_dim=1024)
        
        print("   âœ… æ•°æ®åº“å’Œå‘é‡ç´¢å¼•åˆå§‹åŒ–å®Œæˆ")
        
        # Step 2: æ–‡æœ¬å‘é‡åŒ–å’Œç¼“å­˜
        print("\nğŸ”¤ Step 2: æ–‡æœ¬å‘é‡åŒ–å’Œç¼“å­˜")
        
        from core.memory.embedding.vectorizer import TextVectorizer
        
        vectorizer = TextVectorizer(
            model_type="sentence-transformers",
            model_name="Qwen/Qwen3-Embedding-0.6B",
            cache_dir="data/memory/cache",
            use_cache=True
        )
        
        # æµ‹è¯•å‘é‡åŒ–
        test_texts = [
            "æˆ‘ä»Šå¤©å­¦ä¹ äº†Pythonç¼–ç¨‹",
            "å¤©æ°”å¾ˆå¥½ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥",
            "æˆ‘å–œæ¬¢å¬éŸ³ä¹ï¼Œç‰¹åˆ«æ˜¯å¤å…¸éŸ³ä¹",
            "æ˜¨å¤©çœ‹äº†ä¸€éƒ¨å¾ˆå¥½çš„ç”µå½±",
            "æˆ‘æ­£åœ¨å­¦ä¹ æœºå™¨å­¦ä¹ ç®—æ³•"
        ]
        
        start_time = time.time()
        vectors = []
        for text in test_texts:
            vector = vectorizer.encode(text)
            vectors.append(vector)
        
        vectorization_time = time.time() - start_time
        print(f"   âœ… å‘é‡åŒ–å®Œæˆï¼Œ{len(vectors)}ä¸ªå‘é‡ï¼Œè€—æ—¶: {vectorization_time*1000:.2f}ms")
        print(f"   ğŸ“Š å‘é‡ç»´åº¦: {vectors[0].shape}")
        
        # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
        cache_stats = vectorizer.get_cache_stats()
        hot_cache_count = cache_stats.get('cache_levels', {}).get('hot_cache_size', 0)
        keyword_count = cache_stats.get('cache_management', {}).get('keyword_count', 0)
        print(f"   ğŸ“ˆ ç¼“å­˜ç»Ÿè®¡: çƒ­ç¼“å­˜{hot_cache_count}æ¡, "
              f"å…³é”®è¯{keyword_count}ä¸ª")
        
        # Step 3: è®°å¿†å­˜å‚¨
        print("\nğŸ’¾ Step 3: è®°å¿†å­˜å‚¨")
        
        from core.memory.storage.memory_store import MemoryStore
        
        memory_store = MemoryStore(
            db_path="assets/memory.db",
            vector_dim=1024,
            model_name="Qwen/Qwen3-Embedding-0.6B"
        )
        
        # å­˜å‚¨æµ‹è¯•è®°å¿†
        memory_ids = []
        session_id = f"test_session_{int(time.time())}"
        group_id = f"test_group_{int(time.time())}"
        
        for i, text in enumerate(test_texts):
            memory_id = memory_store.add_memory(
                content=text,
                source="user" if i % 2 == 0 else "assistant",
                importance=0.7,
                metadata={
                    "test": True,
                    "session_id": session_id,
                    "group_id": group_id,
                    "type": "user_input" if i % 2 == 0 else "assistant_reply"
                }
            )
            if memory_id:
                memory_ids.append(memory_id)
        
        print(f"   âœ… æˆåŠŸå­˜å‚¨ {len(memory_ids)} æ¡è®°å¿†")
        
        # Step 4: FAISSå‘é‡æ£€ç´¢
        print("\nğŸ” Step 4: FAISSå‘é‡æ£€ç´¢")
        
        from core.memory.retrieval.faiss_search import FAISSSearchEngine
        
        search_engine = FAISSSearchEngine(
            index_path="data/vectors/memory_index.bin",
            dimension=1024,
            cache_dir="data/memory/faiss_cache"
        )
        
        # æµ‹è¯•æ£€ç´¢
        query_text = "å­¦ä¹ ç¼–ç¨‹ç›¸å…³çš„å†…å®¹"
        query_vector = vectorizer.encode(query_text)
        
        start_time = time.time()
        search_results = search_engine.search(query_vector, k=3)
        search_time = time.time() - start_time
        
        print(f"   âœ… FAISSæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç»“æœï¼Œè€—æ—¶: {search_time*1000:.2f}ms")
        
        # Step 5: å…³è”ç½‘ç»œæ‹“å±•
        print("\nğŸ•¸ï¸ Step 5: å…³è”ç½‘ç»œæ‹“å±•")
        
        from core.memory.association.network import AssociationNetwork
        
        association_network = AssociationNetwork(db_manager=db_manager)
        
        # åˆ›å»ºä¸€äº›å…³è”ï¼ˆæ‰‹åŠ¨åˆ›å»ºï¼Œå› ä¸ºcreate_associationå¯èƒ½ä¸å­˜åœ¨ï¼‰
        if len(memory_ids) >= 2:
            try:
                # ç›´æ¥æ’å…¥å…³è”è®°å½•
                import uuid
                assoc_id1 = str(uuid.uuid4())
                assoc_id2 = str(uuid.uuid4())
                
                db_manager.execute_query(
                    """
                    INSERT INTO memory_association 
                    (id, source_key, target_key, association_type, strength, created_at, last_activated)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                    """,
                    (assoc_id1, memory_ids[0], memory_ids[1], "related", 0.8, time.time(), time.time())
                )
                
                db_manager.execute_query(
                    """
                    INSERT INTO memory_association 
                    (id, source_key, target_key, association_type, strength, created_at, last_activated)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                    """,
                    (assoc_id2, memory_ids[0], memory_ids[2], "topic_similar", 0.6, time.time(), time.time())
                )
                
                db_manager.conn.commit()
                print("   âœ… æ‰‹åŠ¨åˆ›å»ºäº†2ä¸ªå…³è”")
            except Exception as e:
                print(f"   âš ï¸ åˆ›å»ºå…³è”å¤±è´¥: {e}")
        
        # æµ‹è¯•å…³è”æ‹“å±•
        expanded_memories = set()
        for memory_key, similarity in search_results:
            try:
                related = association_network.get_related_memories(
                    memory_key, depth=1, min_strength=0.3
                )
                if related:
                    expanded_memories.update([r.get('memory_id', r) for r in related])
            except:
                pass
        
        print(f"   âœ… å…³è”æ‹“å±•å®Œæˆï¼Œæ‰¾åˆ° {len(expanded_memories)} ä¸ªå…³è”è®°å¿†")
        
        # Step 6: å†å²å¯¹è¯æ£€ç´¢
        print("\nğŸ“š Step 6: å†å²å¯¹è¯æ£€ç´¢")
        
        from core.memory.context.history import HistoryRetriever
        
        history_retriever = HistoryRetriever(db_manager)
        
        # ç»„åˆæ‰€æœ‰ç›¸å…³è®°å¿†ID
        all_memory_ids = [memory_key for memory_key, _ in search_results]
        all_memory_ids.extend(list(expanded_memories))
        all_memory_ids = list(set(all_memory_ids))  # å»é‡
        
        if all_memory_ids:
            start_time = time.time()
            retrieval_result = history_retriever.retrieve_memory_contents(
                memory_ids=all_memory_ids,
                include_summaries=True,
                include_sessions=True,
                max_recent_dialogues=10
            )
            retrieval_time = time.time() - start_time
            
            print(f"   âœ… å†å²æ£€ç´¢å®Œæˆï¼Œè€—æ—¶: {retrieval_time*1000:.2f}ms")
            
            # æ˜¾ç¤ºæ£€ç´¢ç»Ÿè®¡
            stats = retrieval_result.get("stats", {})
            print(f"   ğŸ“Š æ£€ç´¢ç»Ÿè®¡:")
            print(f"      â€¢ ä¸»è¦è®°å¿†: {stats.get('total_memories', 0)} æ¡")
            print(f"      â€¢ åˆ†ç»„æ•°é‡: {stats.get('groups_found', 0)} ä¸ª")
            print(f"      â€¢ ä¼šè¯æ•°é‡: {stats.get('sessions_found', 0)} ä¸ª")
            print(f"      â€¢ æ€»ç»“æ•°é‡: {stats.get('summaries_found', 0)} ä¸ª")
            
            # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
            context = history_retriever.format_for_context(retrieval_result, max_context_length=500)
            print(f"   ğŸ“ ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
            
            # æ˜¾ç¤ºéƒ¨åˆ†ä¸Šä¸‹æ–‡
            if context and len(context) > 0:
                context_preview = context[:200] + "..." if len(context) > 200 else context
                print(f"   ğŸ“„ ä¸Šä¸‹æ–‡é¢„è§ˆ: {context_preview}")
        else:
            print("   âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®°å¿†è¿›è¡Œå†å²æ£€ç´¢")
        
        # è®¡ç®—æ€»è€—æ—¶
        print(f"\nğŸ“Š å·¥ä½œæµç¨‹æ€§èƒ½åˆ†æ:")
        print(f"   ğŸ”¤ å‘é‡åŒ–è€—æ—¶: {vectorization_time*1000:.2f}ms")
        print(f"   ğŸ” FAISSæ£€ç´¢è€—æ—¶: {search_time*1000:.2f}ms") 
        if 'retrieval_time' in locals():
            print(f"   ğŸ“š å†å²æ£€ç´¢è€—æ—¶: {retrieval_time*1000:.2f}ms")
            total_time = vectorization_time + search_time + retrieval_time
        else:
            total_time = vectorization_time + search_time
        print(f"   âš¡ æ€»å¤„ç†æ—¶é—´: {total_time*1000:.2f}ms")
        
        # æ¸…ç†æµ‹è¯•æ•°æ®
        print(f"\nğŸ§¹ æ¸…ç†æµ‹è¯•æ•°æ®...")
        for memory_id in memory_ids:
            try:
                db_manager.execute_query("DELETE FROM memories WHERE id = ?", (memory_id,))
                db_manager.execute_query("DELETE FROM memory_vectors WHERE memory_id = ?", (memory_id,))
                db_manager.execute_query("DELETE FROM memory_association WHERE source_key = ? OR target_key = ?", (memory_id, memory_id))
            except:
                pass
        
        if db_manager.conn:
            db_manager.conn.commit()
        print(f"   âœ… æ¸…ç†å®Œæˆ")
        
        print(f"\nğŸ‰ å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•æˆåŠŸï¼")
        print(f"âœ… Step 1: æ•°æ®åº“åˆå§‹åŒ– âœ“")
        print(f"âœ… Step 2: æ–‡æœ¬å‘é‡åŒ– âœ“")
        print(f"âœ… Step 3: è®°å¿†å­˜å‚¨ âœ“")
        print(f"âœ… Step 4: FAISSæ£€ç´¢ âœ“")
        print(f"âœ… Step 5: å…³è”æ‹“å±• âœ“")
        print(f"âœ… Step 6: å†å²æ£€ç´¢ âœ“")
        
        return True
        
    except Exception as e:
        print(f"âŒ å·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª Estiaè®°å¿†ç³»ç»Ÿå®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•")
    print("="*60)
    
    success = test_complete_workflow()
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("="*60)
    
    if success:
        print("ğŸ‰ å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡ï¼")
        print("âœ… è®°å¿†ç³»ç»Ÿçš„Step 1-6åŠŸèƒ½å…¨éƒ¨æ­£å¸¸")
        print("\nğŸ’¡ ç³»ç»Ÿç‰¹æ€§:")
        print("   â€¢ æ•°æ®åº“åˆå§‹åŒ–å’Œå‘é‡ç´¢å¼•ç®¡ç†")
        print("   â€¢ é«˜æ•ˆçš„æ–‡æœ¬å‘é‡åŒ–å’Œç¼“å­˜")
        print("   â€¢ å¯é çš„è®°å¿†å­˜å‚¨å’Œç®¡ç†")
        print("   â€¢ å¿«é€Ÿçš„FAISSå‘é‡æ£€ç´¢")
        print("   â€¢ æ™ºèƒ½çš„å…³è”ç½‘ç»œæ‹“å±•")
        print("   â€¢ å®Œæ•´çš„å†å²å¯¹è¯æ£€ç´¢")
        print("\nğŸš€ ç³»ç»Ÿå·²å‡†å¤‡å¥½å¤„ç†å®é™…å¯¹è¯ï¼")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥")
        print("ğŸ’¡ è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤é—®é¢˜")

if __name__ == "__main__":
    main() 