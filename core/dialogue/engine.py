# core/dialogue_engine.py

"""
æœ¬æ¨¡å—æ˜¯ AI çš„"æ€è€ƒ"æ ¸å¿ƒã€‚ï¼ˆV3.0 - å¤šæ¨¡å‹å…¼å®¹ç‰ˆï¼‰
å®ƒè´Ÿè´£æ¥æ”¶ç”¨æˆ·çš„æ–‡æœ¬è¾“å…¥ï¼Œæ ¹æ®é…ç½®å†³å®šè°ƒç”¨æœ¬åœ°æˆ–äº‘ç«¯å¤§æ¨¡å‹APIï¼Œ
å¹¶è§£æè¿”å›çš„å›å¤ã€‚æ”¯æŒå¤šç§æ¨¡å‹æä¾›å•†ï¼šæœ¬åœ°æ¨¡å‹ã€OpenAIã€DeepSeekã€‚
"""

# -----------------------------------------------------------------------------
# å¯¼å…¥å¿…è¦çš„åº“
# -----------------------------------------------------------------------------

import os
# é¢„å…ˆè®¾ç½®ç¯å¢ƒå˜é‡æ¥ä½¿ç”¨é•œåƒç«™ç‚¹
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import requests                 # å¯¼å…¥ requests åº“ï¼Œç”¨äºå‘é€ HTTP API è¯·æ±‚ã€‚
import json                     # å¯¼å…¥ json åº“ï¼Œç”¨äºå¤„ç† JSON æ•°æ®æ ¼å¼ã€‚
from config import settings     # ä»æˆ‘ä»¬çš„é…ç½®æ–‡ä»¶ä¸­å¯¼å…¥ settingsã€‚
import time                     # å¯¼å…¥ time åº“ï¼Œç”¨äºæ ¼å¼åŒ–æ—¶é—´æˆ³ã€‚
import logging
from core.dialogue.personality import get_fallback_prompt, get_estia_persona

# è®¾ç½®æ—¥å¿—
log_dir = getattr(settings, 'LOG_DIR', './logs')
os.makedirs(log_dir, exist_ok=True)

# è·å–logger
logger = logging.getLogger('dialogue_engine')

# è®¾ç½®æ—¥å¿—æ ¼å¼
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
file_handler = logging.FileHandler(
    os.path.join(log_dir, 'dialogue_engine.log'),
    encoding='utf-8'  # æŒ‡å®šUTF-8ç¼–ç 
)
file_handler.setFormatter(formatter)
file_handler.setLevel(getattr(settings, 'LOG_LEVEL', 'INFO'))

# æ·»åŠ å¤„ç†å™¨
logger.addHandler(file_handler)
logger.setLevel(getattr(settings, 'LOG_LEVEL', 'INFO'))

# æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯¼å…¥OpenAIåº“
if settings.MODEL_PROVIDER.lower() in ["openai", "deepseek"]:
    try:
        import openai
        logger.info(f"å·²åŠ è½½OpenAIåº“ï¼Œä½¿ç”¨{settings.MODEL_PROVIDER}æä¾›å•†")
    except ImportError:
        logger.error("æœªæ‰¾åˆ°OpenAIåº“ã€‚è¯·ä½¿ç”¨ 'pip install openai' å®‰è£…")
        # ä½¿ç”¨åŸºæœ¬çš„requestsåº“ä½œä¸ºåå¤‡

try:
    import google.generativeai as genai
    from google.api_core import client_options
    logger.info("å·²åŠ è½½ Google Generative AI SDKã€‚")
except ImportError:
    logger.warning("æœªæ‰¾åˆ° Google Generative AI SDKã€‚å¦‚æœéœ€è¦ä½¿ç”¨Geminiï¼Œè¯·è¿è¡Œ 'pip install google-generativeai'")

# -----------------------------------------------------------------------------
# å¯¹è¯å¼•æ“ç±»å®šä¹‰
# -----------------------------------------------------------------------------

class DialogueEngine:
    """å¯¹è¯å¼•æ“ç±»ï¼Œå°è£…LLMäº¤äº’åŠŸèƒ½"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¯¹è¯å¼•æ“"""
        self.logger = logger
        self.logger.info("å¯¹è¯å¼•æ“åˆå§‹åŒ–")
        
    def generate_response(self, user_query, memory_context=None):
        """
        ç”Ÿæˆå¯¹è¯å›å¤
        
        å‚æ•°:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            memory_context: å·²æ„å»ºçš„å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆç”± ContextLengthManager æ„å»ºï¼‰
        
        è¿”å›:
            ç”Ÿæˆçš„å›å¤
        """
        # ç›´æ¥ä½¿ç”¨å·²æ„å»ºçš„å®Œæ•´ä¸Šä¸‹æ–‡
        if memory_context:
            # memory_context å·²ç»æ˜¯ ContextLengthManager æ„å»ºçš„å®Œæ•´ä¸Šä¸‹æ–‡
            # åŒ…å«ï¼šè§’è‰²è®¾å®šã€å½“å‰ä¼šè¯ã€æ ¸å¿ƒè®°å¿†ã€å†å²å¯¹è¯ã€ç›¸å…³è®°å¿†ã€é‡è¦æ€»ç»“ã€ç”¨æˆ·è¾“å…¥
            full_prompt = memory_context
        else:
            # é™çº§æ–¹æ¡ˆï¼šæ²¡æœ‰ä¸Šä¸‹æ–‡æ—¶ä½¿ç”¨åŸºç¡€æ¨¡æ¿
            full_prompt = get_fallback_prompt(user_query)

        # ç›´æ¥è°ƒç”¨LLMï¼Œä¸è¿›è¡ŒäºŒæ¬¡åŒ…è£…
        response = self._get_llm_response(full_prompt)
        return response
        
    def generate_response_stream(self, user_query, memory_context=None):
        """
        æµå¼ç”Ÿæˆå¯¹è¯å›å¤
        
        å‚æ•°:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            memory_context: å·²æ„å»ºçš„å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆç”± ContextLengthManager æ„å»ºï¼‰
        
        è¿”å›:
            ç”Ÿæˆçš„å®Œæ•´å›å¤
        """
        # ç›´æ¥ä½¿ç”¨å·²æ„å»ºçš„å®Œæ•´ä¸Šä¸‹æ–‡
        if memory_context:
            # memory_context å·²ç»æ˜¯ ContextLengthManager æ„å»ºçš„å®Œæ•´ä¸Šä¸‹æ–‡
            # åŒ…å«ï¼šè§’è‰²è®¾å®šã€å½“å‰ä¼šè¯ã€æ ¸å¿ƒè®°å¿†ã€å†å²å¯¹è¯ã€ç›¸å…³è®°å¿†ã€é‡è¦æ€»ç»“ã€ç”¨æˆ·è¾“å…¥
            full_prompt = memory_context
        else:
            # é™çº§æ–¹æ¡ˆï¼šæ²¡æœ‰ä¸Šä¸‹æ–‡æ—¶ä½¿ç”¨åŸºç¡€æ¨¡æ¿
            full_prompt = get_fallback_prompt(user_query)

        # ç›´æ¥è°ƒç”¨LLMæµå¼ç”Ÿæˆï¼Œä¸è¿›è¡ŒäºŒæ¬¡åŒ…è£…
        return self._get_llm_response_stream(full_prompt)
        
    def _get_llm_response(self, prompt, history=None, personality=""):
        """
        ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå›å¤
        
        å‚æ•°:
            prompt: æç¤ºæ–‡æœ¬ï¼ˆå¯ä»¥æ˜¯å®Œæ•´çš„ä¸Šä¸‹æ–‡æˆ–ç®€å•æç¤ºï¼‰
            history: å†å²å¯¹è¯ (å¯é€‰ï¼Œç”¨äºå…¼å®¹æ€§)
            personality: äººæ ¼è®¾å®š (å¯é€‰ï¼Œç”¨äºå…¼å®¹æ€§)
        
        è¿”å›:
            æ¨¡å‹ç”Ÿæˆçš„å›å¤
        """
        if history is None:
            history = []
        
        # æ„å»ºæ¶ˆæ¯æ•°ç»„
        messages = []
        
        # æ·»åŠ äººæ ¼è®¾å®š (å¦‚æœæœ‰)
        if personality:
            messages.append({
                "role": "system",
                "content": personality
            })
        
        # æ·»åŠ å†å²å¯¹è¯
        for entry in history:
            messages.append({
                "role": entry.get("role", "user"),
                "content": entry.get("content", "")
            })
        
        # æ·»åŠ å½“å‰æç¤º
        # å¦‚æœ prompt å·²ç»æ˜¯å®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è§’è‰²è®¾å®šç­‰ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
        # å¦åˆ™ä½œä¸ºç”¨æˆ·æ¶ˆæ¯å¤„ç†
        if prompt.strip().startswith(('[ç³»ç»Ÿè§’è‰²è®¾å®š]', get_estia_persona()[:10], '[è§’è‰²è®¾å®š]')) or len(prompt) > 500:
            # è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼Œç›´æ¥ä½œä¸ºç”¨æˆ·æ¶ˆæ¯å‘é€
            messages.append({
                "role": "user", 
                "content": prompt
            })
        else:
            # è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æç¤ºæˆ–è¯„ä¼°è¯·æ±‚
            messages.append({
                "role": "user",
                "content": prompt
            })
        
        # æ ¹æ®æä¾›å•†é€‰æ‹©é€‚å½“çš„APIè°ƒç”¨æ–¹æ³•
        provider = settings.MODEL_PROVIDER.lower()
        
        # è¯·æ±‚LLM
        try:
            self.logger.debug(f"ä½¿ç”¨{provider}æä¾›å•†å‘é€è¯·æ±‚ï¼Œæ¶ˆæ¯æ•°: {len(messages)}")
            
            if provider == "local":
                # ä½¿ç”¨æœ¬åœ°LLM API
                return self._call_local_llm(messages)
            elif provider == "openai":
                # ä½¿ç”¨OpenAI API
                return self._call_openai_api(messages)
            elif provider == "deepseek":
                # ä½¿ç”¨DeepSeek API
                return self._call_deepseek_api(messages)
            elif provider == "gemini":
                # ä½¿ç”¨Gemini API
                return self._call_gemini_api(messages)
            else:
                self.logger.error(f"æœªçŸ¥çš„æ¨¡å‹æä¾›å•†: {provider}")
                return "é”™è¯¯ï¼šæœªçŸ¥çš„æ¨¡å‹æä¾›å•†é…ç½®ã€‚è¯·æ£€æŸ¥settings.pyä¸­çš„MODEL_PROVIDERè®¾ç½®ã€‚"
                
        except Exception as e:
            self.logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œæ— æ³•å®Œæˆè¯·æ±‚ã€‚é”™è¯¯: {str(e)}"

    def _get_llm_response_stream(self, prompt, history=None, personality=""):
        """
        ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹æµå¼ç”Ÿæˆå›å¤
        
        å‚æ•°:
            prompt: æç¤ºæ–‡æœ¬ï¼ˆå¯ä»¥æ˜¯å®Œæ•´çš„ä¸Šä¸‹æ–‡æˆ–ç®€å•æç¤ºï¼‰
            history: å†å²å¯¹è¯ (å¯é€‰ï¼Œç”¨äºå…¼å®¹æ€§)
            personality: äººæ ¼è®¾å®š (å¯é€‰ï¼Œç”¨äºå…¼å®¹æ€§)
        
        è¿”å›:
            æ¨¡å‹ç”Ÿæˆçš„å®Œæ•´å›å¤
        """
        if history is None:
            history = []
        
        # æ„å»ºæ¶ˆæ¯æ•°ç»„
        messages = []
        
        # æ·»åŠ äººæ ¼è®¾å®š (å¦‚æœæœ‰)
        if personality:
            messages.append({
                "role": "system",
                "content": personality
            })
        
        # æ·»åŠ å†å²å¯¹è¯
        for entry in history:
            messages.append({
                "role": entry.get("role", "user"),
                "content": entry.get("content", "")
            })
        
        # æ·»åŠ å½“å‰æç¤º
        # å¦‚æœ prompt å·²ç»æ˜¯å®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è§’è‰²è®¾å®šç­‰ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
        # å¦åˆ™ä½œä¸ºç”¨æˆ·æ¶ˆæ¯å¤„ç†
        if prompt.strip().startswith(('[ç³»ç»Ÿè§’è‰²è®¾å®š]', get_estia_persona()[:10], '[è§’è‰²è®¾å®š]')) or len(prompt) > 500:
            # è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼Œç›´æ¥ä½œä¸ºç”¨æˆ·æ¶ˆæ¯å‘é€
            messages.append({
                "role": "user", 
                "content": prompt
            })
        else:
            # è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æç¤ºæˆ–è¯„ä¼°è¯·æ±‚
            messages.append({
                "role": "user",
                "content": prompt
            })
        
        # æ ¹æ®æä¾›å•†é€‰æ‹©é€‚å½“çš„æµå¼APIè°ƒç”¨æ–¹æ³•
        provider = settings.MODEL_PROVIDER.lower()
        
        # è¯·æ±‚LLMæµå¼å“åº”
        try:
            self.logger.debug(f"ä½¿ç”¨{provider}æä¾›å•†å‘é€æµå¼è¯·æ±‚ï¼Œæ¶ˆæ¯æ•°: {len(messages)}")
            
            if provider == "local":
                # ä½¿ç”¨æœ¬åœ°LLM APIæµå¼è°ƒç”¨
                return self._call_local_llm_stream(messages)
            elif provider == "openai":
                # ä½¿ç”¨OpenAI APIæµå¼è°ƒç”¨
                return self._call_openai_api_stream(messages)
            elif provider == "deepseek":
                # ä½¿ç”¨DeepSeek APIæµå¼è°ƒç”¨
                return self._call_deepseek_api_stream(messages)
            elif provider == "gemini":
                # ä½¿ç”¨Gemini APIæµå¼è°ƒç”¨
                return self._call_gemini_api_stream(messages)
            else:
                self.logger.error(f"æœªçŸ¥çš„æ¨¡å‹æä¾›å•†: {provider}")
                return "é”™è¯¯ï¼šæœªçŸ¥çš„æ¨¡å‹æä¾›å•†é…ç½®ã€‚è¯·æ£€æŸ¥settings.pyä¸­çš„MODEL_PROVIDERè®¾ç½®ã€‚"
                
        except Exception as e:
            self.logger.error(f"LLMæµå¼è°ƒç”¨å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œæ— æ³•å®Œæˆè¯·æ±‚ã€‚é”™è¯¯: {str(e)}"

    def _call_local_llm(self, messages):
        """è°ƒç”¨æœ¬åœ°LLM APIï¼ˆå…¼å®¹ OpenAI æ¥å£ï¼‰"""
        try:
            request_data = {
                "model": getattr(settings, "LLM_MODEL", "local-model"),
                "messages": messages,
                "temperature": getattr(settings, "LLM_TEMPERATURE", 0.7),
                "max_tokens": getattr(settings, "LLM_MAX_NEW_TOKENS", 1024)
            }

            headers = {
                "Content-Type": "application/json"
            }

            response = requests.post(
                settings.LLM_API_URL,
                json=request_data,
                headers=headers,
                timeout=60
            )
            response.raise_for_status()

            result = response.json()
            choices = result.get("choices")
            if not choices or "message" not in choices[0]:
                self.logger.warning(f"æœ¬åœ°LLMå“åº”ç»“æ„å¼‚å¸¸: {result}")
                return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"

            content = choices[0]["message"].get("content", "").strip()
            if not content:
                self.logger.warning("æœ¬åœ°LLMè¿”å›äº†ç©ºå›å¤")
                return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"

            self.logger.debug(f"ğŸ¤– æœ¬åœ°LLMåŸå§‹å›å¤: {content}")
            return content

        except requests.RequestException as e:
            self.logger.error(f"æœ¬åœ°LLM APIè¯·æ±‚å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•è¿æ¥åˆ°æˆ‘çš„å¤§è„‘ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å·²å¯åŠ¨ã€‚"

    def _call_local_llm_stream(self, messages):
        """è°ƒç”¨æœ¬åœ°LLM APIæµå¼æ¥å£"""
        try:
            request_data = {
                "model": getattr(settings, "LLM_MODEL", "local-model"),
                "messages": messages,
                "temperature": getattr(settings, "LLM_TEMPERATURE", 0.7),
                "max_tokens": getattr(settings, "LLM_MAX_NEW_TOKENS", 1024),
                "stream": True  # å¯ç”¨æµå¼è¾“å‡º
            }

            headers = {
                "Content-Type": "application/json"
            }

            response = requests.post(
                settings.LLM_API_URL,
                json=request_data,
                headers=headers,
                timeout=60,
                stream=True  # å¯ç”¨æµå¼å“åº”
            )
            response.raise_for_status()

            full_response = ""
            print("ğŸ¤– AI: ", end="", flush=True)
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data = line[6:]  # å»æ‰ 'data: ' å‰ç¼€
                        if data == '[DONE]':
                            break
                        try:
                            json_data = json.loads(data)
                            if 'choices' in json_data and len(json_data['choices']) > 0:
                                choice = json_data['choices'][0]
                                if 'delta' in choice and 'content' in choice['delta']:
                                    content = choice['delta']['content']
                                    if content:
                                        print(content, end="", flush=True)
                                        full_response += content
                        except json.JSONDecodeError:
                            continue
            
            print()  # æ¢è¡Œ
            return full_response.strip()

        except requests.RequestException as e:
            self.logger.error(f"æœ¬åœ°LLMæµå¼APIè¯·æ±‚å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•è¿æ¥åˆ°æˆ‘çš„å¤§è„‘ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å·²å¯åŠ¨ã€‚"

    def _call_openai_api(self, messages):
        """è°ƒç”¨OpenAI API"""
        if not hasattr(settings, 'OPENAI_API_KEY') or not settings.OPENAI_API_KEY:
            raise ValueError("æœªé…ç½®OpenAI APIå¯†é’¥ã€‚è¯·åœ¨settings.pyä¸­è®¾ç½®OPENAI_API_KEYã€‚")
            
        import openai
        openai.api_key = settings.OPENAI_API_KEY

        # è®¾ç½®è‡ªå®šä¹‰åŸºç¡€URLï¼ˆå¦‚æœæœ‰ï¼‰
        if hasattr(settings, 'OPENAI_API_BASE') and settings.OPENAI_API_BASE:
            openai.base_url = settings.OPENAI_API_BASE  # æ³¨æ„ï¼šæ–°ç‰ˆæ˜¯ base_urlï¼Œä¸æ˜¯ api_base

        # è°ƒç”¨APIï¼ˆæ–°ç‰ˆæ¥å£ï¼‰
        response = openai.chat.completions.create(
            model=getattr(settings, "OPENAI_MODEL", "gpt-3.5-turbo"),
            messages=messages,
            temperature=getattr(settings, "LLM_TEMPERATURE", 0.7),
            max_tokens=getattr(settings, "LLM_MAX_NEW_TOKENS", 1024)
        )
        
        # æå–å›å¤æ–‡æœ¬
        content = response.choices[0].message.content
        if content is None:
            self.logger.warning("OpenAI APIè¿”å›äº†ç©ºå›å¤")
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"
        reply = content.strip()
        return reply


    def _call_openai_api_stream(self, messages):
        """è°ƒç”¨OpenAI APIæµå¼æ¥å£"""
        if not hasattr(settings, 'OPENAI_API_KEY') or not settings.OPENAI_API_KEY:
            raise ValueError("æœªé…ç½®OpenAI APIå¯†é’¥ã€‚è¯·åœ¨settings.pyä¸­è®¾ç½®OPENAI_API_KEYã€‚")
            
        import openai
        openai.api_key = settings.OPENAI_API_KEY

        # è®¾ç½®è‡ªå®šä¹‰åŸºç¡€URLï¼ˆå¦‚æœæœ‰ï¼‰
        if hasattr(settings, 'OPENAI_API_BASE') and settings.OPENAI_API_BASE:
            openai.base_url = settings.OPENAI_API_BASE

        # è°ƒç”¨æµå¼API
        response = openai.chat.completions.create(
            model=getattr(settings, "OPENAI_MODEL", "gpt-3.5-turbo"),
            messages=messages,
            temperature=getattr(settings, "LLM_TEMPERATURE", 0.7),
            max_tokens=getattr(settings, "LLM_MAX_NEW_TOKENS", 1024),
            stream=True  # å¯ç”¨æµå¼è¾“å‡º
        )
        
        full_response = ""
        print("ğŸ¤– AI: ", end="", flush=True)
        
        for chunk in response:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                print(content, end="", flush=True)
                full_response += content
        
        print()  # æ¢è¡Œ
        return full_response.strip()

    def _call_deepseek_api(self, messages):
        """è°ƒç”¨DeepSeek API"""
        if not hasattr(settings, 'DEEPSEEK_API_KEY') or not settings.DEEPSEEK_API_KEY:
            raise ValueError("æœªé…ç½®DeepSeek APIå¯†é’¥ã€‚è¯·åœ¨settings.pyä¸­è®¾ç½®DEEPSEEK_API_KEYã€‚")
        
        import openai
        openai.api_key = settings.DEEPSEEK_API_KEY

        if hasattr(settings, 'DEEPSEEK_API_BASE') and settings.DEEPSEEK_API_BASE:
            openai.base_url = settings.DEEPSEEK_API_BASE  # æ³¨æ„æ–°ç‰ˆæ˜¯ base_url

        response = openai.chat.completions.create(
            model=getattr(settings, "DEEPSEEK_MODEL", "deepseek-chat"),
            messages=messages,
            temperature=getattr(settings, "LLM_TEMPERATURE", 0.7),
            max_tokens=getattr(settings, "LLM_MAX_NEW_TOKENS", 1024)
        )

        # æå–å›å¤æ–‡æœ¬
        content = response.choices[0].message.content
        if content is None:
            self.logger.warning("DeepSeek APIè¿”å›äº†ç©ºå›å¤")
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"
        reply = content.strip()
        return reply

    def _call_deepseek_api_stream(self, messages):
        """è°ƒç”¨DeepSeek APIæµå¼æ¥å£"""
        if not hasattr(settings, 'DEEPSEEK_API_KEY') or not settings.DEEPSEEK_API_KEY:
            raise ValueError("æœªé…ç½®DeepSeek APIå¯†é’¥ã€‚è¯·åœ¨settings.pyä¸­è®¾ç½®DEEPSEEK_API_KEYã€‚")
        
        import openai
        openai.api_key = settings.DEEPSEEK_API_KEY

        if hasattr(settings, 'DEEPSEEK_API_BASE') and settings.DEEPSEEK_API_BASE:
            openai.base_url = settings.DEEPSEEK_API_BASE

        response = openai.chat.completions.create(
            model=getattr(settings, "DEEPSEEK_MODEL", "deepseek-chat"),
            messages=messages,
            temperature=getattr(settings, "LLM_TEMPERATURE", 0.7),
            max_tokens=getattr(settings, "LLM_MAX_NEW_TOKENS", 1024),
            stream=True  # å¯ç”¨æµå¼è¾“å‡º
        )

        full_response = ""
        print("ğŸ¤– AI: ", end="", flush=True)
        
        for chunk in response:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                print(content, end="", flush=True)
                full_response += content
        
        print()  # æ¢è¡Œ
        return full_response.strip()

    # ------------------ ä»¥ä¸‹æ˜¯è¢«å®Œå…¨ä¿®æ­£çš„ Gemini ç›¸å…³æ–¹æ³• ------------------

    def _call_gemini_api(self, messages):
        """è°ƒç”¨Gemini APIï¼ˆä½¿ç”¨å®˜æ–¹SDKï¼Œç¨³å®šå¯é ï¼‰"""
        if not hasattr(settings, 'GEMINI_API_KEY') or not settings.GEMINI_API_KEY:
            raise ValueError("æœªé…ç½®Gemini APIå¯†é’¥ã€‚è¯·åœ¨settings.pyä¸­è®¾ç½®GEMINI_API_KEYã€‚")

        try:
            # å…³é”®æ­¥éª¤ï¼šå¤„ç†ä»£ç†é…ç½®
            api_endpoint = None
            if hasattr(settings, 'GEMINI_API_BASE') and settings.GEMINI_API_BASE:
                from urllib.parse import urlparse
                # ä»å®Œæ•´çš„URLä¸­æå–ä¸»æœºåéƒ¨åˆ†ï¼Œä¾‹å¦‚ "gemini-proxy.yourdomain.com"
                api_endpoint = urlparse(settings.GEMINI_API_BASE).netloc
            
            client_opts = client_options.ClientOptions(api_endpoint=api_endpoint) if api_endpoint else None
            
            # 1. é…ç½®API Keyå’Œå®¢æˆ·ç«¯é€‰é¡¹ï¼ˆåŒ…å«ä»£ç†ï¼‰
            genai.configure(
                api_key=settings.GEMINI_API_KEY,
                transport="rest", # æ˜ç¡®ä½¿ç”¨restä¼ è¾“ä»¥åº”ç”¨ä»£ç†
                client_options=client_opts
            )
            
            # 2. è½¬æ¢æ¶ˆæ¯æ ¼å¼ (è°ƒç”¨ä¸‹é¢å·²ä¿®æ­£çš„è¾…åŠ©å‡½æ•°)
            system_instruction, gemini_contents = self._convert_messages_to_gemini_format(messages)

            # 3. è®¾ç½®ç”Ÿæˆå‚æ•°
            generation_config = genai.types.GenerationConfig(
                temperature=getattr(settings, "LLM_TEMPERATURE", 0.7),
                max_output_tokens=getattr(settings, "LLM_MAX_NEW_TOKENS", 2048),
                top_p=0.8,
                top_k=10
            )
            
            # 4. è®¾ç½®å®‰å…¨è®¾ç½®
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]

            # 5. åˆå§‹åŒ–æ¨¡å‹
            model = genai.GenerativeModel(
                model_name=getattr(settings, "GEMINI_MODEL", "gemini-2.5-pro"),
                generation_config=generation_config,
                system_instruction=system_instruction,
                safety_settings=safety_settings
            )

            self.logger.debug(f"Gemini SDK è¯·æ±‚å†…å®¹: {gemini_contents}")
            
            # 6. å‘é€è¯·æ±‚
            response = model.generate_content(gemini_contents)
            
            # æ·»åŠ è¯¦ç»†çš„å“åº”è°ƒè¯•ä¿¡æ¯
            self.logger.debug(f"Gemini API å“åº”å¯¹è±¡ç±»å‹: {type(response)}")
            self.logger.debug(f"Gemini API å“åº”å±æ€§: {dir(response)}")
            
            # 7. è§£æå“åº” - æ”¹è¿›ç‰ˆæœ¬
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å€™é€‰å“åº”
            candidates = getattr(response, 'candidates', [])
            if not candidates:
                self.logger.warning("Gemini API æ²¡æœ‰è¿”å›ä»»ä½•å€™é€‰å“åº”")
                return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"
            
            # è·å–ç¬¬ä¸€ä¸ªå€™é€‰å“åº”
            candidate = candidates[0]
            candidate_finish_reason = getattr(candidate, 'finish_reason', None)
            
            # æ£€æŸ¥finish_reason
            if candidate_finish_reason:
                self.logger.debug(f"Candidate finish reason: {candidate_finish_reason}")
                
                # æ ¹æ®finish_reasonå¤„ç†ä¸åŒæƒ…å†µ
                if candidate_finish_reason == 1:  # STOP - æ­£å¸¸å®Œæˆ
                    pass  # ç»§ç»­å¤„ç†
                elif candidate_finish_reason == 2:  # MAX_TOKENS
                    self.logger.warning("Gemini APIè¾¾åˆ°æœ€å¤§tokené™åˆ¶")
                    return "å›å¤å†…å®¹è¿‡é•¿ï¼Œå·²è¢«æˆªæ–­ã€‚è¯·å°è¯•æ›´ç®€æ´çš„é—®é¢˜ã€‚"
                elif candidate_finish_reason == 3:  # SAFETY
                    self.logger.warning("Gemini APIå› å®‰å…¨ç­–ç•¥é˜»æ­¢")
                    return "æŠ±æ­‰ï¼Œç”±äºå®‰å…¨ç­–ç•¥é™åˆ¶ï¼Œæˆ‘æ— æ³•å›å¤è¿™ä¸ªé—®é¢˜ã€‚è¯·å°è¯•æ¢ä¸ªè¯é¢˜ã€‚"
                elif candidate_finish_reason == 4:  # RECITATION
                    self.logger.warning("Gemini APIå› ç‰ˆæƒé—®é¢˜é˜»æ­¢")
                    return "æŠ±æ­‰ï¼Œè¿™ä¸ªé—®é¢˜å¯èƒ½æ¶‰åŠç‰ˆæƒå†…å®¹ï¼Œæˆ‘æ— æ³•å›å¤ã€‚"
                else:
                    self.logger.warning(f"æœªçŸ¥çš„finish_reason: {candidate_finish_reason}")
                    return f"æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ç”Ÿæˆå›å¤ã€‚(åŸå› ç : {candidate_finish_reason})"
            
            # æ£€æŸ¥å“åº”å†…å®¹
            if not response.parts:
                # æ£€æŸ¥å…·ä½“çš„å¤±è´¥åŸå› 
                finish_reason = getattr(response, 'finish_reason', 'UNKNOWN')
                prompt_feedback = getattr(response, 'prompt_feedback', {})
                
                # æ·»åŠ æ›´å¤šè°ƒè¯•ä¿¡æ¯
                self.logger.warning(f"Gemini API è¿”å›äº†ç©ºå†…å®¹")
                self.logger.warning(f"Finish Reason: {finish_reason}")
                self.logger.warning(f"Prompt Feedback: {prompt_feedback}")
                self.logger.warning(f"Response candidates: {getattr(response, 'candidates', [])}")
                self.logger.warning(f"Response text: {getattr(response, 'text', 'N/A')}")
                
                # å°è¯•ä»candidatesä¸­è·å–ä¿¡æ¯
                candidates = getattr(response, 'candidates', [])
                if candidates:
                    candidate = candidates[0]
                    self.logger.warning(f"First candidate: {candidate}")
                    candidate_finish_reason = getattr(candidate, 'finish_reason', 'UNKNOWN')
                    self.logger.warning(f"Candidate finish reason: {candidate_finish_reason}")
                
                # æ ¹æ®å…·ä½“åŸå› è¿”å›ä¸åŒçš„é”™è¯¯ä¿¡æ¯
                if finish_reason == 'SAFETY':
                    return "æŠ±æ­‰ï¼Œç”±äºå®‰å…¨ç­–ç•¥é™åˆ¶ï¼Œæˆ‘æ— æ³•å›å¤è¿™ä¸ªé—®é¢˜ã€‚è¯·å°è¯•æ¢ä¸ªè¯é¢˜ã€‚"
                elif finish_reason == 'MAX_TOKENS':
                    return "å›å¤å†…å®¹è¿‡é•¿ï¼Œå·²è¢«æˆªæ–­ã€‚è¯·å°è¯•æ›´ç®€æ´çš„é—®é¢˜ã€‚"
                elif finish_reason == 'RECITATION':
                    return "æŠ±æ­‰ï¼Œè¿™ä¸ªé—®é¢˜å¯èƒ½æ¶‰åŠç‰ˆæƒå†…å®¹ï¼Œæˆ‘æ— æ³•å›å¤ã€‚"
                else:
                    return f"æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ç”Ÿæˆå›å¤ã€‚(åŸå› : {finish_reason})"
                
            # å®‰å…¨åœ°è·å–å“åº”æ–‡æœ¬
            try:
                reply = response.text.strip()
                if not reply:
                    self.logger.warning("Gemini APIè¿”å›äº†ç©ºæ–‡æœ¬")
                    return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆæœ‰æ•ˆçš„å›å¤ã€‚"
                    
                self.logger.debug(f"ğŸ¤– Gemini API å›å¤: {reply}")
                return reply
            except Exception as text_error:
                self.logger.error(f"è·å–å“åº”æ–‡æœ¬æ—¶å‡ºé”™: {text_error}")
                return "æŠ±æ­‰ï¼Œå¤„ç†å›å¤æ—¶å‡ºç°é”™è¯¯ã€‚"

        except Exception as e:
            self.logger.error(f"Gemini SDK è°ƒç”¨å¼‚å¸¸: {e}")
            return f"æŠ±æ­‰ï¼Œå¤„ç†Geminiè¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"

    def _call_gemini_api_stream(self, messages):
        """
        è°ƒç”¨Gemini APIå¹¶ä»¥æµå¼è¿”å›å“åº”ï¼ˆä½¿ç”¨å®˜æ–¹SDKï¼‰ã€‚
        è¿™æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨å‡½æ•°ï¼Œä¼šé€å— yield å“åº”æ–‡æœ¬ã€‚
        """
        if not hasattr(settings, 'GEMINI_API_KEY') or not settings.GEMINI_API_KEY:
            # å¯¹äºç”Ÿæˆå™¨ï¼Œæˆ‘ä»¬å¯ä»¥yieldä¸€ä¸ªé”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯raiseå¼‚å¸¸
            # è¿™æ ·è°ƒç”¨æ–¹å¯ä»¥åœ¨UIä¸Šæ˜¾ç¤ºé”™è¯¯
            yield "[ERROR] æœªé…ç½®Gemini APIå¯†é’¥ã€‚è¯·åœ¨settings.pyä¸­è®¾ç½®GEMINI_API_KEYã€‚"
            return # å¿…é¡»returnæ¥ç»“æŸç”Ÿæˆå™¨

        # æ‚¨ç°æœ‰çš„æ‰€æœ‰é…ç½®å’Œåˆå§‹åŒ–ä»£ç éƒ½å¯ä»¥å¤ç”¨
        try:
            # å…³é”®æ­¥éª¤ï¼šå¤„ç†ä»£ç†é…ç½®
            api_endpoint = None
            if hasattr(settings, 'GEMINI_API_BASE') and settings.GEMINI_API_BASE:
                from urllib.parse import urlparse
                api_endpoint = urlparse(settings.GEMINI_API_BASE).netloc
            
            client_opts = client_options.ClientOptions(api_endpoint=api_endpoint) if api_endpoint else None
            
            # 1. é…ç½®API Keyå’Œå®¢æˆ·ç«¯é€‰é¡¹ï¼ˆåŒ…å«ä»£ç†ï¼‰
            genai.configure(
                api_key=settings.GEMINI_API_KEY,
                transport="rest", # æ˜ç¡®ä½¿ç”¨restä¼ è¾“ä»¥åº”ç”¨ä»£ç†
                client_options=client_opts
            )
            
            # 2. è½¬æ¢æ¶ˆæ¯æ ¼å¼
            system_instruction, gemini_contents = self._convert_messages_to_gemini_format(messages)

            # 3. è®¾ç½®ç”Ÿæˆå‚æ•°
            generation_config = genai.types.GenerationConfig(
                temperature=getattr(settings, "LLM_TEMPERATURE", 0.7),
                max_output_tokens=getattr(settings, "LLM_MAX_NEW_TOKENS", 2048),
                top_p=0.8,
                top_k=10
            )
            
            # 4. è®¾ç½®å®‰å…¨è®¾ç½®
            safety_settings = [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
            ]

            # 5. åˆå§‹åŒ–æ¨¡å‹
            model = genai.GenerativeModel(
                model_name=getattr(settings, "GEMINI_MODEL", "gemini-1.5-pro-latest"),
                generation_config=generation_config,
                system_instruction=system_instruction,
                safety_settings=safety_settings
            )

            self.logger.debug(f"Gemini SDK [STREAM] è¯·æ±‚å†…å®¹: {gemini_contents}")
            
            # 6. å‘é€æµå¼è¯·æ±‚ (æ ¸å¿ƒå˜åŒ–)
            response_stream = model.generate_content(
                gemini_contents,
                stream=True  # <--- âœ¨ å¼€å¯æµå¼æ¨¡å¼ï¼
            )
            
            # 7. å¾ªç¯å¤„ç†æ•°æ®æµå¹¶ yield æ¯ä¸€å—æ–‡æœ¬ (æ ¸å¿ƒå˜åŒ–)
            for chunk in response_stream:
                # å®‰å…¨åœ°è·å–æ–‡æœ¬å—ï¼Œé˜²æ­¢å› å¥‡æ€ªçš„å“åº”ï¼ˆå¦‚åªæœ‰finish_reasonï¼‰è€ŒæŠ¥é”™
                if chunk.text:
                    yield chunk.text # <--- âœ¨ ä½¿ç”¨yieldè€Œä¸æ˜¯return

        except Exception as e:
            self.logger.error(f"Gemini SDK [STREAM] è°ƒç”¨å¼‚å¸¸: {e}")
            # åœ¨ç”Ÿæˆå™¨ä¸­ï¼Œé€šè¿‡yieldè¿”å›é”™è¯¯ä¿¡æ¯æ˜¯æ›´å¥½çš„æ–¹å¼
            yield f"\n[ERROR] æŠ±æ­‰ï¼Œå¤„ç†Geminiæµå¼è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"
    
    def _convert_messages_to_gemini_format(self, messages):
        """
        [å·²ä¿®æ­£] å°†OpenAIæ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºGemini SDKæ‰€éœ€çš„æ ¼å¼ã€‚
        - æå– system æŒ‡ä»¤ã€‚
        - ç¡®ä¿ user/model è§’è‰²äº¤æ›¿ã€‚
        - åˆå¹¶è¿ç»­çš„åŒè§’è‰²æ¶ˆæ¯ã€‚
        """
        system_instruction = None
        gemini_contents = []
        
        if not messages:
            return None, []

        # 1. æå– system æŒ‡ä»¤ (é€šå¸¸æ˜¯åˆ—è¡¨ä¸­çš„ç¬¬ä¸€æ¡)
        if messages[0]['role'] == 'system':
            system_instruction = messages[0]['content']
            messages = messages[1:]

        if not messages:
            return system_instruction, []

        # 2. åˆå¹¶è¿ç»­çš„åŒè§’è‰²æ¶ˆæ¯ï¼Œé¿å…APIæŠ¥é”™
        merged_messages = []
        current_role = messages[0]['role']
        current_content = [messages[0]['content']]

        for msg in messages[1:]:
            if msg['role'] == current_role:
                current_content.append(msg['content'])
            else:
                merged_messages.append({'role': current_role, 'content': "\n".join(current_content)})
                current_role = msg['role']
                current_content = [msg['content']]
        merged_messages.append({'role': current_role, 'content': "\n".join(current_content)})
        
        # 3. è½¬æ¢ä¸ºGeminiæ ¼å¼ï¼Œå¹¶ç¡®ä¿è§’è‰²äº¤æ›¿
        for msg in merged_messages:
            # è§’è‰²æ˜ å°„: assistant -> model
            role = 'model' if msg['role'] == 'assistant' else 'user'
            
            # ä¿è¯å†å²è®°å½•ä»¥ user å¼€å¤´ï¼Œä¸” user/model äº¤æ›¿
            if role == 'user' or (role == 'model' and len(gemini_contents) > 0 and gemini_contents[-1]['role'] == 'user'):
                gemini_contents.append({'role': role, 'parts': [msg['content']]})
            else:
                # å¦‚æœå‡ºç°ä¸è§„èŒƒçš„å¼€å¤´(å¦‚model)æˆ–è¿ç»­çš„modelè§’è‰²ï¼Œåˆ™è®°å½•å¹¶è·³è¿‡ï¼Œä»¥é˜²APIæŠ¥é”™
                self.logger.warning(f"ä¸¢å¼ƒäº†æ ¼å¼ä¸æ­£ç¡®çš„å¯¹è¯å†å²éƒ¨åˆ†: {msg}")

        return system_instruction, gemini_contents


# -----------------------------------------------------------------------------
# å…¼å®¹æ€§å¯¼å‡ºå‡½æ•° (ç”¨äºå‘åå…¼å®¹)
# -----------------------------------------------------------------------------

def get_llm_response(prompt, history=None, personality=""):
    """
    ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå›å¤ (å…¼å®¹æ—§ç‰ˆ)
    """
    engine = DialogueEngine()
    return engine._get_llm_response(prompt, history, personality)

def generate_response(user_query, memory_context, personality=""):
    """
    ç”Ÿæˆå›å¤ï¼Œè€ƒè™‘è®°å¿†ä¸Šä¸‹æ–‡å’Œäººæ ¼ (å…¼å®¹æ—§ç‰ˆ)
    """
    engine = DialogueEngine()
    return engine.generate_response(user_query, memory_context, personality)

def retrieve_memories(query, limit=5, memory_manager=None):
    """
    ä»è®°å¿†ç®¡ç†å™¨ä¸­æ£€ç´¢ç›¸å…³è®°å¿†
    
    å‚æ•°:
        query: æŸ¥è¯¢æ–‡æœ¬
        limit: æœ€å¤§è¿”å›æ•°é‡
        memory_manager: è®°å¿†ç®¡ç†å™¨å®ä¾‹
    
    è¿”å›:
        æ ¼å¼åŒ–çš„è®°å¿†æ–‡æœ¬
    """
    if not memory_manager:
        return "æ²¡æœ‰å¯ç”¨çš„è®°å¿†ã€‚"
    
    try:
        # ä»è®°å¿†ç®¡ç†å™¨æ£€ç´¢è®°å¿†
        results = memory_manager.retrieve_memory(
            query, 
            limit=limit,
            parallel=True,
            include_associations=True,
            check_conflicts=True
        )
        
        if not results:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®°å¿†ã€‚"
        
        # æ ¼å¼åŒ–è®°å¿†
        formatted_memories = []
        
        for memory in results:
            role = memory.get("role", "system")
            content = memory.get("content", "")
            timestamp = memory.get("timestamp", "")
            
            # æ ¼å¼åŒ–æ—¶é—´
            if isinstance(timestamp, (int, float)):
                import datetime
                timestamp = datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M')
            
            # å¤„ç†ç‰¹æ®Šæ ‡è®°
            prefix = ""
            if memory.get("is_associated", False):
                prefix = "[å…³è”è®°å¿†] "
            elif memory.get("is_summary", False):
                prefix = "[è®°å¿†æ‘˜è¦] "
            elif memory.get("status") == "superseded":
                prefix = "[å·²æ›´æ–°çš„ä¿¡æ¯] "
            
            # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
            context = ""
            if "context" in memory:
                context = f" (å¤‡æ³¨: {memory['context']})"
            
            formatted_memories.append(f"{prefix}[{timestamp}] {role}: {content}{context}")
        
        # æ·»åŠ ä¸€ä¸ªç®€çŸ­çš„ä»‹ç»
        header = "ç³»ç»Ÿè®°å¿†:"
        formatted_text = header + "\n" + "\n".join(formatted_memories)
        
        return formatted_text
        
    except Exception as e:
        logger.error(f"è®°å¿†æ£€ç´¢å¤±è´¥: {e}")
        return "è®°å¿†æ£€ç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ã€‚"