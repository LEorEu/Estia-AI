# core/dialogue_engine.py

"""
æœ¬æ¨¡å—æ˜¯ AI çš„"æ€è€ƒ"æ ¸å¿ƒã€‚ï¼ˆV3.0 - å¤šæ¨¡å‹å…¼å®¹ç‰ˆï¼‰
å®ƒè´Ÿè´£æ¥æ”¶ç”¨æˆ·çš„æ–‡æœ¬è¾“å…¥ï¼Œæ ¹æ®é…ç½®å†³å®šè°ƒç”¨æœ¬åœ°æˆ–äº‘ç«¯å¤§æ¨¡å‹APIï¼Œ
å¹¶è§£æè¿”å›çš„å›å¤ã€‚æ”¯æŒå¤šç§æ¨¡å‹æä¾›å•†ï¼šæœ¬åœ°æ¨¡å‹ã€OpenAIã€DeepSeekã€‚
"""

# -----------------------------------------------------------------------------
# å¯¼å…¥å¿…è¦çš„åº“
# -----------------------------------------------------------------------------

import os
# é¢„å…ˆè®¾ç½®ç¯å¢ƒå˜é‡æ¥ä½¿ç”¨é•œåƒç«™ç‚¹
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import requests                 # å¯¼å…¥ requests åº“ï¼Œç”¨äºå‘é€ HTTP API è¯·æ±‚ã€‚
import json                     # å¯¼å…¥ json åº“ï¼Œç”¨äºå¤„ç† JSON æ•°æ®æ ¼å¼ã€‚
from config import settings     # ä»æˆ‘ä»¬çš„é…ç½®æ–‡ä»¶ä¸­å¯¼å…¥ settingsã€‚
import time                     # å¯¼å…¥ time åº“ï¼Œç”¨äºæ ¼å¼åŒ–æ—¶é—´æˆ³ã€‚
import logging

# è®¾ç½®æ—¥å¿—
log_dir = getattr(settings, 'LOG_DIR', './logs')
os.makedirs(log_dir, exist_ok=True)

# è·å–logger
logger = logging.getLogger('dialogue_engine')

# è®¾ç½®æ—¥å¿—æ ¼å¼
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
file_handler = logging.FileHandler(
    os.path.join(log_dir, 'dialogue_engine.log'),
    encoding='utf-8'  # æŒ‡å®šUTF-8ç¼–ç 
)
file_handler.setFormatter(formatter)
file_handler.setLevel(getattr(settings, 'LOG_LEVEL', 'INFO'))

# æ·»åŠ å¤„ç†å™¨
logger.addHandler(file_handler)
logger.setLevel(getattr(settings, 'LOG_LEVEL', 'INFO'))

# æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯¼å…¥OpenAIåº“
if settings.MODEL_PROVIDER.lower() in ["openai", "deepseek", "gemini"]:
    try:
        import openai
        logger.info(f"å·²åŠ è½½OpenAIåº“ï¼Œä½¿ç”¨{settings.MODEL_PROVIDER}æä¾›å•†")
    except ImportError:
        logger.error("æœªæ‰¾åˆ°OpenAIåº“ã€‚è¯·ä½¿ç”¨ 'pip install openai' å®‰è£…")
        # ä½¿ç”¨åŸºæœ¬çš„requestsåº“ä½œä¸ºåå¤‡


# -----------------------------------------------------------------------------
# å¯¹è¯å¼•æ“ç±»å®šä¹‰
# -----------------------------------------------------------------------------

class DialogueEngine:
    """å¯¹è¯å¼•æ“ç±»ï¼Œå°è£…LLMäº¤äº’åŠŸèƒ½"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¯¹è¯å¼•æ“"""
        self.logger = logger
        self.logger.info("å¯¹è¯å¼•æ“åˆå§‹åŒ–")
        
    def generate_response(self, user_query, memory_context=None, personality=""):
        """
        ç”Ÿæˆå›å¤ï¼Œè€ƒè™‘è®°å¿†ä¸Šä¸‹æ–‡å’Œäººæ ¼
        
        å‚æ•°:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            memory_context: ç›¸å…³è®°å¿†ä¸Šä¸‹æ–‡
            personality: äººæ ¼è®¾å®š
        
        è¿”å›:
            ç”Ÿæˆçš„å›å¤
        """
        # æ„å»ºå®Œæ•´æç¤º
        full_prompt = f"""è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜æˆ–è¯·æ±‚ã€‚

{memory_context if memory_context else "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®°å¿†ã€‚"}

ç”¨æˆ·è¯·æ±‚: {user_query}

è¯·æ³¨æ„:
1. å¦‚æœè®°å¿†ä¸­åŒ…å«çŸ›ç›¾ä¿¡æ¯ï¼Œè¯·ä¼˜å…ˆè€ƒè™‘æ ‡è®°ä¸ºæœ€æ–°çš„ä¿¡æ¯
2. å›ç­”æ—¶è€ƒè™‘å…³è”è®°å¿†æä¾›çš„é¢å¤–ä¸Šä¸‹æ–‡
3. å¦‚æœçœ‹åˆ°è®°å¿†æ‘˜è¦ï¼Œå¯ä»¥åˆ©ç”¨å…¶æä¾›çš„æ•´åˆä¿¡æ¯
4. ä¿æŒç®€æ´è‡ªç„¶çš„å¯¹è¯é£æ ¼

è¯·åŸºäºä¸Šè¿°ä¿¡æ¯ç»™å‡ºå›å¤:"""

        # è°ƒç”¨LLMç”Ÿæˆå›å¤
        response = self._get_llm_response(full_prompt, [], personality)
        return response
        
    def _get_llm_response(self, prompt, history=None, personality=""):
        """
        ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå›å¤
        
        å‚æ•°:
            prompt: æç¤ºæ–‡æœ¬
            history: å†å²å¯¹è¯ (å¯é€‰)
            personality: äººæ ¼è®¾å®š (å¯é€‰)
        
        è¿”å›:
            æ¨¡å‹ç”Ÿæˆçš„å›å¤
        """
        if history is None:
            history = []
        
        # æ„å»ºæ¶ˆæ¯æ•°ç»„
        messages = []
        
        # æ·»åŠ äººæ ¼è®¾å®š (å¦‚æœæœ‰)
        if personality:
            messages.append({
                "role": "system",
                "content": personality
            })
        
        # æ·»åŠ å†å²å¯¹è¯
        for entry in history:
            messages.append({
                "role": entry.get("role", "user"),
                "content": entry.get("content", "")
            })
        
        # æ·»åŠ å½“å‰æç¤º
        messages.append({
            "role": "user",
            "content": prompt
        })
        
        # æ ¹æ®æä¾›å•†é€‰æ‹©é€‚å½“çš„APIè°ƒç”¨æ–¹æ³•
        provider = settings.MODEL_PROVIDER.lower()
        
        # è¯·æ±‚LLM
        try:
            self.logger.debug(f"ä½¿ç”¨{provider}æä¾›å•†å‘é€è¯·æ±‚ï¼Œæ¶ˆæ¯æ•°: {len(messages)}")
            
            if provider == "local":
                # ä½¿ç”¨æœ¬åœ°LLM API
                return self._call_local_llm(messages)
            elif provider == "openai":
                # ä½¿ç”¨OpenAI API
                return self._call_openai_api(messages)
            elif provider == "deepseek":
                # ä½¿ç”¨DeepSeek API
                return self._call_deepseek_api(messages)
            elif provider == "gemini":
                # ä½¿ç”¨Gemini API
                return self._call_gemini_api(messages)
            else:
                self.logger.error(f"æœªçŸ¥çš„æ¨¡å‹æä¾›å•†: {provider}")
                return "é”™è¯¯ï¼šæœªçŸ¥çš„æ¨¡å‹æä¾›å•†é…ç½®ã€‚è¯·æ£€æŸ¥settings.pyä¸­çš„MODEL_PROVIDERè®¾ç½®ã€‚"
                
        except Exception as e:
            self.logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œæ— æ³•å®Œæˆè¯·æ±‚ã€‚é”™è¯¯: {str(e)}"

    def _call_local_llm(self, messages):
        """è°ƒç”¨æœ¬åœ°LLM APIï¼ˆå…¼å®¹ OpenAI æ¥å£ï¼‰"""
        try:
            request_data = {
                "model": getattr(settings, "LLM_MODEL", "local-model"),
                "messages": messages,
                "temperature": getattr(settings, "LLM_TEMPERATURE", 0.7),
                "max_tokens": getattr(settings, "LLM_MAX_NEW_TOKENS", 1024)
            }

            headers = {
                "Content-Type": "application/json"
            }

            response = requests.post(
                settings.LLM_API_URL,
                json=request_data,
                headers=headers,
                timeout=60
            )
            response.raise_for_status()

            result = response.json()
            choices = result.get("choices")
            if not choices or "message" not in choices[0]:
                self.logger.warning(f"æœ¬åœ°LLMå“åº”ç»“æ„å¼‚å¸¸: {result}")
                return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"

            content = choices[0]["message"].get("content", "").strip()
            if not content:
                self.logger.warning("æœ¬åœ°LLMè¿”å›äº†ç©ºå›å¤")
                return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"

            self.logger.debug(f"ğŸ¤– æœ¬åœ°LLMåŸå§‹å›å¤: {content}")
            return content

        except requests.RequestException as e:
            self.logger.error(f"æœ¬åœ°LLM APIè¯·æ±‚å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•è¿æ¥åˆ°æˆ‘çš„å¤§è„‘ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å·²å¯åŠ¨ã€‚"

    def _call_openai_api(self, messages):
        """è°ƒç”¨OpenAI API"""
        if not hasattr(settings, 'OPENAI_API_KEY') or not settings.OPENAI_API_KEY:
            raise ValueError("æœªé…ç½®OpenAI APIå¯†é’¥ã€‚è¯·åœ¨settings.pyä¸­è®¾ç½®OPENAI_API_KEYã€‚")
            
        import openai
        openai.api_key = settings.OPENAI_API_KEY

        # è®¾ç½®è‡ªå®šä¹‰åŸºç¡€URLï¼ˆå¦‚æœæœ‰ï¼‰
        if hasattr(settings, 'OPENAI_API_BASE') and settings.OPENAI_API_BASE:
            openai.base_url = settings.OPENAI_API_BASE  # æ³¨æ„ï¼šæ–°ç‰ˆæ˜¯ base_urlï¼Œä¸æ˜¯ api_base

        # è°ƒç”¨APIï¼ˆæ–°ç‰ˆæ¥å£ï¼‰
        response = openai.chat.completions.create(
            model=getattr(settings, "OPENAI_MODEL", "gpt-3.5-turbo"),
            messages=messages,
            temperature=getattr(settings, "LLM_TEMPERATURE", 0.7),
            max_tokens=getattr(settings, "LLM_MAX_NEW_TOKENS", 1024)
        )
        
        # æå–å›å¤æ–‡æœ¬
        content = response.choices[0].message.content
        if content is None:
            self.logger.warning("OpenAI APIè¿”å›äº†ç©ºå›å¤")
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"
        reply = content.strip()
        return reply


    def _call_deepseek_api(self, messages):
        """è°ƒç”¨DeepSeek API"""
        if not hasattr(settings, 'DEEPSEEK_API_KEY') or not settings.DEEPSEEK_API_KEY:
            raise ValueError("æœªé…ç½®DeepSeek APIå¯†é’¥ã€‚è¯·åœ¨settings.pyä¸­è®¾ç½®DEEPSEEK_API_KEYã€‚")
        
        import openai
        openai.api_key = settings.DEEPSEEK_API_KEY

        if hasattr(settings, 'DEEPSEEK_API_BASE') and settings.DEEPSEEK_API_BASE:
            openai.base_url = settings.DEEPSEEK_API_BASE  # æ³¨æ„æ–°ç‰ˆæ˜¯ base_url

        response = openai.chat.completions.create(
            model=getattr(settings, "DEEPSEEK_MODEL", "deepseek-chat"),
            messages=messages,
            temperature=getattr(settings, "LLM_TEMPERATURE", 0.7),
            max_tokens=getattr(settings, "LLM_MAX_NEW_TOKENS", 1024)
        )

        # æå–å›å¤æ–‡æœ¬
        content = response.choices[0].message.content
        if content is None:
            self.logger.warning("DeepSeek APIè¿”å›äº†ç©ºå›å¤")
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"
        reply = content.strip()
        return reply

    def _call_gemini_api(self, messages):
        """è°ƒç”¨Gemini APIï¼ˆä½¿ç”¨åŸç”Ÿæ ¼å¼ï¼‰"""
        if not hasattr(settings, 'GEMINI_API_KEY') or not settings.GEMINI_API_KEY:
            raise ValueError("æœªé…ç½®Gemini APIå¯†é’¥ã€‚è¯·åœ¨settings.pyä¸­è®¾ç½®GEMINI_API_KEYã€‚")
        
        try:
            # è½¬æ¢OpenAIæ ¼å¼çš„messagesåˆ°Geminiæ ¼å¼
            gemini_contents = self._convert_messages_to_gemini_format(messages)
            
            # æ„å»ºGemini API URL
            model_name = getattr(settings, "GEMINI_MODEL", "gemini-2.5-pro")
            api_base = getattr(settings, "GEMINI_API_BASE", "https://generativelanguage.googleapis.com")
            
            # ç¡®ä¿APIåŸºç¡€URLæ ¼å¼æ­£ç¡®
            if not api_base.endswith('/'):
                api_base += '/'
            
            url = f"{api_base}v1beta/models/{model_name}:generateContent"
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                "key": settings.GEMINI_API_KEY
            }
            
            # æ„å»ºè¯·æ±‚ä½“
            request_data = {
                "contents": gemini_contents,
                "generationConfig": {
                    "temperature": getattr(settings, "LLM_TEMPERATURE", 0.7),
                    "maxOutputTokens": getattr(settings, "LLM_MAX_NEW_TOKENS", 2048),  # å¢åŠ tokené™åˆ¶
                    "topP": 0.8,
                    "topK": 10
                }
            }
            
            headers = {
                "Content-Type": "application/json"
            }
            
            self.logger.debug(f"Gemini APIè¯·æ±‚URL: {url}")
            self.logger.debug(f"Gemini APIè¯·æ±‚æ•°æ®: {json.dumps(request_data, ensure_ascii=False, indent=2)}")
            
            # å‘é€è¯·æ±‚
            response = requests.post(
                url,
                params=params,
                json=request_data,
                headers=headers,
                timeout=60
            )
            
            self.logger.debug(f"Gemini APIå“åº”çŠ¶æ€: {response.status_code}")
            
            response.raise_for_status()
            
            result = response.json()
            self.logger.debug(f"Gemini APIå“åº”: {json.dumps(result, ensure_ascii=False, indent=2)}")
            
            # è§£æGeminiå“åº”æ ¼å¼
            if "candidates" not in result or not result["candidates"]:
                self.logger.warning(f"Gemini APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"
            
            candidate = result["candidates"][0]
            
            # æ£€æŸ¥æ˜¯å¦å› ä¸ºè¾¾åˆ°tokené™åˆ¶è€Œæˆªæ–­
            finish_reason = candidate.get("finishReason", "")
            if finish_reason == "MAX_TOKENS":
                self.logger.warning("Gemini APIè¾¾åˆ°æœ€å¤§tokené™åˆ¶ï¼Œå“åº”å¯èƒ½ä¸å®Œæ•´")
            
            if "content" not in candidate:
                self.logger.warning(f"Gemini APIå“åº”ç¼ºå°‘contentå­—æ®µ: {candidate}")
                return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"
            
            content_obj = candidate["content"]
            
            # æ£€æŸ¥contentç»“æ„
            if "parts" not in content_obj:
                self.logger.warning(f"Gemini APIå“åº”contentç¼ºå°‘partså­—æ®µ: {content_obj}")
                # å°è¯•ä»å…¶ä»–å¯èƒ½çš„å­—æ®µæå–å†…å®¹
                if "text" in content_obj:
                    content = content_obj["text"]
                else:
                    return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"
            else:
                # æå–æ–‡æœ¬å†…å®¹
                parts = content_obj["parts"]
                if not parts or "text" not in parts[0]:
                    self.logger.warning("Gemini APIè¿”å›äº†ç©ºå†…å®¹")
                    return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"
                content = parts[0]["text"]
            
            if content is None or content.strip() == "":
                self.logger.warning("Gemini APIè¿”å›äº†ç©ºå›å¤")
                return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚"
            
            reply = content.strip()
            self.logger.debug(f"ğŸ¤– Gemini APIå›å¤: {reply}")
            return reply
            
        except requests.RequestException as e:
            self.logger.error(f"Gemini APIè¯·æ±‚å¤±è´¥: {e}")
            if hasattr(e, 'response') and e.response is not None:
                try:
                    error_detail = e.response.json()
                    self.logger.error(f"Gemini APIé”™è¯¯è¯¦æƒ…: {error_detail}")
                except:
                    self.logger.error(f"Gemini APIé”™è¯¯å“åº”: {e.response.text}")
            return f"æŠ±æ­‰ï¼Œæ— æ³•è¿æ¥åˆ°GeminiæœåŠ¡ã€‚é”™è¯¯: {str(e)}"
        except Exception as e:
            self.logger.error(f"Gemini APIè°ƒç”¨å¼‚å¸¸: {e}")
            return f"æŠ±æ­‰ï¼Œå¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯ã€‚é”™è¯¯: {str(e)}"
    
    def _convert_messages_to_gemini_format(self, messages):
        """å°†OpenAIæ ¼å¼çš„messagesè½¬æ¢ä¸ºGeminiæ ¼å¼"""
        gemini_contents = []
        
        for message in messages:
            role = message.get("role", "user")
            content = message.get("content", "")
            
            # Geminiçš„è§’è‰²æ˜ å°„
            if role == "system":
                # ç³»ç»Ÿæ¶ˆæ¯ä½œä¸ºç”¨æˆ·æ¶ˆæ¯çš„å‰ç¼€
                gemini_role = "user"
                content = f"[ç³»ç»Ÿæç¤º] {content}"
            elif role == "assistant":
                gemini_role = "model"
            else:  # user
                gemini_role = "user"
            
            gemini_contents.append({
                "role": gemini_role,
                "parts": [{"text": content}]
            })
        
        return gemini_contents


# -----------------------------------------------------------------------------
# å…¼å®¹æ€§å¯¼å‡ºå‡½æ•° (ç”¨äºå‘åå…¼å®¹)
# -----------------------------------------------------------------------------

def get_llm_response(prompt, history=None, personality=""):
    """
    ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå›å¤ (å…¼å®¹æ—§ç‰ˆ)
    """
    engine = DialogueEngine()
    return engine._get_llm_response(prompt, history, personality)

def generate_response(user_query, memory_context, personality=""):
    """
    ç”Ÿæˆå›å¤ï¼Œè€ƒè™‘è®°å¿†ä¸Šä¸‹æ–‡å’Œäººæ ¼ (å…¼å®¹æ—§ç‰ˆ)
    """
    engine = DialogueEngine()
    return engine.generate_response(user_query, memory_context, personality)

def retrieve_memories(query, limit=5, memory_manager=None):
    """
    ä»è®°å¿†ç®¡ç†å™¨ä¸­æ£€ç´¢ç›¸å…³è®°å¿†
    
    å‚æ•°:
        query: æŸ¥è¯¢æ–‡æœ¬
        limit: æœ€å¤§è¿”å›æ•°é‡
        memory_manager: è®°å¿†ç®¡ç†å™¨å®ä¾‹
    
    è¿”å›:
        æ ¼å¼åŒ–çš„è®°å¿†æ–‡æœ¬
    """
    if not memory_manager:
        return "æ²¡æœ‰å¯ç”¨çš„è®°å¿†ã€‚"
    
    try:
        # ä»è®°å¿†ç®¡ç†å™¨æ£€ç´¢è®°å¿†
        results = memory_manager.retrieve_memory(
            query, 
            limit=limit,
            parallel=True,
            include_associations=True,
            check_conflicts=True
        )
        
        if not results:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®°å¿†ã€‚"
        
        # æ ¼å¼åŒ–è®°å¿†
        formatted_memories = []
        
        for memory in results:
            role = memory.get("role", "system")
            content = memory.get("content", "")
            timestamp = memory.get("timestamp", "")
            
            # æ ¼å¼åŒ–æ—¶é—´
            if isinstance(timestamp, (int, float)):
                import datetime
                timestamp = datetime.datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M')
            
            # å¤„ç†ç‰¹æ®Šæ ‡è®°
            prefix = ""
            if memory.get("is_associated", False):
                prefix = "[å…³è”è®°å¿†] "
            elif memory.get("is_summary", False):
                prefix = "[è®°å¿†æ‘˜è¦] "
            elif memory.get("status") == "superseded":
                prefix = "[å·²æ›´æ–°çš„ä¿¡æ¯] "
            
            # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
            context = ""
            if "context" in memory:
                context = f" (å¤‡æ³¨: {memory['context']})"
            
            formatted_memories.append(f"{prefix}[{timestamp}] {role}: {content}{context}")
        
        # æ·»åŠ ä¸€ä¸ªç®€çŸ­çš„ä»‹ç»
        header = "ç³»ç»Ÿè®°å¿†:"
        formatted_text = header + "\n" + "\n".join(formatted_memories)
        
        return formatted_text
        
    except Exception as e:
        logger.error(f"è®°å¿†æ£€ç´¢å¤±è´¥: {e}")
        return "è®°å¿†æ£€ç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ã€‚"