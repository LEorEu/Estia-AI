# core/dialogue_engine.py

"""
æœ¬æ¨¡å—æ˜¯ AI çš„â€œæ€è€ƒâ€æ ¸å¿ƒã€‚ï¼ˆV2.0 - OpenAI API å…¼å®¹ç‰ˆï¼‰
å®ƒè´Ÿè´£æ¥æ”¶ç”¨æˆ·çš„æ–‡æœ¬è¾“å…¥ï¼Œå°†å…¶æ‰“åŒ…æˆ OpenAI å…¼å®¹çš„æ ¼å¼ï¼Œ
å‘é€ç»™æœ¬åœ°è¿è¡Œçš„ã€è½»é‡çº§çš„ llama.cpp æœåŠ¡å™¨ï¼Œå¹¶è§£æè¿”å›çš„å›å¤ã€‚
"""

# -----------------------------------------------------------------------------
# å¯¼å…¥å¿…è¦çš„åº“
# -----------------------------------------------------------------------------

import requests                 # å¯¼å…¥ requests åº“ï¼Œç”¨äºå‘é€ HTTP API è¯·æ±‚ã€‚
import json                     # å¯¼å…¥ json åº“ï¼Œç”¨äºå¤„ç† JSON æ•°æ®æ ¼å¼ã€‚
from config import settings     # ä»æˆ‘ä»¬çš„é…ç½®æ–‡ä»¶ä¸­å¯¼å…¥ settingsã€‚


# -----------------------------------------------------------------------------
# åŠŸèƒ½å‡½æ•°å®šä¹‰
# -----------------------------------------------------------------------------

def get_llm_response(user_prompt: str, chat_history: list, retrieved_memories: list, personality: str) -> str:

    # æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„é•¿æœŸè®°å¿†ï¼Œä½œä¸ºâ€œèƒŒæ™¯èµ„æ–™â€æä¾›ç»™LLM
    context_header = "--- ä»¥ä¸‹æ˜¯ä½ å¯èƒ½ä¼šç”¨åˆ°çš„ã€ä»ä½ çš„é•¿æœŸè®°å¿†ä¸­æå–çš„ç›¸å…³ä¿¡æ¯ï¼Œè¯·å‚è€ƒè¿™äº›ä¿¡æ¯æ¥æ›´å¥½åœ°å›ç­”å½“å‰é—®é¢˜ ---"
    formatted_memories = "\n".join([f"- [å†å²å¯¹è¯äº {mem['timestamp']}] {mem['role']}: {mem['content']}" for mem in retrieved_memories])

    # åªæœ‰åœ¨æ‰¾åˆ°äº†ç›¸å…³è®°å¿†æ—¶ï¼Œæ‰æ„å»ºè¿™æ®µèƒŒæ™¯èµ„æ–™
    if retrieved_memories:
        memory_context = f"{context_header}\n{formatted_memories}\n--- èƒŒæ™¯èµ„æ–™ç»“æŸ ---"
    else:
        memory_context = ""

    # æ„å»ºæœ€ç»ˆçš„æ¶ˆæ¯åˆ—è¡¨
    messages = [{"role": "system", "content": personality}]

    # å¦‚æœæœ‰èƒŒæ™¯èµ„æ–™ï¼Œå°±ä½œä¸ºä¸€æ¡é¢å¤–çš„ç³»ç»Ÿä¿¡æ¯æ’å…¥
    if memory_context:
        messages.append({"role": "system", "name": "memory", "content": memory_context})

    messages.extend(chat_history)
    messages.append({"role": "user", "content": user_prompt})
    
    """
    å‘æœ¬åœ°çš„ llama.cpp æœåŠ¡å™¨ (OpenAI å…¼å®¹ API) å‘é€è¯·æ±‚å¹¶è·å–å›å¤ã€‚

    å‚æ•°:
        user_prompt (str): ä»è¯­éŸ³è¯†åˆ«æ¨¡å—ä¼ æ¥çš„ã€ç”¨æˆ·çš„æé—®æ–‡æœ¬ã€‚
        personality (str): AI çš„ç³»ç»Ÿçº§äººæ ¼è®¾å®šï¼Œå¯ä»¥åŠ¨æ€ä¼ å…¥ã€‚

    è¿”å›:
        str: LLM ç”Ÿæˆçš„å›å¤æ–‡æœ¬ã€‚å¦‚æœå‡ºé”™åˆ™è¿”å›ä¸€æ¡é”™è¯¯ä¿¡æ¯ã€‚
    """
    # æ‰“å°æç¤ºï¼Œè¡¨ç¤ºâ€œå¤§è„‘â€æ­£åœ¨æ€è€ƒ
    print("ğŸ§  LLM æ­£åœ¨æ€è€ƒä¸­...")

    # --- API è¯·æ±‚çš„å¤´éƒ¨ä¿¡æ¯ ---
    # æŒ‡å®šæˆ‘ä»¬å‘é€çš„æ•°æ®æ˜¯ JSON æ ¼å¼
    headers = {
        "Content-Type": "application/json"
    }

    # å…ˆæ„å»ºä¸€ä¸ªåŒ…å«ç³»ç»Ÿäººæ ¼è®¾å®šçš„åŸºç¡€æ¶ˆæ¯åˆ—è¡¨
    messages = [
        {"role": "system", "content": personality}
    ]
    # ä½¿ç”¨ .extend() æ–¹æ³•ï¼ŒæŠŠâ€œè®°å¿†ç¬”è®°æœ¬â€ï¼ˆchat_historyï¼‰é‡Œçš„æ‰€æœ‰å†å²å¯¹è¯éƒ½åŠ è¿›æ¥
    messages.extend(chat_history)

    # æœ€åï¼Œå†æŠŠç”¨æˆ·è¿™ä¸€è½®çš„æ–°é—®é¢˜åŠ åˆ°æœ«å°¾
    messages.append({"role": "user", "content": user_prompt})

    # æ„å»ºç¬¦åˆ OpenAI API æ ¼å¼çš„â€œè½½è·â€(payload)
    payload = {
        "model": "Mistral-Small-3.1-24B-Instruct-2503-Q4_K_M.gguf",  # è¿™é‡Œçš„æ¨¡å‹åå¯ä»¥éšä¾¿å†™ï¼Œå› ä¸ºæœåŠ¡å™¨åªåŠ è½½äº†ä¸€ä¸ªæ¨¡å‹ã€‚
        "messages": messages, # <- ä½¿ç”¨æˆ‘ä»¬æ„å»ºçš„ã€åŒ…å«å†å²çš„å®Œæ•´æ¶ˆæ¯åˆ—è¡¨
        "temperature": settings.LLM_TEMPERATURE,    # ä»é…ç½®æ–‡ä»¶è¯»å–â€œæ¸©åº¦â€å‚æ•°
        "max_tokens": settings.LLM_MAX_NEW_TOKENS   # ä»é…ç½®æ–‡ä»¶è¯»å–â€œæœ€å¤§ç”Ÿæˆé•¿åº¦â€å‚æ•°
    }

    try:
        # ä½¿ç”¨ requests.post() æ–¹æ³•å‘æˆ‘ä»¬æ–°çš„ API URL å‘é€è¯·æ±‚
        # æ³¨æ„ï¼šURL æ˜¯ä» settings æ–‡ä»¶ä¸­è¯»å–çš„ï¼Œè¯·ç¡®ä¿ä½ å·²ç»æŠŠå®ƒæ”¹æˆäº† http://127.0.0.1:8080/v1/chat/completions
        response = requests.post(settings.LLM_API_URL, headers=headers, json=payload)

        # æ£€æŸ¥æœåŠ¡å™¨çš„è¿”å›çŠ¶æ€ç ï¼Œ200 ä»£è¡¨æˆåŠŸ
        if response.status_code == 200:
            # è§£æè¿”å›çš„ JSON æ•°æ®
            result = response.json()
            
            # --- è§£æ OpenAI æ ¼å¼çš„å›å¤ ---
            # æå–å‡ºæˆ‘ä»¬éœ€è¦çš„ã€ç”±AIç”Ÿæˆçš„æ–‡æœ¬
            # æ–°çš„æ ¼å¼ä¸‹ï¼Œå›å¤æ–‡æœ¬åœ¨ 'choices' åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ çš„ 'message' å­—å…¸çš„ 'content' é”®ä¸­
            ai_response = result['choices'][0]['message']['content']

            # æ‰“å° AI çš„åŸå§‹å›å¤ï¼Œæ–¹ä¾¿è°ƒè¯•
            print(f"ğŸ¤– AI åŸå§‹å›å¤: {ai_response}")
            
            # è¿”å›æœ€ç»ˆçš„å›å¤æ–‡æœ¬
            return ai_response
        else:
            # å¦‚æœæœåŠ¡å™¨è¿”å›äº†é”™è¯¯çŠ¶æ€ç ï¼Œæ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
            print(f"âŒ LLM API è¿”å›é”™è¯¯ï¼ŒçŠ¶æ€ç : {response.status_code}, å†…å®¹: {response.text}")
            return "æŠ±æ­‰ï¼Œæˆ‘çš„å¤§è„‘å¥½åƒå‡ºäº†ä¸€ç‚¹å°é—®é¢˜ã€‚"

    except requests.exceptions.RequestException as e:
        # å¦‚æœåœ¨å‘é€è¯·æ±‚æ—¶å‘ç”Ÿäº†ç½‘ç»œé”™è¯¯ï¼ˆä¾‹å¦‚ llama.cpp æœåŠ¡å™¨æ²¡æ‰“å¼€ï¼‰
        print(f"âŒ æ— æ³•è¿æ¥åˆ° LLM API: {e}")
        return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•è¿æ¥åˆ°æˆ‘çš„å¤§è„‘ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å·²å¯åŠ¨ã€‚"