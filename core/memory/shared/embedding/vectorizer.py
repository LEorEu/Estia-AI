"""
æ–‡æœ¬å‘é‡åŒ–æ¨¡å— - è´Ÿè´£å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
"""

import os
import numpy as np
import logging
from typing import List, Dict, Any, Optional, Union, Tuple
import time
from pathlib import Path

# å°è¯•å¯¼å…¥æ—¥å¿—å·¥å…·
try:
    from core.utils.logger import get_logger
    logger = get_logger("estia.memory.embedding.vectorizer")
except ImportError:
    # å¦‚æœè¿˜æ²¡æœ‰æ—¥å¿—å·¥å…·ï¼Œä½¿ç”¨æ ‡å‡†æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger("estia.memory.embedding.vectorizer")

# å°è¯•å¯¼å…¥ç¼“å­˜æ¨¡å—
try:
    from .cache import EnhancedMemoryCache
    EmbeddingCache = EnhancedMemoryCache  # ä½¿ç”¨æ–°çš„å¢å¼ºç¼“å­˜
except ImportError:
    logger.warning("æ— æ³•å¯¼å…¥EnhancedMemoryCacheï¼Œå°†ä¸ä½¿ç”¨ç¼“å­˜åŠŸèƒ½")
    EmbeddingCache = None

def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
    
    å‚æ•°:
        vec1: ç¬¬ä¸€ä¸ªå‘é‡
        vec2: ç¬¬äºŒä¸ªå‘é‡
        
    è¿”å›:
        ä½™å¼¦ç›¸ä¼¼åº¦å€¼ (0-1ä¹‹é—´)
    """
    try:
        # ç¡®ä¿å‘é‡æ˜¯numpyæ•°ç»„
        vec1 = np.array(vec1, dtype=np.float32)
        vec2 = np.array(vec2, dtype=np.float32)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        similarity = dot_product / (norm1 * norm2)
        return float(similarity)
        
    except Exception as e:
        logger.error(f"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¤±è´¥: {e}")
        return 0.0

class TextVectorizer:
    """
    æ–‡æœ¬å‘é‡åŒ–ç±»ï¼Œè´Ÿè´£å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
    
    æ”¯æŒå¤šç§Embeddingæ¨¡å‹:
    - æœ¬åœ°æ¨¡å‹ (sentence-transformers)
    - OpenAI API
    - è‡ªå®šä¹‰æ¨¡å‹
    """
    
    # ğŸ”¥ å•ä¾‹æ¨¡å¼ï¼šå…¨å±€å”¯ä¸€å®ä¾‹
    _instance = None
    _initialized = False
    
    # æ”¯æŒçš„æ¨¡å‹ç±»å‹
    MODEL_TYPES = ["sentence-transformers", "openai", "custom"]
    
    # é»˜è®¤æ¨¡å‹é…ç½®
    DEFAULT_MODEL = "sentence-transformers"
    DEFAULT_MODEL_NAME = "Qwen/Qwen3-Embedding-0.6B"  # ä½¿ç”¨é˜¿é‡Œå·´å·´çš„Qwenæ¨¡å‹
    
    def __new__(cls, *args, **kwargs):
        """å•ä¾‹æ¨¡å¼ï¼šç¡®ä¿å…¨å±€åªæœ‰ä¸€ä¸ªå®ä¾‹"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, model_type: Optional[str] = None, model_name: Optional[str] = None, 
                 api_key: Optional[str] = None, cache_dir: Optional[str] = None, 
                 use_cache: bool = True, device: str = "cpu"):
        """
        åˆå§‹åŒ–æ–‡æœ¬å‘é‡åŒ–å™¨
        
        å‚æ•°:
            model_type: æ¨¡å‹ç±»å‹ï¼Œå¯é€‰å€¼ä¸º "sentence-transformers", "openai", "custom"
            model_name: æ¨¡å‹åç§°ï¼Œå¯¹äºsentence-transformersæ˜¯æ¨¡å‹IDï¼Œå¯¹äºopenaiæ˜¯æ¨¡å‹åç§°
            api_key: APIå¯†é’¥ï¼Œç”¨äºOpenAI API
            cache_dir: ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä½¿ç”¨é¡¹ç›®å†…éƒ¨cacheç›®å½•
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            device: è®¾å¤‡ï¼Œå¯é€‰å€¼ä¸º "cpu", "cuda", "mps"ï¼ˆå¯¹äºApple Siliconï¼‰
        """
        # ğŸ”¥ å•ä¾‹æ¨¡å¼ï¼šåªåˆå§‹åŒ–ä¸€æ¬¡
        if self._initialized:
            logger.debug("TextVectorizerå·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
            return
            
        self.model_type = model_type or self.DEFAULT_MODEL
        self.model_name = model_name or self.DEFAULT_MODEL_NAME
        if self.model_name is None:
            self.model_name = self.DEFAULT_MODEL_NAME
            logger.warning("æ¨¡å‹åç§°ä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
        self.api_key = api_key
        self.device = device
        self.use_cache = use_cache and EmbeddingCache is not None
        
        # ğŸ”¥ ä¼˜åŒ–ï¼šåŒºåˆ†æ¨¡å‹ç¼“å­˜å’Œè¿è¡Œæ—¶ç¼“å­˜
        if cache_dir is None:
            # è¿è¡Œæ—¶ç¼“å­˜ä½¿ç”¨data/memory/cacheï¼ˆä¿æŒç°æœ‰æ•°æ®ï¼‰
            self.cache_dir = os.path.join("data", "memory", "cache")
        else:
            self.cache_dir = cache_dir
        
        # æ¨¡å‹ç¼“å­˜ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•çš„cacheï¼ˆé¿å…ç½‘ç»œä¸‹è½½ï¼‰
        self.model_cache_dir = str(Path(__file__).parent.parent.parent.parent.parent / "cache")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = None
        self.vector_dim = 0
        
        # åˆå§‹åŒ–å¢å¼ºç¼“å­˜
        self.cache = None
        if self.use_cache and EmbeddingCache is not None:
            self.cache = EmbeddingCache(cache_dir=self.cache_dir)
            logger.info("å·²å¯ç”¨å¢å¼ºç‰ˆè®°å¿†ç¼“å­˜")
        
        # ğŸ”¥ æ–°å¢ï¼šè‡ªåŠ¨æ³¨å†Œç¼“å­˜é€‚é…å™¨åˆ°ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
        self._register_cache_adapters()
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
        
        # ğŸ”¥ æ ‡è®°ä¸ºå·²åˆå§‹åŒ–
        self._initialized = True
        
        logger.info(f"æ–‡æœ¬å‘é‡åŒ–å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {self.model_type}/{self.model_name}")
        logger.info(f"ç¼“å­˜ç›®å½•: {self.cache_dir}")
    
    def _register_cache_adapters(self) -> None:
        """è‡ªåŠ¨æ³¨å†Œç¼“å­˜é€‚é…å™¨åˆ°ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨"""
        if not self.use_cache:
            return
            
        try:
            from ...shared.caching.cache_manager import UnifiedCacheManager
            from ...shared.caching.cache_adapters import EnhancedMemoryCacheAdapter
            
            unified_cache = UnifiedCacheManager.get_instance()
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»æ³¨å†Œäº†å‘é‡ç¼“å­˜é€‚é…å™¨
            registered_caches = list(unified_cache.caches.keys())
            if "embedding_cache" not in registered_caches and self.cache:
                # åˆ›å»ºå¹¶æ³¨å†Œå‘é‡ç¼“å­˜é€‚é…å™¨
                vector_adapter = EnhancedMemoryCacheAdapter(
                    source_cache=self.cache,
                    cache_id="embedding_cache",
                    auto_register=False  # æ‰‹åŠ¨æ³¨å†Œï¼Œé¿å…é‡å¤
                )
                unified_cache.register_cache(vector_adapter)
                logger.info("âœ… å·²è‡ªåŠ¨æ³¨å†Œå‘é‡ç¼“å­˜é€‚é…å™¨åˆ°ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨")
            else:
                logger.debug("å‘é‡ç¼“å­˜é€‚é…å™¨å·²å­˜åœ¨æˆ–æ— éœ€æ³¨å†Œ")
                
        except Exception as e:
            logger.warning(f"æ³¨å†Œç¼“å­˜é€‚é…å™¨å¤±è´¥: {e}")
            # ä¸å½±å“ä¸»è¦åŠŸèƒ½ï¼Œç»§ç»­æ‰§è¡Œ
    
    def _load_model(self) -> None:
        """åŠ è½½Embeddingæ¨¡å‹"""
        if self.model_type == "sentence-transformers":
            self._load_sentence_transformers()
        elif self.model_type == "openai":
            self._load_openai()
        elif self.model_type == "custom":
            self._load_custom_model()
        else:
            logger.warning(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {self.model_type}ï¼Œå°†ä½¿ç”¨é»˜è®¤æ¨¡å‹")
            self.model_type = self.DEFAULT_MODEL
            self._load_sentence_transformers()
    
    def _load_sentence_transformers(self) -> None:
        """åŠ è½½sentence-transformersæ¨¡å‹ï¼Œä½¿ç”¨é¡¹ç›®å†…éƒ¨ç¼“å­˜"""
        try:
            from sentence_transformers import SentenceTransformer
            
            # ğŸ”¥ è®¾ç½®é¡¹ç›®å†…éƒ¨ç¼“å­˜ç¯å¢ƒï¼ˆç”¨äºæ¨¡å‹ç¼“å­˜ï¼‰
            os.environ['HUGGINGFACE_HUB_CACHE'] = self.model_cache_dir
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = self.model_cache_dir
            os.environ['HF_HOME'] = self.model_cache_dir
            
            logger.info(f"ğŸ”§ ä½¿ç”¨é¡¹ç›®æ¨¡å‹ç¼“å­˜ç›®å½•: {self.model_cache_dir}")
            
            # æ£€æŸ¥model_nameæ˜¯å¦æœ‰æ•ˆ
            if self.model_name is None:
                logger.error("æ¨¡å‹åç§°æœªè®¾ç½®")
                raise ValueError("æ¨¡å‹åç§°æœªè®¾ç½®")
            
            # ç¡®ä¿model_nameæ˜¯å­—ç¬¦ä¸²
            if not isinstance(self.model_name, str):
                logger.error(f"æ¨¡å‹åç§°ç±»å‹é”™è¯¯: {type(self.model_name)}")
                self.model_name = str(self.model_name)
            
            logger.info(f"ğŸ”„ åŠ è½½æ¨¡å‹: {self.model_name}")
            
            # ğŸ”¥ é¦–å…ˆæ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å­˜åœ¨
            expected_model_path = os.path.join(self.model_cache_dir, f"models--{self.model_name.replace('/', '--')}")
            
            if os.path.exists(expected_model_path):
                logger.info(f"ğŸ¯ å‘ç°æœ¬åœ°æ¨¡å‹: {expected_model_path}")
                # è®¾ç½®ä¸ºç¦»çº¿æ¨¡å¼ï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å‹
                os.environ['HF_HUB_OFFLINE'] = '1'
                os.environ['TRANSFORMERS_OFFLINE'] = '1'
                
                try:
                    self.model = SentenceTransformer(
                        self.model_name,
                        device=self.device,
                        cache_folder=self.model_cache_dir,
                        trust_remote_code=True
                    )
                    logger.info(f"âœ… æˆåŠŸåŠ è½½æœ¬åœ°æ¨¡å‹: {self.model_name}")
                    
                except Exception as local_error:
                    logger.warning(f"æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {local_error}")
                    # æ¸…é™¤ç¦»çº¿è®¾ç½®ï¼Œå°è¯•åœ¨çº¿åŠ è½½
                    if 'HF_HUB_OFFLINE' in os.environ:
                        del os.environ['HF_HUB_OFFLINE']
                    if 'TRANSFORMERS_OFFLINE' in os.environ:
                        del os.environ['TRANSFORMERS_OFFLINE']
                    raise local_error
            else:
                logger.warning(f"æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨: {expected_model_path}")
                logger.info("ğŸŒ å°è¯•åœ¨çº¿æ¨¡å¼...")
                
                # å°è¯•å¤šä¸ªé•œåƒç«™å’Œæ¨¡å‹åç§°
                model_options = [
                    # åŸå§‹æ¨¡å‹åç§° + é•œåƒç«™
                    (self.model_name, 'https://hf-mirror.com'),
                    # å¤‡ç”¨æ¨¡å‹åç§° + é•œåƒç«™
                    ('sentence-transformers/all-MiniLM-L6-v2', 'https://hf-mirror.com'),
                    # åŸå§‹æ¨¡å‹åç§° + å®˜æ–¹ç«™ç‚¹
                    (self.model_name, 'https://huggingface.co'),
                    # å¤‡ç”¨æ¨¡å‹åç§° + å®˜æ–¹ç«™ç‚¹
                    ('sentence-transformers/all-MiniLM-L6-v2', 'https://huggingface.co'),
                ]
                
                model_loaded = False
                for model_name, endpoint in model_options:
                    try:
                        logger.info(f"ğŸ”„ å°è¯•åŠ è½½ {model_name} ä» {endpoint}")
                        os.environ['HF_ENDPOINT'] = endpoint
                        
                        self.model = SentenceTransformer(
                            model_name,
                            device=self.device,
                            cache_folder=self.model_cache_dir,
                            trust_remote_code=True
                        )
                        self.model_name = model_name  # æ›´æ–°æˆåŠŸçš„æ¨¡å‹åç§°
                        logger.info(f"âœ… åœ¨çº¿æ¨¡å¼åŠ è½½æˆåŠŸ: {model_name}")
                        model_loaded = True
                        break
                        
                    except Exception as online_error:
                        logger.warning(f"å°è¯• {model_name} å¤±è´¥: {online_error}")
                        continue
                
                if not model_loaded:
                    logger.error("æ‰€æœ‰åœ¨çº¿æ¨¡å‹åŠ è½½å°è¯•éƒ½å¤±è´¥äº†")
                    raise RuntimeError("æ— æ³•åŠ è½½ä»»ä½• sentence-transformers æ¨¡å‹")
            
            # è·å–å‘é‡ç»´åº¦
            self.vector_dim = self.model.get_sentence_embedding_dimension()
            logger.info(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {self.vector_dim}")
            
        except ImportError:
            logger.error("æœªæ‰¾åˆ°sentence-transformersåº“ï¼Œè¯·å®‰è£…: pip install sentence-transformers")
            raise
        except Exception as e:
            logger.error(f"åŠ è½½sentence-transformersæ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def _load_openai(self) -> None:
        """åŠ è½½OpenAI API"""
        try:
            import openai
            
            # è®¾ç½®APIå¯†é’¥
            if self.api_key:
                openai.api_key = self.api_key
            elif "OPENAI_API_KEY" in os.environ:
                openai.api_key = os.environ["OPENAI_API_KEY"]
            else:
                logger.error("æœªæä¾›OpenAI APIå¯†é’¥")
                raise ValueError("æœªæä¾›OpenAI APIå¯†é’¥ï¼Œè¯·é€šè¿‡api_keyå‚æ•°æˆ–OPENAI_API_KEYç¯å¢ƒå˜é‡è®¾ç½®")
            
            # é»˜è®¤æ¨¡å‹åç§°
            if not self.model_name:
                self.model_name = "text-embedding-ada-002"
            
            # è®¾ç½®æ¨¡å‹ï¼ˆå®é™…ä¸Šä¸éœ€è¦é¢„åŠ è½½ï¼‰
            self.model = openai
            
            # æ ¹æ®æ¨¡å‹è®¾ç½®å‘é‡ç»´åº¦
            model_dims = {
                "text-embedding-ada-002": 1536,
                "text-embedding-3-small": 1536,
                "text-embedding-3-large": 3072
            }
            self.vector_dim = model_dims.get(self.model_name, 1536)
            
            logger.info(f"OpenAI APIé…ç½®æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {self.model_name}, å‘é‡ç»´åº¦: {self.vector_dim}")
            
        except ImportError:
            logger.error("æœªæ‰¾åˆ°openaiåº“ï¼Œè¯·å®‰è£…: pip install openai")
            raise
        except Exception as e:
            logger.error(f"é…ç½®OpenAI APIå¤±è´¥: {e}")
            raise
    
    def _load_custom_model(self) -> None:
        """åŠ è½½è‡ªå®šä¹‰æ¨¡å‹"""
        # è¿™é‡Œå¯ä»¥å®ç°åŠ è½½è‡ªå®šä¹‰æ¨¡å‹çš„é€»è¾‘
        # ä¾‹å¦‚åŸºäºå…¶ä»–æ¡†æ¶çš„æ¨¡å‹æˆ–è‡ªå·±è®­ç»ƒçš„æ¨¡å‹
        logger.warning("è‡ªå®šä¹‰æ¨¡å‹åŠ è½½æœªå®ç°ï¼Œè¯·åœ¨å­ç±»ä¸­å®ç°")
        raise NotImplementedError("è‡ªå®šä¹‰æ¨¡å‹åŠ è½½æœªå®ç°")
    
    def encode(self, texts: Union[str, List[str]], 
               batch_size: int = 32, 
               show_progress: bool = False,
               memory_weights: Optional[Union[float, List[float]]] = None) -> np.ndarray:
        """
        å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡
        
        å‚æ•°:
            texts: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            memory_weights: è®°å¿†é‡è¦æ€§æƒé‡ï¼Œç”¨äºæ™ºèƒ½ç¼“å­˜
            
        è¿”å›:
            np.ndarray: æ–‡æœ¬å‘é‡ï¼Œå½¢çŠ¶ä¸º (n, vector_dim) æˆ– (vector_dim,)
        """
        if self.model is None:
            logger.error("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç¼–ç æ–‡æœ¬")
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç¼–ç æ–‡æœ¬")
        
        # ç¡®ä¿è¾“å…¥æ˜¯åˆ—è¡¨
        is_single_text = isinstance(texts, str)
        if is_single_text:
            texts = [texts]
            if memory_weights is not None and not isinstance(memory_weights, list):
                memory_weights = [memory_weights]
        
        # å¤„ç†æƒé‡
        if memory_weights is None:
            memory_weights = [1.0] * len(texts)
        elif isinstance(memory_weights, (int, float)):
            # å•ä¸ªæƒé‡å€¼ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
            memory_weights = [float(memory_weights)] * len(texts)
        elif len(memory_weights) != len(texts):
            logger.warning(f"æƒé‡æ•°é‡({len(memory_weights)})ä¸æ–‡æœ¬æ•°é‡({len(texts)})ä¸åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤æƒé‡")
            memory_weights = [1.0] * len(texts)
        
        # å°è¯•ä½¿ç”¨ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
        unified_cache = None
        try:
            from ...shared.caching.cache_manager import UnifiedCacheManager
            unified_cache = UnifiedCacheManager.get_instance()
        except Exception as e:
            logger.debug(f"ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨ä¸å¯ç”¨: {e}")
        
        # æ£€æŸ¥ç¼“å­˜
        if self.use_cache and (unified_cache or self.cache):
            # å°è¯•ä»ç¼“å­˜è·å–æ‰€æœ‰å‘é‡
            cached_vectors = []
            texts_to_encode = []
            text_indices = []
            weights_to_encode = []
            
            for i, (text, weight) in enumerate(zip(texts, memory_weights)):
                vector = None
                
                # ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
                if unified_cache:
                    try:
                        vector = unified_cache.get(text)
                    except Exception as e:
                        logger.debug(f"ç»Ÿä¸€ç¼“å­˜è·å–å¤±è´¥: {e}")
                
                # é™çº§åˆ°ç›´æ¥ç¼“å­˜
                if vector is None and self.cache:
                    try:
                        vector = self.cache.get(text, memory_weight=weight)
                    except Exception as e:
                        logger.debug(f"ç›´æ¥ç¼“å­˜è·å–å¤±è´¥: {e}")
                
                if vector is not None:
                    cached_vectors.append((i, vector))
                else:
                    texts_to_encode.append(text)
                    text_indices.append(i)
                    weights_to_encode.append(weight)
            
            # å¦‚æœæ‰€æœ‰å‘é‡éƒ½åœ¨ç¼“å­˜ä¸­
            if len(texts_to_encode) == 0:
                logger.debug(f"æ‰€æœ‰ {len(texts)} ä¸ªæ–‡æœ¬éƒ½ä»ç¼“å­˜ä¸­è·å–")
                # æ„å»ºç»“æœæ•°ç»„
                results = np.zeros((len(texts), self.vector_dim), dtype=np.float32)
                for i, vector in cached_vectors:
                    results[i] = vector
                
                return results[0] if is_single_text else results
            
            # ç¼–ç æœªç¼“å­˜çš„æ–‡æœ¬
            if len(texts_to_encode) > 0:
                logger.debug(f"ä»ç¼“å­˜è·å– {len(cached_vectors)} ä¸ªå‘é‡ï¼Œéœ€è¦ç¼–ç  {len(texts_to_encode)} ä¸ªæ–‡æœ¬")
                new_vectors = self._encode_texts(texts_to_encode, batch_size, show_progress)
                
                # å°†æ–°å‘é‡æ·»åŠ åˆ°ç¼“å­˜
                for text, vector, weight in zip(texts_to_encode, new_vectors, weights_to_encode):
                    # ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
                    if unified_cache:
                        try:
                            unified_cache.put(text, vector, {"source": "vectorizer", "weight": weight})
                        except Exception as e:
                            logger.debug(f"ç»Ÿä¸€ç¼“å­˜å­˜å‚¨å¤±è´¥: {e}")
                    
                    # é™çº§åˆ°ç›´æ¥ç¼“å­˜
                    if self.cache:
                        try:
                            self.cache.put(text, vector, memory_weight=weight)
                        except Exception as e:
                            logger.debug(f"ç›´æ¥ç¼“å­˜å­˜å‚¨å¤±è´¥: {e}")
                
                # æ„å»ºå®Œæ•´ç»“æœæ•°ç»„
                results = np.zeros((len(texts), self.vector_dim), dtype=np.float32)
                
                # å¡«å…¥ç¼“å­˜çš„å‘é‡
                for i, vector in cached_vectors:
                    results[i] = vector
                
                # å¡«å…¥æ–°ç¼–ç çš„å‘é‡
                for idx, vector in zip(text_indices, new_vectors):
                    results[idx] = vector
                
                return results[0] if is_single_text else results
        
        # å¦‚æœä¸ä½¿ç”¨ç¼“å­˜ï¼Œç›´æ¥ç¼–ç 
        vectors = self._encode_texts(texts, batch_size, show_progress)
        
        # å¦‚æœä½¿ç”¨ç¼“å­˜ï¼Œä¿å­˜æ–°ç¼–ç çš„å‘é‡
        if self.use_cache:
            for text, vector, weight in zip(texts, vectors, memory_weights):
                # ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
                if unified_cache:
                    try:
                        unified_cache.put(text, vector, {"source": "vectorizer", "weight": weight})
                    except Exception as e:
                        logger.debug(f"ç»Ÿä¸€ç¼“å­˜å­˜å‚¨å¤±è´¥: {e}")
                
                # é™çº§åˆ°ç›´æ¥ç¼“å­˜
                if self.cache:
                    try:
                        self.cache.put(text, vector, memory_weight=weight)
                    except Exception as e:
                        logger.debug(f"ç›´æ¥ç¼“å­˜å­˜å‚¨å¤±è´¥: {e}")
        
        return vectors[0] if is_single_text else vectors
    
    def _encode_texts(self, texts: List[str], batch_size: int = 32, 
                     show_progress: bool = False) -> np.ndarray:
        """å®é™…çš„æ–‡æœ¬ç¼–ç é€»è¾‘"""
        if self.model_type == "sentence-transformers":
            return self._encode_with_sentence_transformers(texts, batch_size, show_progress)
        elif self.model_type == "openai":
            return self._encode_with_openai(texts, batch_size)
        elif self.model_type == "custom":
            return self._encode_with_custom_model(texts, batch_size)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
    
    def _encode_with_sentence_transformers(self, texts: List[str], 
                                          batch_size: int = 32, 
                                          show_progress: bool = False) -> np.ndarray:
        """ä½¿ç”¨sentence-transformersç¼–ç æ–‡æœ¬"""
        try:
            # ä½¿ç”¨æ¨¡å‹ç¼–ç 
            start_time = time.time()
            embeddings = self.model.encode(
                texts, 
                batch_size=batch_size, 
                show_progress_bar=show_progress,
                convert_to_numpy=True
            )
            encode_time = time.time() - start_time
            
            logger.debug(f"ä½¿ç”¨sentence-transformersç¼–ç  {len(texts)} ä¸ªæ–‡æœ¬ï¼Œè€—æ—¶: {encode_time:.2f}ç§’")
            return embeddings
            
        except Exception as e:
            logger.error(f"sentence-transformersç¼–ç å¤±è´¥: {e}")
            raise
    
    def _encode_with_openai(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """ä½¿ç”¨OpenAI APIç¼–ç æ–‡æœ¬"""
        try:
            all_embeddings = []
            
            # æ‰¹é‡å¤„ç†
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i+batch_size]
                
                start_time = time.time()
                response = self.model.Embedding.create(
                    input=batch,
                    model=self.model_name
                )
                batch_time = time.time() - start_time
                
                # æå–åµŒå…¥å‘é‡
                batch_embeddings = [item["embedding"] for item in response["data"]]
                all_embeddings.extend(batch_embeddings)
                
                logger.debug(f"OpenAIæ‰¹å¤„ç† {len(batch)} ä¸ªæ–‡æœ¬ï¼Œè€—æ—¶: {batch_time:.2f}ç§’")
                
                # APIé€Ÿç‡é™åˆ¶ï¼Œé¿å…è§¦å‘é™åˆ¶
                if i + batch_size < len(texts):
                    time.sleep(0.5)
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            embeddings = np.array(all_embeddings, dtype=np.float32)
            return embeddings
            
        except Exception as e:
            logger.error(f"OpenAIç¼–ç å¤±è´¥: {e}")
            raise
    
    def _encode_with_custom_model(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹ç¼–ç æ–‡æœ¬"""
        # è¿™é‡Œå®ç°è‡ªå®šä¹‰æ¨¡å‹çš„ç¼–ç é€»è¾‘
        logger.warning("è‡ªå®šä¹‰æ¨¡å‹ç¼–ç æœªå®ç°")
        raise NotImplementedError("è‡ªå®šä¹‰æ¨¡å‹ç¼–ç æœªå®ç°")
    
    def get_vector_dimension(self) -> int:
        """
        è·å–å‘é‡ç»´åº¦
        
        è¿”å›:
            int: å‘é‡ç»´åº¦
        """
        return self.vector_dim
    
    def compute_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        
        å‚æ•°:
            vec1: ç¬¬ä¸€ä¸ªå‘é‡
            vec2: ç¬¬äºŒä¸ªå‘é‡
            
        è¿”å›:
            float: ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        # ç¡®ä¿å‘é‡æ˜¯ä¸€ç»´çš„
        if vec1.ndim > 1:
            vec1 = vec1.flatten()
        if vec2.ndim > 1:
            vec2 = vec2.flatten()
            
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        return dot_product / (norm1 * norm2)
    
    def batch_compute_similarity(self, query_vec: np.ndarray, 
                                vectors: np.ndarray) -> np.ndarray:
        """
        æ‰¹é‡è®¡ç®—æŸ¥è¯¢å‘é‡ä¸å¤šä¸ªå‘é‡çš„ç›¸ä¼¼åº¦
        
        å‚æ•°:
            query_vec: æŸ¥è¯¢å‘é‡ï¼Œå½¢çŠ¶ä¸º (vector_dim,)
            vectors: å‘é‡æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (n, vector_dim)
            
        è¿”å›:
            np.ndarray: ç›¸ä¼¼åº¦åˆ†æ•°æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (n,)
        """
        # ç¡®ä¿æŸ¥è¯¢å‘é‡æ˜¯ä¸€ç»´çš„
        if query_vec.ndim > 1:
            query_vec = query_vec.flatten()
            
        # è®¡ç®—ç‚¹ç§¯
        dot_products = np.dot(vectors, query_vec)
        
        # è®¡ç®—èŒƒæ•°
        query_norm = np.linalg.norm(query_vec)
        vector_norms = np.linalg.norm(vectors, axis=1)
        
        # é¿å…é™¤é›¶
        mask = vector_norms != 0
        similarities = np.zeros_like(vector_norms)
        similarities[mask] = dot_products[mask] / (vector_norms[mask] * query_norm)
        
        return similarities
    
    def search_cached_memories(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """
        åœ¨ç¼“å­˜ä¸­æœç´¢ç›¸å…³è®°å¿†ï¼Œåˆ©ç”¨å…³é”®è¯ç¼“å­˜åŠ é€Ÿ
        
        å‚æ•°:
            query: æœç´¢æŸ¥è¯¢
            limit: æœ€å¤§è¿”å›æ¡æ•°
            
        è¿”å›:
            List[Dict]: åŒ¹é…çš„è®°å¿†åˆ—è¡¨
        """
        # å°è¯•ä½¿ç”¨ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
        unified_cache = None
        try:
            from ...shared.caching.cache_manager import UnifiedCacheManager
            unified_cache = UnifiedCacheManager.get_instance()
        except Exception as e:
            logger.debug(f"ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨ä¸å¯ç”¨: {e}")
        
        if unified_cache:
            try:
                # ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨ç›®å‰ä¸æ”¯æŒå†…å®¹æœç´¢ï¼Œé™çº§åˆ°ç›´æ¥ç¼“å­˜
                if self.use_cache and self.cache:
                    return self.cache.search_by_content(query, limit)
            except Exception as e:
                logger.debug(f"ç»Ÿä¸€ç¼“å­˜æœç´¢å¤±è´¥: {e}")
        elif self.use_cache and self.cache:
            return self.cache.search_by_content(query, limit)
        
        logger.warning("ç¼“å­˜æœªå¯ç”¨æˆ–ä¸å¯ç”¨ï¼Œæ— æ³•æœç´¢ç¼“å­˜è®°å¿†")
        return []
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        # å°è¯•ä½¿ç”¨ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
        unified_cache = None
        try:
            from ...shared.caching.cache_manager import UnifiedCacheManager
            unified_cache = UnifiedCacheManager.get_instance()
        except Exception as e:
            logger.debug(f"ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨ä¸å¯ç”¨: {e}")
        
        if unified_cache:
            try:
                stats = unified_cache.get_stats()
                stats["cache_enabled"] = True
                stats["cache_type"] = "unified"
                return stats
            except Exception as e:
                logger.debug(f"ç»Ÿä¸€ç¼“å­˜ç»Ÿè®¡è·å–å¤±è´¥: {e}")
        
        if self.use_cache and self.cache:
            stats = self.cache.get_stats()
            stats["cache_enabled"] = True
            stats["cache_type"] = "direct"
            return stats
        
        return {"cache_enabled": False, "cache_type": "none"}
    
    def clear_cache(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        # å°è¯•ä½¿ç”¨ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨
        unified_cache = None
        try:
            from ...shared.caching.cache_manager import UnifiedCacheManager
            unified_cache = UnifiedCacheManager.get_instance()
        except Exception as e:
            logger.debug(f"ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨ä¸å¯ç”¨: {e}")
        
        if unified_cache:
            try:
                unified_cache.clear_all()
                logger.info("ç»Ÿä¸€ç¼“å­˜å·²æ¸…ç©º")
            except Exception as e:
                logger.debug(f"ç»Ÿä¸€ç¼“å­˜æ¸…ç©ºå¤±è´¥: {e}")
        
        if self.use_cache and self.cache:
            self.cache.clear_all_cache()
            logger.info("ç›´æ¥ç¼“å­˜å·²æ¸…ç©º")

# æ¨¡å—æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("æµ‹è¯•æ–‡æœ¬å‘é‡åŒ–æ¨¡å—...")
    
    # å°è¯•åŠ è½½sentence-transformersæ¨¡å‹
    try:
        print("\n1. æµ‹è¯•sentence-transformersæ¨¡å‹")
        vectorizer = TextVectorizer(
            model_type="sentence-transformers",
            model_name="paraphrase-multilingual-MiniLM-L12-v2",
            use_cache=True
        )
        
        # æµ‹è¯•æ–‡æœ¬
        test_texts = [
            "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯å‘é‡åŒ–åŠŸèƒ½",
            "This is a test text for verifying vectorization functionality",
            "è¿™æ˜¯å¦ä¸€ä¸ªç›¸ä¼¼çš„æµ‹è¯•æ–‡æœ¬"
        ]
        
        # ç¼–ç æ–‡æœ¬
        print("ç¼–ç æ–‡æœ¬...")
        vectors = vectorizer.encode(test_texts, show_progress=True)
        
        print(f"å‘é‡å½¢çŠ¶: {vectors.shape}")
        print(f"å‘é‡ç»´åº¦: {vectorizer.get_vector_dimension()}")
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        print("\nè®¡ç®—ç›¸ä¼¼åº¦:")
        for i in range(len(test_texts)):
            for j in range(i+1, len(test_texts)):
                sim = vectorizer.compute_similarity(vectors[i], vectors[j])
                print(f"æ–‡æœ¬ {i+1} å’Œæ–‡æœ¬ {j+1} çš„ç›¸ä¼¼åº¦: {sim:.4f}")
        
        # æµ‹è¯•æ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—
        print("\næ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦:")
        query_vec = vectors[0]
        similarities = vectorizer.batch_compute_similarity(query_vec, vectors)
        for i, sim in enumerate(similarities):
            print(f"æŸ¥è¯¢ä¸æ–‡æœ¬ {i+1} çš„ç›¸ä¼¼åº¦: {sim:.4f}")
        
        # æµ‹è¯•ç¼“å­˜
        print("\næµ‹è¯•ç¼“å­˜:")
        start_time = time.time()
        cached_vectors = vectorizer.encode(test_texts)
        cache_time = time.time() - start_time
        print(f"ä»ç¼“å­˜è·å–å‘é‡è€—æ—¶: {cache_time:.6f}ç§’")
        
        # éªŒè¯å‘é‡ä¸€è‡´æ€§
        is_equal = np.array_equal(vectors, cached_vectors)
        print(f"å‘é‡ä¸€è‡´æ€§: {is_equal}")
        
    except ImportError:
        print("æœªå®‰è£…sentence-transformersï¼Œè·³è¿‡æµ‹è¯•")
    except Exception as e:
        print(f"æµ‹è¯•sentence-transformerså¤±è´¥: {e}")
    
    # å¦‚æœæœ‰OpenAI APIå¯†é’¥ï¼Œæµ‹è¯•OpenAIæ¨¡å‹
    if "OPENAI_API_KEY" in os.environ:
        try:
            print("\n2. æµ‹è¯•OpenAIæ¨¡å‹")
            openai_vectorizer = TextVectorizer(
                model_type="openai",
                model_name="text-embedding-ada-002",
                use_cache=True
            )
            
            # æµ‹è¯•å•ä¸ªæ–‡æœ¬
            test_text = "è¿™æ˜¯ä¸€ä¸ªç®€çŸ­çš„æµ‹è¯•æ–‡æœ¬"
            
            print("ç¼–ç æ–‡æœ¬...")
            vector = openai_vectorizer.encode(test_text)
            
            print(f"å‘é‡å½¢çŠ¶: {vector.shape}")
            print(f"å‘é‡ç»´åº¦: {openai_vectorizer.get_vector_dimension()}")
            
        except ImportError:
            print("æœªå®‰è£…openaiåº“ï¼Œè·³è¿‡æµ‹è¯•")
        except Exception as e:
            print(f"æµ‹è¯•OpenAIæ¨¡å‹å¤±è´¥: {e}")
    else:
        print("\næœªè®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡ï¼Œè·³è¿‡OpenAIæ¨¡å‹æµ‹è¯•")
    
    print("\næµ‹è¯•å®Œæˆ")
