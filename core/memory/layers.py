"""
è®°å¿†å±‚çº§æ¨¡å— - å®ç°å¤šå±‚è®°å¿†ç³»ç»Ÿ
åŒ…æ‹¬æ ¸å¿ƒè®°å¿†ã€å½’æ¡£è®°å¿†ã€é•¿æœŸè®°å¿†å’ŒçŸ­æœŸè®°å¿†
"""

import json
import os
import time
import hashlib
from collections import deque
from config import settings

# é…ç½®å‚æ•°ï¼Œåº”ä»settingsä¸­è¯»å–æˆ–è®¾ç½®åˆç†é»˜è®¤å€¼
CORE_MEMORY_PATH = os.path.join(settings.LOG_DIR, "memory/core_memory.json")
ARCHIVAL_MEMORY_PATH = os.path.join(settings.LOG_DIR, "memory/archival_memory.json")
SHORT_TERM_MEMORY_SIZE = 50  # çŸ­æœŸè®°å¿†å®¹é‡

class BaseMemory:
    """è®°å¿†åŸºç±»ï¼Œæä¾›é€šç”¨æ–¹æ³•"""
    
    def _get_key(self, content):
        """ç”Ÿæˆè®°å¿†å”¯ä¸€é”®å€¼"""
        text = content.get("content", "")
        role = content.get("role", "unknown")
        timestamp = content.get("timestamp", str(time.time()))
        return hashlib.md5(f"{role}:{text}:{timestamp}".encode()).hexdigest()
    
    def _similar_to(self, query, text, threshold=0.5):
        """ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦æ£€æŸ¥ï¼Œå¯æ›¿æ¢ä¸ºå‘é‡ç›¸ä¼¼åº¦"""
        # è¿™é‡Œä»…ä½œç¤ºä¾‹ï¼Œå®é™…åº”è¯¥ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦
        query_words = set(query.lower().split())
        text_words = set(text.lower().split())
        if not query_words or not text_words:
            return False
        intersection = query_words.intersection(text_words)
        return len(intersection) / max(len(query_words), 1) >= threshold

class CoreMemory(BaseMemory):
    """æ ¸å¿ƒè®°å¿† - å­˜å‚¨æœ€é‡è¦çš„ä¿¡æ¯ï¼Œæ°¸ä¸é—å¿˜"""
    
    def __init__(self):
        self.memory = {}
        self._ensure_dir()
        self._load()
    
    def _ensure_dir(self):
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        os.makedirs(os.path.dirname(CORE_MEMORY_PATH), exist_ok=True)
    
    def _load(self):
        """åŠ è½½è®°å¿†"""
        try:
            with open(CORE_MEMORY_PATH, "r", encoding="utf-8") as f:
                self.memory = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            self.memory = {}
    
    def _save(self):
        """ä¿å­˜è®°å¿†"""
        with open(CORE_MEMORY_PATH, "w", encoding="utf-8") as f:
            json.dump(self.memory, f, ensure_ascii=False, indent=2)
    
    def add(self, content):
        """æ·»åŠ æ ¸å¿ƒè®°å¿†"""
        key = self._get_key(content)
        self.memory[key] = {
            **content,
            "level": "core",
            "weight": 10.0,  # æ ¸å¿ƒè®°å¿†æƒé‡å›ºå®šä¸ºæœ€é«˜
            "last_accessed": time.time()
        }
        self._save()
        return key
    
    def retrieve(self, query, limit=5):
        """æ£€ç´¢æ ¸å¿ƒè®°å¿†"""
        results = []
        for key, item in self.memory.items():
            content = item.get("content", "")
            if query.lower() in content.lower() or self._similar_to(query, content):
                results.append(item)
                # æ›´æ–°è®¿é—®æ—¶é—´
                self.memory[key]["last_accessed"] = time.time()
        
        # æŒ‰æƒé‡æ’åºå¹¶é™åˆ¶ç»“æœæ•°é‡
        results = sorted(results, key=lambda x: x.get("weight", 0), reverse=True)[:limit]
        if results:
            self._save()  # ä¿å­˜æ›´æ–°çš„è®¿é—®æ—¶é—´
        return results

class ArchivalMemory(BaseMemory):
    """å½’æ¡£è®°å¿† - å­˜å‚¨é‡è¦äº‹ä»¶å’ŒèƒŒæ™¯ä¿¡æ¯"""
    
    def __init__(self):
        self.memory = {}
        self._ensure_dir()
        self._load()
    
    def _ensure_dir(self):
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        os.makedirs(os.path.dirname(ARCHIVAL_MEMORY_PATH), exist_ok=True)
    
    def _load(self):
        """åŠ è½½è®°å¿†"""
        try:
            with open(ARCHIVAL_MEMORY_PATH, "r", encoding="utf-8") as f:
                self.memory = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            self.memory = {}
    
    def _save(self):
        """ä¿å­˜è®°å¿†"""
        with open(ARCHIVAL_MEMORY_PATH, "w", encoding="utf-8") as f:
            json.dump(self.memory, f, ensure_ascii=False, indent=2)
    
    def add(self, content):
        """æ·»åŠ å½’æ¡£è®°å¿†"""
        key = self._get_key(content)
        self.memory[key] = {
            **content,
            "level": "archival",
            "weight": 8.0,  # å½’æ¡£è®°å¿†åˆå§‹æƒé‡è¾ƒé«˜
            "last_accessed": time.time()
        }
        self._save()
        return key
    
    def retrieve(self, query, limit=5):
        """æ£€ç´¢å½’æ¡£è®°å¿†"""
        results = []
        for key, item in self.memory.items():
            content = item.get("content", "")
            if query.lower() in content.lower() or self._similar_to(query, content):
                results.append(item)
                # æ›´æ–°è®¿é—®æ—¶é—´
                self.memory[key]["last_accessed"] = time.time()
        
        # æŒ‰æƒé‡æ’åºå¹¶é™åˆ¶ç»“æœæ•°é‡
        results = sorted(results, key=lambda x: x.get("weight", 0), reverse=True)[:limit]
        if results:
            self._save()  # ä¿å­˜æ›´æ–°çš„è®¿é—®æ—¶é—´
        return results

class LongTermMemory(BaseMemory):
    """é•¿æœŸè®°å¿† - ä½¿ç”¨å‘é‡æ•°æ®åº“å­˜å‚¨"""
    
    def __init__(self, vector_store=None):
        self.vector_store = vector_store  # è¿™é‡Œåº”è¯¥ä¼ å…¥å®é™…çš„å‘é‡å­˜å‚¨å¯¹è±¡
        self.memory = {}  # æœ¬åœ°ç¼“å­˜ï¼Œç”¨äºå­˜å‚¨å…ƒæ•°æ®
        self._load_metadata()
    
    def _load_metadata(self):
        """åŠ è½½è®°å¿†å…ƒæ•°æ®"""
        import os
        from config import settings
        
        metadata_path = os.path.join(settings.LOG_DIR, "memory/long_term_metadata.json")
        os.makedirs(os.path.dirname(metadata_path), exist_ok=True)
        
        try:
            with open(metadata_path, "r", encoding="utf-8") as f:
                import json
                self.memory = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            self.memory = {}
    
    def _save_metadata(self):
        """ä¿å­˜è®°å¿†å…ƒæ•°æ®"""
        import os
        import json
        from config import settings
        
        metadata_path = os.path.join(settings.LOG_DIR, "memory/long_term_metadata.json")
        os.makedirs(os.path.dirname(metadata_path), exist_ok=True)
        
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(self.memory, f, ensure_ascii=False, indent=2)
    
    def add(self, content):
        """æ·»åŠ åˆ°é•¿æœŸè®°å¿†"""
        key = self._get_key(content)
        
        # æ›´æ–°å…ƒæ•°æ®
        self.memory[key] = {
            **content,
            "level": "long_term",
            "last_accessed": time.time(),
            "access_count": 0
        }
        self._save_metadata()
        
        # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
        if self.vector_store and "content" in content:
            try:
                # å¦‚æœå‘é‡å­˜å‚¨æ”¯æŒadd_textsæ–¹æ³•
                if hasattr(self.vector_store, 'add_texts'):
                    metadata = {
                        "role": content.get("role", "unknown"),
                        "timestamp": content.get("timestamp", str(time.time())),
                        "key": key,
                        "level": "long_term",
                        "weight": content.get("weight", 5.0)
                    }
                    self.vector_store.add_texts(
                        [content["content"]], 
                        metadatas=[metadata]
                    )
                # å¦‚æœå‘é‡å­˜å‚¨æ”¯æŒè‡ªå®šä¹‰çš„addæ–¹æ³•
                elif hasattr(self.vector_store, 'add'):
                    self.vector_store.add(content["content"], key)
            except Exception as e:
                print(f"âš ï¸ å‘é‡å­˜å‚¨æ·»åŠ å¤±è´¥: {e}")
        
        return key
    
    def retrieve(self, query, limit=5):
        """ä»é•¿æœŸè®°å¿†æ£€ç´¢"""
        results = []
        
        # ä»å‘é‡å­˜å‚¨ä¸­æ£€ç´¢
        if self.vector_store:
            try:
                # å¦‚æœå‘é‡å­˜å‚¨æ”¯æŒsimilarity_searchæ–¹æ³•
                if hasattr(self.vector_store, 'similarity_search'):
                    docs = self.vector_store.similarity_search(query, k=limit)
                    for doc in docs:
                        content = doc.page_content
                        metadata = doc.metadata
                        key = metadata.get("key", self._get_key({"content": content}))
                        
                        # æ›´æ–°å…ƒæ•°æ®
                        if key in self.memory:
                            self.memory[key]["last_accessed"] = time.time()
                            self.memory[key]["access_count"] = self.memory[key].get("access_count", 0) + 1
                        
                        results.append({
                            "content": content,
                            "role": metadata.get("role", "unknown"),
                            "timestamp": metadata.get("timestamp", ""),
                            "level": "long_term",
                            "weight": metadata.get("weight", 5.0),
                            "key": key,
                            "last_accessed": time.time()
                        })
                
                # å¦‚æœå‘é‡å­˜å‚¨æ”¯æŒè‡ªå®šä¹‰çš„searchæ–¹æ³•
                elif hasattr(self.vector_store, 'search'):
                    items = self.vector_store.search(query, limit)
                    for item in items:
                        # å‡è®¾itemæ˜¯(content, key, score)å½¢å¼çš„å…ƒç»„
                        if isinstance(item, tuple) and len(item) >= 2:
                            content, key = item[0], item[1]
                            
                            # æ›´æ–°å…ƒæ•°æ®
                            if key in self.memory:
                                self.memory[key]["last_accessed"] = time.time()
                                self.memory[key]["access_count"] = self.memory[key].get("access_count", 0) + 1
                                
                                results.append(self.memory[key])
            
            except Exception as e:
                print(f"âš ï¸ å‘é‡å­˜å‚¨æ£€ç´¢å¤±è´¥: {e}")
        
        # ä¿å­˜æ›´æ–°çš„å…ƒæ•°æ®
        if results:
            self._save_metadata()
        
        return results

class ShortTermMemory(BaseMemory):
    """çŸ­æœŸè®°å¿† - åŸºäºé˜Ÿåˆ—çš„ä¸´æ—¶å­˜å‚¨"""
    
    def __init__(self):
        self.memory = deque(maxlen=SHORT_TERM_MEMORY_SIZE)
    
    def add(self, content):
        """æ·»åŠ åˆ°çŸ­æœŸè®°å¿†"""
        key = self._get_key(content)
        item = {
            **content,
            "key": key,
            "level": "short_term",
            "weight": 3.0,  # çŸ­æœŸè®°å¿†åˆå§‹æƒé‡è¾ƒä½
            "timestamp": content.get("timestamp", time.time())
        }
        self.memory.append(item)
        return key
    
    def retrieve(self, query, limit=5):
        """ä»çŸ­æœŸè®°å¿†æ£€ç´¢"""
        results = []
        for item in self.memory:
            content = item.get("content", "")
            if query.lower() in content.lower() or self._similar_to(query, content):
                results.append(item)
        
        # æŒ‰æƒé‡å’Œæ—¶é—´æ’åº
        results = sorted(
            results, 
            key=lambda x: (x.get("weight", 0), x.get("timestamp", 0)), 
            reverse=True
        )[:limit]
        
        return results

class MemoryManager:
    """è®°å¿†ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰è®°å¿†å±‚"""
    
    def __init__(self, vector_store=None):
        # åˆå§‹åŒ–è®°å¿†å±‚
        self.core = CoreMemory()
        self.archival = ArchivalMemory()
        self.long_term = LongTermMemory(vector_store)
        self.short_term = ShortTermMemory()
        self.last_consolidation = time.time()
        self.last_decay = time.time()
        
        # è®°å¿†ç®¡ç†é…ç½®
        self.consolidation_interval = 3600 * 24  # é»˜è®¤æ¯24å°æ—¶å·©å›ºä¸€æ¬¡
        self.decay_interval = 3600 * 24 * 7      # é»˜è®¤æ¯7å¤©è¡°å‡ä¸€æ¬¡
        self.similarity_threshold = 0.75         # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œç”¨äºå»é‡
        
        # é«˜çº§è®°å¿†åŠŸèƒ½
        from .memory_association import MemoryAssociationNetwork
        from .memory_conflict import MemoryConflictDetector
        from .memory_summarizer import MemorySummarizer
        
        # åˆå§‹åŒ–é«˜çº§åŠŸèƒ½æ¨¡å—
        self.association_network = MemoryAssociationNetwork()
        self.conflict_detector = MemoryConflictDetector(self, self.association_network)
        self.summarizer = MemorySummarizer(self, self.association_network)
    
    def add_memory(self, content, level=None):
        """æ·»åŠ è®°å¿†åˆ°æŒ‡å®šå±‚æˆ–è‡ªåŠ¨åˆ†å±‚"""
        # æ£€æŸ¥æ–°è®°å¿†æ˜¯å¦ä¸ç°æœ‰è®°å¿†å†²çª
        conflicts = self.conflict_detector.detect_conflicts_for_new_memory(content)
        if conflicts:
            print(f"ğŸ“¢ æ£€æµ‹åˆ° {len(conflicts)} ä¸ªè®°å¿†å†²çªå¹¶è‡ªåŠ¨è§£å†³")
        
        # 1. æ·»åŠ è®°å¿†åˆ°ç›¸åº”å±‚çº§
        if level == "core":
            memory_key = self.core.add(content)
        elif level == "archival":
            memory_key = self.archival.add(content)
        elif level == "long_term":
            memory_key = self.long_term.add(content)
        elif level == "short_term":
            memory_key = self.short_term.add(content)
        else:
            # è‡ªåŠ¨åˆ†å±‚é€»è¾‘
            weight = content.get("weight", 0)
            if weight >= 9.0:
                memory_key = self.core.add(content)
                level = "core"
            elif weight >= 7.0:
                memory_key = self.archival.add(content)
                level = "archival"
            elif weight >= 5.0:
                memory_key = self.long_term.add(content)
                level = "long_term"
            else:
                memory_key = self.short_term.add(content)
                level = "short_term"
        
        # 2. è‡ªåŠ¨å»ºç«‹å…³è”
        if memory_key and hasattr(content, 'get') and content.get('content'):
            memory_with_key = dict(content)
            memory_with_key['key'] = memory_key
            self.association_network.auto_associate_memory(memory_with_key, self, self.long_term.vector_store)
        
        # 3. å¦‚æœæ˜¯é‡è¦è®°å¿†ï¼Œæå‰å°è¯•æ€»ç»“
        if level in ["core", "archival"] or (level == "long_term" and content.get("weight", 0) >= 6.0):
            # åªå¯¹é‡è¦è®°å¿†è§¦å‘å³æ—¶æ€»ç»“ï¼Œæé«˜ç³»ç»Ÿå“åº”æ€§
            if hasattr(content, 'get') and content.get('content'):
                # æŸ¥æ‰¾ç›¸å…³è®°å¿†
                related_keys = self.association_network.get_related_memories(memory_key, depth=1)
                if len(related_keys) >= 2:  # è‡³å°‘éœ€è¦3æ¡è®°å¿†æ‰æ€»ç»“(æ–°è®°å¿†+è‡³å°‘2æ¡ç›¸å…³)
                    related_memories = []
                    for key in related_keys:
                        mem = self._get_memory_by_key(key)
                        if mem:
                            related_memories.append(mem)
                    
                    # å¦‚æœæœ‰è¶³å¤Ÿçš„ç›¸å…³è®°å¿†ï¼Œç”Ÿæˆæ€»ç»“
                    if len(related_memories) >= 2:
                        all_memories = [memory_with_key] + related_memories
                        self.summarizer.generate_memory_summary(all_memories)
        
        return memory_key
    
    def _get_memory_by_key(self, key):
        """æ ¹æ®é”®è·å–è®°å¿†ï¼Œåœ¨æ‰€æœ‰å±‚çº§ä¸­æŸ¥æ‰¾"""
        # æ£€æŸ¥æ ¸å¿ƒè®°å¿†
        if hasattr(self.core, 'memory') and key in self.core.memory:
            return self.core.memory[key]
            
        # æ£€æŸ¥å½’æ¡£è®°å¿†
        if hasattr(self.archival, 'memory') and key in self.archival.memory:
            return self.archival.memory[key]
            
        # æ£€æŸ¥é•¿æœŸè®°å¿†
        if hasattr(self.long_term, 'memory') and key in self.long_term.memory:
            return self.long_term.memory[key]
            
        # æ£€æŸ¥çŸ­æœŸè®°å¿†
        if hasattr(self.short_term, 'memory'):
            for mem in self.short_term.memory:
                if mem.get("key") == key:
                    return mem
        
        return None
    
    def retrieve_memory(self, query, limit=5, parallel=True, include_associations=True, check_conflicts=True):
        """ä»æ‰€æœ‰è®°å¿†å±‚æ£€ç´¢å¹¶åˆå¹¶ç»“æœï¼Œæ”¯æŒå…³è”å’Œå†²çªæ£€æµ‹"""
        # 1. åŸºæœ¬æ£€ç´¢
        if parallel:
            raw_results = self._retrieve_memory_parallel(query, limit)
        else:
            raw_results = self._retrieve_memory_sequential(query, limit)
        
        # 2. åº”ç”¨å†²çªæ£€æµ‹
        if check_conflicts:
            results = self.conflict_detector.get_conflict_aware_memories(raw_results)
        else:
            results = raw_results
        
        # 3. æ·»åŠ å…³è”è®°å¿†
        if include_associations and results:
            # ä»æ£€ç´¢ç»“æœä¸­æ‰¾å‡ºæƒé‡æœ€é«˜çš„è®°å¿†
            primary_result = max(results, key=lambda x: x.get("weight", 0))
            if "key" in primary_result:
                # è·å–å…³è”è®°å¿†
                related_keys = self.association_network.get_related_memories(
                    primary_result["key"], 
                    depth=1, 
                    min_strength=0.5
                )
                
                # æ£€ç´¢å…³è”è®°å¿†å†…å®¹
                related_memories = []
                for key in related_keys:
                    memory = self._get_memory_by_key(key)
                    if memory and memory not in results:
                        # æ ‡è®°ä¸ºå…³è”è®°å¿†
                        memory["is_associated"] = True
                        memory["association_source"] = primary_result.get("key", "")
                        related_memories.append(memory)
                
                # æŒ‰æƒé‡æ’åºå…³è”è®°å¿†
                related_memories.sort(key=lambda x: x.get("weight", 0), reverse=True)
                
                # æ·»åŠ æœ€ç›¸å…³çš„è®°å¿†ï¼ˆæœ€å¤šlimit/2ä¸ªï¼‰
                addition_limit = max(1, limit // 2)
                results.extend(related_memories[:addition_limit])
                
                # 4. æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³æ‘˜è¦
                if hasattr(self.summarizer, 'get_summary_for_entity'):
                    # å°è¯•ä»ä¸»è¦ç»“æœå†…å®¹ä¸­æå–å®ä½“
                    entities = self.conflict_detector.extract_entities(primary_result.get("content", ""))
                    for entity in entities:
                        entity_value = entity.get("value", "")
                        if entity_value:
                            summaries = self.summarizer.get_summary_for_entity(entity_value, limit=1)
                            for summary in summaries:
                                if summary not in results:
                                    summary["is_summary"] = True
                                    results.append(summary)
                                    break  # åªæ·»åŠ ä¸€ä¸ªæœ€ç›¸å…³çš„æ‘˜è¦
        
        # æœ€ç»ˆæ’åºå’Œé™åˆ¶ç»“æœæ•°
        final_results = sorted(results, key=lambda x: x.get("weight", 0), reverse=True)
        return final_results[:limit]
    
    def _retrieve_memory_parallel(self, query, limit=5):
        """å¹¶è¡Œä»æ‰€æœ‰è®°å¿†å±‚æ£€ç´¢"""
        import threading
        import queue
        
        results_queue = queue.Queue()
        
        # å®šä¹‰çº¿ç¨‹å‡½æ•°
        def retrieve_from_layer(layer_name, retriever, query, limit):
            try:
                results = retriever.retrieve(query, limit)
                # ç»™ç»“æœæ·»åŠ æ¥æºæ ‡è®°
                for item in results:
                    if "level" not in item:
                        item["level"] = layer_name
                results_queue.put(results)
            except Exception as e:
                print(f"âš ï¸ ä»{layer_name}æ£€ç´¢æ—¶å‡ºé”™: {e}")
                results_queue.put([])
        
        # åˆ›å»ºå¹¶å¯åŠ¨æ‰€æœ‰æ£€ç´¢çº¿ç¨‹
        threads = []
        layers = [
            ("core", self.core),
            ("archival", self.archival), 
            ("long_term", self.long_term),
            ("short_term", self.short_term)
        ]
        
        for layer_name, layer in layers:
            thread = threading.Thread(
                target=retrieve_from_layer,
                args=(layer_name, layer, query, limit)
            )
            thread.start()
            threads.append(thread)
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        # æ”¶é›†æ‰€æœ‰ç»“æœ
        all_results = []
        while not results_queue.empty():
            layer_results = results_queue.get()
            all_results.extend(layer_results)
        
        # å»é‡ã€æ’åºå¹¶é™åˆ¶ç»“æœæ•°é‡
        return self._process_retrieved_results(all_results, query, limit)
    
    def _retrieve_memory_sequential(self, query, limit=5):
        """æŒ‰é¡ºåºä»æ‰€æœ‰è®°å¿†å±‚æ£€ç´¢"""
        # ä»å„å±‚è·å–ç»“æœ
        core_results = self.core.retrieve(query, limit)
        archival_results = self.archival.retrieve(query, limit)
        long_term_results = self.long_term.retrieve(query, limit)
        short_term_results = self.short_term.retrieve(query, limit)
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_results = core_results + archival_results + long_term_results + short_term_results
        
        # å»é‡ã€æ’åºå¹¶é™åˆ¶ç»“æœæ•°é‡
        return self._process_retrieved_results(all_results, query, limit)
    
    def _process_retrieved_results(self, all_results, query, limit=5):
        """å¤„ç†æ£€ç´¢ç»“æœï¼šå»é‡ã€æ’åºå’Œç­›é€‰"""
        # 1. å»é‡ï¼ˆåŸºäºå†…å®¹ï¼‰
        unique_results = {}
        for item in all_results:
            content = item.get("content", "")
            # å¦‚æœå†…å®¹å·²å­˜åœ¨ä¸”æ–°çš„æƒé‡æ›´é«˜ï¼Œæ›¿æ¢
            if content not in unique_results or item.get("weight", 0) > unique_results[content].get("weight", 0):
                unique_results[content] = item
                
                # è®°å½•è®¿é—®æ¬¡æ•°
                item["access_count"] = item.get("access_count", 0) + 1
                
                # æ›´æ–°æœ€åè®¿é—®æ—¶é—´
                item["last_accessed"] = time.time()
        
        results_list = list(unique_results.values())
        
        # 2. åŸºäºå¤šç§å› ç´ ä¸ºç»“æœè¯„åˆ†
        scored_results = []
        for item in results_list:
            # åˆå§‹åˆ†æ•°å°±æ˜¯è®°å¿†æƒé‡
            score = item.get("weight", 1.0)
            
            # å†…å®¹ç›¸å…³æ€§åŠ åˆ†
            content = item.get("content", "").lower()
            query_terms = query.lower().split()
            
            # è®¡ç®—æŸ¥è¯¢è¯åœ¨å†…å®¹ä¸­å‡ºç°çš„æ¬¡æ•°
            term_matches = sum(1 for term in query_terms if term in content)
            relevance_score = term_matches / max(1, len(query_terms))
            score += relevance_score * 2  # ç›¸å…³æ€§æƒé‡
            
            # é•¿åº¦é€‚ä¸­çš„å†…å®¹åŠ åˆ†
            content_length = len(content)
            if 20 <= content_length <= 200:
                score += 0.5  # é€‚ä¸­é•¿åº¦åŠ åˆ†
            
            # è®°å¿†å±‚çº§åŠ æƒ
            level_weights = {
                "core": 3.0,
                "archival": 2.0,
                "long_term": 1.0,
                "short_term": 0.5
            }
            level = item.get("level", "short_term")
            score += level_weights.get(level, 0)
            
            # æœ€è¿‘è®¿é—®åŠ åˆ†ï¼ˆæ—¶é—´è¡°å‡ï¼‰
            last_accessed = item.get("last_accessed", 0)
            days_since_access = (time.time() - last_accessed) / (3600 * 24)
            recency_score = 1.0 / (1 + days_since_access)
            score += recency_score
            
            # æ·»åŠ åˆ°è¯„åˆ†ç»“æœ
            scored_results.append((score, item))
        
        # 3. æŒ‰æœ€ç»ˆè¯„åˆ†æ’åº
        sorted_results = [item for _, item in sorted(scored_results, key=lambda x: x[0], reverse=True)]
        
        # 4. é™åˆ¶è¿”å›ç»“æœæ•°é‡
        return sorted_results[:limit]
    
    def consolidate_memories(self):
        """æ‰§è¡Œå¤šç§è®°å¿†ç»´æŠ¤ä»»åŠ¡ï¼šå·©å›ºã€è¡°å‡ã€å…³è”ç»´æŠ¤å’Œæ€»ç»“"""
        print("ğŸ§  å¼€å§‹å…¨é¢è®°å¿†ç»´æŠ¤...")
        
        # 1. æ‰§è¡ŒåŸºç¡€çš„çŸ­æœŸâ†’é•¿æœŸè®°å¿†å·©å›º
        print("ğŸ§  å¼€å§‹è®°å¿†å·©å›ºæµç¨‹...")
        
        # æ£€æŸ¥é—´éš”æ—¶é—´
        now = time.time()
        if now - self.last_consolidation < self.consolidation_interval:
            print(f"â±ï¸ è·ç¦»ä¸Šæ¬¡å·©å›ºé—´éš”ä¸è¶³ï¼Œè·³è¿‡æœ¬æ¬¡å·©å›º")
            return
            
        self.last_consolidation = now
        consolidated_count = 0
        
        # æå–çŸ­æœŸè®°å¿†ä¸­éœ€è¦å·©å›ºçš„é¡¹ç›®
        to_consolidate = []
        for item in list(self.short_term.memory):
            # æƒé‡é«˜çš„çŸ­æœŸè®°å¿†è½¬ä¸ºé•¿æœŸè®°å¿†
            if item.get("weight", 0) >= 5.0:
                to_consolidate.append(item)
                consolidated_count += 1
            
            # é¢‘ç¹è®¿é—®çš„çŸ­æœŸè®°å¿†ä¹Ÿè½¬ä¸ºé•¿æœŸè®°å¿†
            elif item.get("access_count", 0) >= 3:
                item["weight"] = max(item.get("weight", 0), 5.0)  # æå‡æƒé‡
                to_consolidate.append(item)
                consolidated_count += 1
        
        # å°†éœ€è¦å·©å›ºçš„è®°å¿†è½¬ç§»åˆ°é•¿æœŸè®°å¿†
        for item in to_consolidate:
            self.long_term.add(item)
            print(f"ğŸ“ å·©å›ºè®°å¿†: {item.get('content', '')[:30]}...")
        
        print(f"âœ… è®°å¿†å·©å›ºå®Œæˆï¼Œå…±å¤„ç† {consolidated_count} æ¡è®°å¿†")
        
        # 2. è§¦å‘è®°å¿†è¡°å‡æµç¨‹
        if now - self.last_decay >= self.decay_interval:
            self.decay_memories()
        
        # 3. ç»´æŠ¤å…³è”ç½‘ç»œ
        if hasattr(self.association_network, 'decay_associations'):
            print("ğŸ”„ ç»´æŠ¤è®°å¿†å…³è”ç½‘ç»œ...")
            decay_count, deleted_count = self.association_network.decay_associations()
            print(f"âœ… å…³è”ç½‘ç»œç»´æŠ¤å®Œæˆ: {decay_count}æ¡å…³è”è¡°å‡, {deleted_count}æ¡å…³è”åˆ é™¤")
        
        # 4. æ‰§è¡Œè®°å¿†æ€»ç»“
        if hasattr(self.summarizer, 'schedule_summarization'):
            print("ğŸ“‹ æ‰§è¡Œè®°å¿†æ€»ç»“...")
            summary_count = self.summarizer.schedule_summarization()
            print(f"âœ… è®°å¿†æ€»ç»“å®Œæˆ: ç”Ÿæˆäº†{summary_count}æ¡æ‘˜è¦è®°å¿†")
        
        print("ğŸ§  å…¨é¢è®°å¿†ç»´æŠ¤å®Œæˆ!")
    
    def decay_memories(self):
        """è®°å¿†è¡°å‡ï¼šéšæ—¶é—´é™ä½è®°å¿†æƒé‡ï¼Œå¯èƒ½å¯¼è‡´é—å¿˜"""
        print("ğŸ§  å¼€å§‹è®°å¿†è¡°å‡æµç¨‹...")
        self.last_decay = time.time()
        
        # 1. é•¿æœŸè®°å¿†è¡°å‡
        decayed_count = 0
        forgotten_count = 0
        
        # è¿™é‡Œå‡è®¾é•¿æœŸè®°å¿†æœ‰ä¸€ä¸ªå­—å…¸ç»“æ„
        if hasattr(self.long_term, 'memory') and isinstance(self.long_term.memory, dict):
            for key, item in list(self.long_term.memory.items()):
                # è®¡ç®—è¡°å‡å› å­ï¼šä¸Šæ¬¡è®¿é—®è¶Šä¹…è¿œï¼Œè¡°å‡è¶Šæ˜æ˜¾
                last_accessed = item.get("last_accessed", 0)
                days_since_access = (time.time() - last_accessed) / (3600 * 24)
                decay_factor = 0.05 * days_since_access  # æ¯å¤©è¡°å‡5%
                
                # æ ¸å¿ƒè®°å¿†å’Œæ ‡è®°ä¸ºé‡è¦çš„è®°å¿†ä¸è¡°å‡
                if item.get("level") == "core" or item.get("important", False):
                    continue
                    
                # åº”ç”¨æƒé‡è¡°å‡
                original_weight = item.get("weight", 5.0)
                new_weight = max(1.0, original_weight - decay_factor)
                
                if new_weight < 3.0:  # æƒé‡ä½äºé˜ˆå€¼ï¼Œè€ƒè™‘é—å¿˜
                    # æ£€æŸ¥æ˜¯å¦æœ‰é¢‘ç¹è®¿é—®ï¼Œé¢‘ç¹è®¿é—®çš„ä¸é—å¿˜
                    if item.get("access_count", 0) < 2:
                        del self.long_term.memory[key]
                        forgotten_count += 1
                        continue
                
                # æ›´æ–°æƒé‡
                if new_weight != original_weight:
                    self.long_term.memory[key]["weight"] = new_weight
                    decayed_count += 1
        
        print(f"âœ… è®°å¿†è¡°å‡å®Œæˆï¼Œè¡°å‡ {decayed_count} æ¡è®°å¿†ï¼Œé—å¿˜ {forgotten_count} æ¡è®°å¿†")
    
    def deduplicate_memories(self):
        """å»é™¤å†—ä½™è®°å¿†ï¼šåˆå¹¶ç›¸ä¼¼çš„è®°å¿†é¡¹"""
        print("ğŸ§  å¼€å§‹è®°å¿†å»é‡æµç¨‹...")
        
        # è¿™é‡Œéœ€è¦ä¾èµ–å‘é‡å­˜å‚¨å®ç°ï¼Œç®€å•ç¤ºä¾‹
        if not hasattr(self.long_term, 'vector_store') or not self.long_term.vector_store:
            print("âš ï¸ ç¼ºå°‘å‘é‡å­˜å‚¨ï¼Œæ— æ³•æ‰§è¡Œå»é‡")
            return
        
        # å®é™…å»é‡é€»è¾‘éœ€è¦æ ¹æ®æ‚¨çš„å‘é‡å­˜å‚¨å®ç°æ–¹å¼è°ƒæ•´
        print("âœ… è®°å¿†å»é‡å®Œæˆ")
    
    def mark_important(self, memory_key, level="long_term"):
        """å°†ç‰¹å®šè®°å¿†æ ‡è®°ä¸ºé‡è¦ï¼Œé˜²æ­¢é—å¿˜"""
        if level == "long_term" and hasattr(self.long_term, 'memory'):
            if memory_key in self.long_term.memory:
                self.long_term.memory[memory_key]["important"] = True
                self.long_term.memory[memory_key]["weight"] = 9.0  # æå‡æƒé‡
                print(f"ğŸ”’ å·²å°†è®°å¿†æ ‡è®°ä¸ºé‡è¦: {memory_key}")
                return True
        
        elif level == "archival":
            if memory_key in self.archival.memory:
                self.archival.memory[memory_key]["important"] = True
                print(f"ğŸ”’ å·²å°†å½’æ¡£è®°å¿†æ ‡è®°ä¸ºé‡è¦: {memory_key}")
                return True
        
        return False 