#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è®°å¿†ç”Ÿå‘½å‘¨æœŸç®¡ç†æ¨¡å—
è´Ÿè´£è®°å¿†çš„å½’æ¡£ã€æ¢å¤ã€æ¸…ç†ç­‰ç”Ÿå‘½å‘¨æœŸæ“ä½œ
"""

import time
import json
import logging
from typing import Dict, Any, List, Optional

# ğŸ”¥ ä½¿ç”¨ç»Ÿä¸€çš„å†…éƒ¨å·¥å…·
from .internal import MemoryLayer, handle_memory_errors, ErrorHandlerMixin, QueryBuilder

logger = logging.getLogger(__name__)

class LifecycleManager(ErrorHandlerMixin):
    """è®°å¿†ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨"""
    
    def __init__(self, db_manager, weight_manager=None):
        """
        åˆå§‹åŒ–ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨
        
        Args:
            db_manager: æ•°æ®åº“ç®¡ç†å™¨
            weight_manager: æƒé‡ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
        """
        super().__init__()
        self.db_manager = db_manager
        self.weight_manager = weight_manager
        self.query_builder = QueryBuilder()
        self.logger = logger
        
        # ç”Ÿå‘½å‘¨æœŸé…ç½®
        self.lifecycle_config = {
            'archive_threshold_days': 30,  # å½’æ¡£é˜ˆå€¼å¤©æ•°
            'cleanup_threshold_days': 90,  # æ¸…ç†é˜ˆå€¼å¤©æ•°
            'archive_weight_penalty': 0.5,  # å½’æ¡£æƒé‡æƒ©ç½š
            'restore_weight_bonus': 1.2,   # æ¢å¤æƒé‡å¥–åŠ±
            'min_weight_for_permanent': 7.0,  # æ°¸ä¹…ä¿å­˜çš„æœ€ä½æƒé‡
            'batch_size': 100  # æ‰¹å¤„ç†å¤§å°
        }
    
    @handle_memory_errors({'success': False, 'message': 'å½’æ¡£æ“ä½œå¤±è´¥'})
    def archive_old_memories(self, days_threshold: int = None, archive_weight_penalty: float = None) -> Dict[str, Any]:
        """
        å½’æ¡£è¿‡æœŸè®°å¿†ï¼ˆè½¯åˆ é™¤ï¼Œä¸ç‰©ç†åˆ é™¤ï¼‰
        
        Args:
            days_threshold: å½’æ¡£å¤©æ•°é˜ˆå€¼
            archive_weight_penalty: å½’æ¡£æƒé‡æƒ©ç½šç³»æ•°
            
        Returns:
            Dict: å½’æ¡£ç»“æœ
        """
        # ä½¿ç”¨é»˜è®¤é…ç½®
        days_threshold = days_threshold or self.lifecycle_config['archive_threshold_days']
        archive_weight_penalty = archive_weight_penalty or self.lifecycle_config['archive_weight_penalty']
        
        current_time = time.time()
        cutoff_time = current_time - (days_threshold * 24 * 3600)
        
        # ç¡®ä¿archivedå­—æ®µå­˜åœ¨
        try:
            self.db_manager.execute_query("ALTER TABLE memories ADD COLUMN archived INTEGER DEFAULT 0")
        except:
            pass  # å­—æ®µå¯èƒ½å·²å­˜åœ¨
        
        # ğŸ”¥ ä½¿ç”¨ç»Ÿä¸€çš„å±‚çº§èŒƒå›´è·å–çŸ­æœŸè®°å¿†çš„æƒé‡ä¸Šé™
        min_weight, max_weight = MemoryLayer.get_weight_range('short_term')
        
        # å½’æ¡£çŸ­æœŸè®°å¿†ä¸”è¶…è¿‡é˜ˆå€¼çš„è®°å¿†
        archive_query = """
            UPDATE memories 
            SET archived = 1,
                weight = weight * ?,
                metadata = CASE 
                    WHEN metadata IS NULL THEN ?
                    ELSE json_patch(metadata, ?)
                END
            WHERE weight < ? 
            AND timestamp < ? 
            AND archived = 0
            AND type NOT IN ('system', 'summary')
        """
        
        metadata_json = json.dumps({
            "archived_at": current_time,
            "archive_reason": "automatic_cleanup",
            "original_weight": "preserved_in_calculation"
        })
        
        result = self.db_manager.execute_query(
            archive_query, 
            (archive_weight_penalty, metadata_json, metadata_json, max_weight, cutoff_time)
        )
        
        archived_count = result.rowcount if result and hasattr(result, 'rowcount') else 0
        
        self.logger.info(f"å½’æ¡£äº† {archived_count} æ¡è¿‡æœŸçŸ­æœŸè®°å¿†")
        
        return {
            'success': True,
            'archived_count': archived_count,
            'threshold_days': days_threshold,
            'weight_penalty': archive_weight_penalty,
            'message': f'æˆåŠŸå½’æ¡£ {archived_count} æ¡è¿‡æœŸè®°å¿†'
        }
    
    def restore_archived_memories(self, memory_ids: List[str] = None, restore_weight_bonus: float = None) -> Dict[str, Any]:
        """
        æ¢å¤å½’æ¡£è®°å¿†
        
        Args:
            memory_ids: è¦æ¢å¤çš„è®°å¿†IDåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ¢å¤æ‰€æœ‰
            restore_weight_bonus: æ¢å¤æ—¶çš„æƒé‡å¥–åŠ±ç³»æ•°
            
        Returns:
            Dict: æ¢å¤ç»“æœ
        """
        try:
            restore_weight_bonus = restore_weight_bonus or self.lifecycle_config['restore_weight_bonus']
            current_time = time.time()
            
            if memory_ids:
                # æ¢å¤æŒ‡å®šè®°å¿†
                placeholders = ','.join(['?' for _ in memory_ids])
                restore_query = f"""
                    UPDATE memories 
                    SET archived = 0,
                        weight = CASE 
                            WHEN weight * ? <= 10.0 THEN weight * ?
                            ELSE 10.0
                        END,
                        last_accessed = ?,
                        metadata = CASE 
                            WHEN metadata IS NULL THEN ?
                            ELSE json_patch(metadata, ?)
                        END
                    WHERE id IN ({placeholders}) AND archived = 1
                """
                
                metadata_json = json.dumps({
                    "restored_at": current_time,
                    "restore_reason": "manual_restore",
                    "weight_bonus_applied": restore_weight_bonus
                })
                
                params = [restore_weight_bonus, restore_weight_bonus, current_time, metadata_json, metadata_json] + memory_ids
            else:
                # æ¢å¤æ‰€æœ‰å½’æ¡£è®°å¿†ï¼ˆæ…ç”¨ï¼‰
                restore_query = """
                    UPDATE memories 
                    SET archived = 0,
                        weight = CASE 
                            WHEN weight * ? <= 10.0 THEN weight * ?
                            ELSE 10.0
                        END,
                        last_accessed = ?,
                        metadata = CASE 
                            WHEN metadata IS NULL THEN ?
                            ELSE json_patch(metadata, ?)
                        END
                    WHERE archived = 1
                """
                
                metadata_json = json.dumps({
                    "restored_at": current_time,
                    "restore_reason": "batch_restore",
                    "weight_bonus_applied": restore_weight_bonus
                })
                
                params = [restore_weight_bonus, restore_weight_bonus, current_time, metadata_json, metadata_json]
            
            result = self.db_manager.execute_query(restore_query, params)
            restored_count = result.rowcount if result and hasattr(result, 'rowcount') else 0
            
            self.logger.info(f"æ¢å¤äº† {restored_count} æ¡å½’æ¡£è®°å¿†")
            
            return {
                'success': True,
                'restored_count': restored_count,
                'weight_bonus': restore_weight_bonus,
                'message': f'æˆåŠŸæ¢å¤ {restored_count} æ¡å½’æ¡£è®°å¿†'
            }
            
        except Exception as e:
            self.logger.error(f"æ¢å¤å½’æ¡£è®°å¿†å¤±è´¥: {e}")
            return {'success': False, 'message': f'æ¢å¤å¤±è´¥: {str(e)}'}
    
    def cleanup_old_memories(self, days_threshold: int = None, permanent_delete: bool = False) -> Dict[str, Any]:
        """
        æ¸…ç†è¿‡æœŸè®°å¿†ï¼ˆå¯é€‰æ‹©æ°¸ä¹…åˆ é™¤ï¼‰
        
        Args:
            days_threshold: æ¸…ç†å¤©æ•°é˜ˆå€¼
            permanent_delete: æ˜¯å¦æ°¸ä¹…åˆ é™¤ï¼ˆå¦åˆ™ä»…æ ‡è®°ä¸ºå·²åˆ é™¤ï¼‰
            
        Returns:
            Dict: æ¸…ç†ç»“æœ
        """
        try:
            days_threshold = days_threshold or self.lifecycle_config['cleanup_threshold_days']
            min_weight = self.lifecycle_config['min_weight_for_permanent']
            
            current_time = time.time()
            cutoff_time = current_time - (days_threshold * 24 * 3600)
            
            if permanent_delete:
                # æ°¸ä¹…åˆ é™¤ï¼ˆåªåˆ é™¤æƒé‡å¾ˆä½çš„è®°å¿†ï¼‰
                cleanup_query = """
                    DELETE FROM memories 
                    WHERE weight < 2.0 
                    AND timestamp < ? 
                    AND archived = 1
                    AND type NOT IN ('system', 'summary')
                """
                
                result = self.db_manager.execute_query(cleanup_query, (cutoff_time,))
                cleaned_count = result.rowcount if result and hasattr(result, 'rowcount') else 0
                
                operation_type = "æ°¸ä¹…åˆ é™¤"
            else:
                # è½¯åˆ é™¤ï¼ˆæ ‡è®°ä¸ºå·²åˆ é™¤ï¼‰
                try:
                    self.db_manager.execute_query("ALTER TABLE memories ADD COLUMN deleted INTEGER DEFAULT 0")
                except:
                    pass  # å­—æ®µå¯èƒ½å·²å­˜åœ¨
                
                cleanup_query = """
                    UPDATE memories 
                    SET deleted = 1,
                        metadata = CASE 
                            WHEN metadata IS NULL THEN ?
                            ELSE json_patch(metadata, ?)
                        END
                    WHERE weight < 2.0 
                    AND timestamp < ? 
                    AND archived = 1
                    AND deleted = 0
                    AND type NOT IN ('system', 'summary')
                """
                
                metadata_json = json.dumps({
                    "deleted_at": current_time,
                    "delete_reason": "automatic_cleanup",
                    "cleanup_threshold": days_threshold
                })
                
                result = self.db_manager.execute_query(
                    cleanup_query, 
                    (metadata_json, metadata_json, cutoff_time)
                )
                cleaned_count = result.rowcount if result and hasattr(result, 'rowcount') else 0
                
                operation_type = "è½¯åˆ é™¤"
            
            self.logger.info(f"{operation_type}äº† {cleaned_count} æ¡è¿‡æœŸè®°å¿†")
            
            return {
                'success': True,
                'cleaned_count': cleaned_count,
                'operation_type': operation_type,
                'threshold_days': days_threshold,
                'permanent_delete': permanent_delete,
                'message': f'æˆåŠŸ{operation_type} {cleaned_count} æ¡è¿‡æœŸè®°å¿†'
            }
            
        except Exception as e:
            self.logger.error(f"æ¸…ç†è¿‡æœŸè®°å¿†å¤±è´¥: {e}")
            return {'success': False, 'message': f'æ¸…ç†å¤±è´¥: {str(e)}'}
    
    @handle_memory_errors({'error': 'è·å–ç”Ÿå‘½å‘¨æœŸç»Ÿè®¡å¤±è´¥'})
    def get_memory_lifecycle_stats(self) -> Dict[str, Any]:
        """
        è·å–è®°å¿†ç”Ÿå‘½å‘¨æœŸç»Ÿè®¡ - é‡æ„ç‰ˆæœ¬
        
        Returns:
            Dict: ç”Ÿå‘½å‘¨æœŸç»Ÿè®¡ä¿¡æ¯
        """
        # ğŸ”¥ ä½¿ç”¨ç»Ÿä¸€çš„æƒé‡åˆ†å¸ƒæŸ¥è¯¢æ„å»ºå™¨
        query, params = self.query_builder.build_weight_distribution_query()
        results = self.db_manager.execute_query(query, params)
        
        layer_stats = {}
        total_active = 0
        
        if results:
            for row in results:
                layer = row[0]
                count = row[1]
                total_active += count
                
                layer_stats[layer] = {
                    'count': count,
                    'avg_weight': round(row[2], 2),
                    'oldest_days': int((time.time() - row[5]) / 86400) if row[5] else 0,
                    'newest_days': int((time.time() - row[6]) / 86400) if row[6] else 0
                }
        
        # è·å–å½’æ¡£å’Œåˆ é™¤ç»Ÿè®¡
        archive_query = """
            SELECT 
                COUNT(*) as archived_count,
                AVG(weight) as avg_archived_weight
            FROM memories 
            WHERE archived = 1
            AND (deleted IS NULL OR deleted = 0)
        """
        
        archive_result = self.db_manager.execute_query(archive_query)
        archived_count = archive_result[0][0] if archive_result else 0
        avg_archived_weight = archive_result[0][1] if archive_result and archive_result[0][1] else 0
        
        delete_query = """
            SELECT COUNT(*) as deleted_count
            FROM memories 
            WHERE deleted = 1
        """
        
        delete_result = self.db_manager.execute_query(delete_query)
        deleted_count = delete_result[0][0] if delete_result else 0
        
        return {
            'layer_statistics': layer_stats,
            'total_active_memories': total_active,
            'archived_memories': {
                'count': archived_count,
                'avg_weight': round(avg_archived_weight, 2)
            },
            'deleted_memories': {
                'count': deleted_count
            },
            'lifecycle_config': self.lifecycle_config,
            'last_updated': time.time()
        }
    
    def schedule_lifecycle_maintenance(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®šæœŸç”Ÿå‘½å‘¨æœŸç»´æŠ¤
        
        Returns:
            Dict: ç»´æŠ¤ç»“æœ
        """
        try:
            maintenance_results = {
                'archive_result': None,
                'cleanup_result': None,
                'start_time': time.time(),
                'end_time': None,
                'total_processed': 0
            }
            
            # 1. å½’æ¡£è¿‡æœŸè®°å¿†
            self.logger.info("å¼€å§‹å½’æ¡£è¿‡æœŸè®°å¿†")
            archive_result = self.archive_old_memories()
            maintenance_results['archive_result'] = archive_result
            
            if archive_result['success']:
                maintenance_results['total_processed'] += archive_result['archived_count']
            
            # 2. æ¸…ç†éå¸¸æ—§çš„è®°å¿†ï¼ˆè½¯åˆ é™¤ï¼‰
            self.logger.info("å¼€å§‹æ¸…ç†è¿‡æœŸè®°å¿†")
            cleanup_result = self.cleanup_old_memories(permanent_delete=False)
            maintenance_results['cleanup_result'] = cleanup_result
            
            if cleanup_result['success']:
                maintenance_results['total_processed'] += cleanup_result['cleaned_count']
            
            # 3. æ›´æ–°æƒé‡ï¼ˆå¦‚æœæœ‰æƒé‡ç®¡ç†å™¨ï¼‰
            if self.weight_manager:
                self.logger.info("å¼€å§‹æ›´æ–°è®°å¿†æƒé‡")
                # è¿™é‡Œå¯ä»¥å®ç°æ‰¹é‡æƒé‡æ›´æ–°é€»è¾‘
                pass
            
            maintenance_results['end_time'] = time.time()
            maintenance_results['duration'] = maintenance_results['end_time'] - maintenance_results['start_time']
            
            success = (archive_result.get('success', False) and 
                      cleanup_result.get('success', False))
            
            return {
                'success': success,
                'maintenance_results': maintenance_results,
                'message': f'ç»´æŠ¤å®Œæˆï¼Œå…±å¤„ç† {maintenance_results["total_processed"]} æ¡è®°å¿†'
            }
            
        except Exception as e:
            self.logger.error(f"ç”Ÿå‘½å‘¨æœŸç»´æŠ¤å¤±è´¥: {e}")
            return {'success': False, 'message': f'ç»´æŠ¤å¤±è´¥: {str(e)}'}
    
    def get_lifecycle_recommendations(self) -> Dict[str, Any]:
        """
        è·å–ç”Ÿå‘½å‘¨æœŸç»´æŠ¤å»ºè®®
        
        Returns:
            Dict: ç»´æŠ¤å»ºè®®
        """
        try:
            recommendations = []
            stats = self.get_memory_lifecycle_stats()
            
            if 'error' in stats:
                return {'error': stats['error']}
            
            # æ£€æŸ¥çŸ­æœŸè®°å¿†æ•°é‡
            short_term_count = stats['layer_statistics'].get('çŸ­æœŸè®°å¿†', {}).get('count', 0)
            total_active = stats['total_active_memories']
            
            if total_active > 0:
                short_term_ratio = short_term_count / total_active
                if short_term_ratio > 0.6:  # çŸ­æœŸè®°å¿†è¶…è¿‡60%
                    recommendations.append({
                        'type': 'archive',
                        'priority': 'medium',
                        'message': f'çŸ­æœŸè®°å¿†æ¯”ä¾‹è¿‡é«˜ ({short_term_ratio:.1%})ï¼Œå»ºè®®æ‰§è¡Œå½’æ¡£æ“ä½œ',
                        'action': 'archive_old_memories'
                    })
            
            # æ£€æŸ¥å½’æ¡£è®°å¿†æ•°é‡
            archived_count = stats['archived_memories']['count']
            if archived_count > 1000:  # å½’æ¡£è®°å¿†è¶…è¿‡1000æ¡
                recommendations.append({
                    'type': 'cleanup',
                    'priority': 'low',
                    'message': f'å½’æ¡£è®°å¿†æ•°é‡è¾ƒå¤š ({archived_count})ï¼Œå»ºè®®å®šæœŸæ¸…ç†',
                    'action': 'cleanup_old_memories'
                })
            
            # æ£€æŸ¥æ ¸å¿ƒè®°å¿†æ¯”ä¾‹
            core_count = stats['layer_statistics'].get('æ ¸å¿ƒè®°å¿†', {}).get('count', 0)
            if total_active > 0:
                core_ratio = core_count / total_active
                if core_ratio > 0.15:  # æ ¸å¿ƒè®°å¿†è¶…è¿‡15%
                    recommendations.append({
                        'type': 'weight_adjustment',
                        'priority': 'medium',
                        'message': f'æ ¸å¿ƒè®°å¿†æ¯”ä¾‹è¿‡é«˜ ({core_ratio:.1%})ï¼Œå»ºè®®è°ƒæ•´æƒé‡ç­–ç•¥',
                        'action': 'review_weight_thresholds'
                    })
            
            return {
                'recommendations': recommendations,
                'stats_summary': stats,
                'check_time': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"è·å–ç”Ÿå‘½å‘¨æœŸå»ºè®®å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def validate_lifecycle_health(self) -> Dict[str, Any]:
        """
        éªŒè¯ç”Ÿå‘½å‘¨æœŸç³»ç»Ÿå¥åº·çŠ¶å†µ
        
        Returns:
            Dict: å¥åº·æ£€æŸ¥ç»“æœ
        """
        try:
            health_issues = []
            warnings = []
            
            stats = self.get_memory_lifecycle_stats()
            
            if 'error' in stats:
                return {'status': 'error', 'message': stats['error']}
            
            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            total_active = stats['total_active_memories']
            archived_count = stats['archived_memories']['count']
            deleted_count = stats['deleted_memories']['count']
            
            if total_active == 0:
                health_issues.append("æ²¡æœ‰æ´»è·ƒè®°å¿†")
            
            # æ£€æŸ¥å½’æ¡£çŠ¶æ€
            if archived_count > total_active * 2:
                warnings.append(f"å½’æ¡£è®°å¿†æ•°é‡ ({archived_count}) è¿œè¶…æ´»è·ƒè®°å¿† ({total_active})")
            
            # æ£€æŸ¥åˆ é™¤çŠ¶æ€
            if deleted_count > 0:
                warnings.append(f"å­˜åœ¨ {deleted_count} æ¡å·²åˆ é™¤è®°å¿†")
            
            # æ£€æŸ¥æƒé‡åˆ†å¸ƒ
            layer_stats = stats['layer_statistics']
            if 'çŸ­æœŸè®°å¿†' in layer_stats:
                short_term_count = layer_stats['çŸ­æœŸè®°å¿†']['count']
                if short_term_count > total_active * 0.8:
                    health_issues.append(f"çŸ­æœŸè®°å¿†å æ¯”è¿‡é«˜: {short_term_count}/{total_active}")
            
            # ç¡®å®šå¥åº·çŠ¶æ€
            if health_issues:
                health_status = 'unhealthy'
            elif warnings:
                health_status = 'warning'
            else:
                health_status = 'healthy'
            
            return {
                'status': health_status,
                'issues': health_issues,
                'warnings': warnings,
                'statistics': stats,
                'check_time': time.time()
            }
            
        except Exception as e:
            self.logger.error(f"ç”Ÿå‘½å‘¨æœŸå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return {
                'status': 'error',
                'message': str(e),
                'check_time': time.time()
            } 