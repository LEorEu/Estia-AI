"""
æ–‡æœ¬å‘é‡åŒ–æ¨¡å— - è´Ÿè´£å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
"""

import os
import numpy as np
import logging
from typing import List, Dict, Any, Optional, Union, Tuple
import time

# å°è¯•å¯¼å…¥æ—¥å¿—å·¥å…·
try:
    from core.utils.logger import get_logger
    logger = get_logger("estia.memory.embedding.vectorizer")
except ImportError:
    # å¦‚æœè¿˜æ²¡æœ‰æ—¥å¿—å·¥å…·ï¼Œä½¿ç”¨æ ‡å‡†æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger("estia.memory.embedding.vectorizer")

# å°è¯•å¯¼å…¥ç¼“å­˜æ¨¡å—
try:
    from .cache import EnhancedMemoryCache
    EmbeddingCache = EnhancedMemoryCache  # ä½¿ç”¨æ–°çš„å¢å¼ºç¼“å­˜
except ImportError:
    logger.warning("æ— æ³•å¯¼å…¥EnhancedMemoryCacheï¼Œå°†ä¸ä½¿ç”¨ç¼“å­˜åŠŸèƒ½")
    EmbeddingCache = None

class TextVectorizer:
    """
    æ–‡æœ¬å‘é‡åŒ–ç±»ï¼Œè´Ÿè´£å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
    
    æ”¯æŒå¤šç§Embeddingæ¨¡å‹:
    - æœ¬åœ°æ¨¡å‹ (sentence-transformers)
    - OpenAI API
    - è‡ªå®šä¹‰æ¨¡å‹
    """
    
    # æ”¯æŒçš„æ¨¡å‹ç±»å‹
    MODEL_TYPES = ["sentence-transformers", "openai", "custom"]
    
    # é»˜è®¤æ¨¡å‹é…ç½®
    DEFAULT_MODEL = "sentence-transformers"
    DEFAULT_MODEL_NAME = "Qwen/Qwen3-Embedding-0.6B"  # ä½¿ç”¨é˜¿é‡Œå·´å·´çš„Qwenæ¨¡å‹
    
    def __init__(self, model_type: Optional[str] = None, model_name: Optional[str] = None, 
                 api_key: Optional[str] = None, cache_dir: Optional[str] = None, 
                 use_cache: bool = True, device: str = "cpu"):
        """
        åˆå§‹åŒ–æ–‡æœ¬å‘é‡åŒ–å™¨
        
        å‚æ•°:
            model_type: æ¨¡å‹ç±»å‹ï¼Œå¯é€‰å€¼ä¸º "sentence-transformers", "openai", "custom"
            model_name: æ¨¡å‹åç§°ï¼Œå¯¹äºsentence-transformersæ˜¯æ¨¡å‹IDï¼Œå¯¹äºopenaiæ˜¯æ¨¡å‹åç§°
            api_key: APIå¯†é’¥ï¼Œç”¨äºOpenAI API
            cache_dir: ç¼“å­˜ç›®å½•
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            device: è®¾å¤‡ï¼Œå¯é€‰å€¼ä¸º "cpu", "cuda", "mps"ï¼ˆå¯¹äºApple Siliconï¼‰
        """
        self.model_type = model_type or self.DEFAULT_MODEL
        self.model_name = model_name or self.DEFAULT_MODEL_NAME
        self.api_key = api_key
        self.device = device
        self.use_cache = use_cache and EmbeddingCache is not None
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = None
        self.vector_dim = 0
        
        # åˆå§‹åŒ–å¢å¼ºç¼“å­˜
        self.cache = None
        if self.use_cache and EmbeddingCache is not None:
            self.cache = EmbeddingCache(cache_dir=cache_dir)
            logger.info("å·²å¯ç”¨å¢å¼ºç‰ˆè®°å¿†ç¼“å­˜")
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
        
        logger.info(f"æ–‡æœ¬å‘é‡åŒ–å™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {self.model_type}/{self.model_name}")
    
    def _load_model(self) -> None:
        """åŠ è½½Embeddingæ¨¡å‹"""
        if self.model_type == "sentence-transformers":
            self._load_sentence_transformers()
        elif self.model_type == "openai":
            self._load_openai()
        elif self.model_type == "custom":
            self._load_custom_model()
        else:
            logger.warning(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {self.model_type}ï¼Œå°†ä½¿ç”¨é»˜è®¤æ¨¡å‹")
            self.model_type = self.DEFAULT_MODEL
            self._load_sentence_transformers()
    
    def _load_sentence_transformers(self) -> None:
        """åŠ è½½sentence-transformersæ¨¡å‹"""
        try:
            # ğŸ”¥ å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ - ä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜
            # è®¾ç½®ç¯å¢ƒå˜é‡ç¦ç”¨ç½‘ç»œæ£€æŸ¥
            os.environ["HF_HUB_OFFLINE"] = "1"  # å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
            os.environ["TRANSFORMERS_OFFLINE"] = "1"  # transformersç¦»çº¿æ¨¡å¼
            os.environ["HF_DATASETS_OFFLINE"] = "1"  # datasetsç¦»çº¿æ¨¡å¼
            
            # è®¾ç½®é•œåƒæºï¼ˆç”¨äºå¿…è¦æ—¶çš„ä¸‹è½½ï¼‰
            os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
            
            from sentence_transformers import SentenceTransformer
            
            # å…ˆå°è¯•å®Œå…¨ç¦»çº¿åŠ è½½
            try:
                logger.info(f"ğŸ”„ å°è¯•ç¦»çº¿åŠ è½½æ¨¡å‹: {self.model_name}")
                self.model = SentenceTransformer(
                    self.model_name, 
                    device=self.device,
                    cache_folder="cache"  # æ˜ç¡®æŒ‡å®šç¼“å­˜ç›®å½•
                )
                logger.info("âœ… ç¦»çº¿æ¨¡å¼åŠ è½½æˆåŠŸ")
                
            except Exception as offline_error:
                logger.warning(f"ç¦»çº¿åŠ è½½å¤±è´¥: {offline_error}")
                logger.info("ğŸŒ åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼...")
                
                # å¦‚æœç¦»çº¿å¤±è´¥ï¼Œæ¸…é™¤ç¦»çº¿è®¾ç½®ï¼Œå…è®¸è”ç½‘
                if "HF_HUB_OFFLINE" in os.environ:
                    del os.environ["HF_HUB_OFFLINE"]
                if "TRANSFORMERS_OFFLINE" in os.environ:
                    del os.environ["TRANSFORMERS_OFFLINE"]
                
                # é‡æ–°å°è¯•åŠ è½½
                self.model = SentenceTransformer(
                    self.model_name, 
                    device=self.device,
                    cache_folder="cache"
                )
                logger.info("âœ… åœ¨çº¿æ¨¡å¼åŠ è½½æˆåŠŸ")
            
            # è·å–å‘é‡ç»´åº¦
            self.vector_dim = self.model.get_sentence_embedding_dimension()
            logger.info(f"æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦: {self.vector_dim}")
            
        except ImportError:
            logger.error("æœªæ‰¾åˆ°sentence-transformersåº“ï¼Œè¯·å®‰è£…: pip install sentence-transformers")
            raise
        except Exception as e:
            logger.error(f"åŠ è½½sentence-transformersæ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def _load_openai(self) -> None:
        """åŠ è½½OpenAI API"""
        try:
            import openai
            
            # è®¾ç½®APIå¯†é’¥
            if self.api_key:
                openai.api_key = self.api_key
            elif "OPENAI_API_KEY" in os.environ:
                openai.api_key = os.environ["OPENAI_API_KEY"]
            else:
                logger.error("æœªæä¾›OpenAI APIå¯†é’¥")
                raise ValueError("æœªæä¾›OpenAI APIå¯†é’¥ï¼Œè¯·é€šè¿‡api_keyå‚æ•°æˆ–OPENAI_API_KEYç¯å¢ƒå˜é‡è®¾ç½®")
            
            # é»˜è®¤æ¨¡å‹åç§°
            if not self.model_name:
                self.model_name = "text-embedding-ada-002"
            
            # è®¾ç½®æ¨¡å‹ï¼ˆå®é™…ä¸Šä¸éœ€è¦é¢„åŠ è½½ï¼‰
            self.model = openai
            
            # æ ¹æ®æ¨¡å‹è®¾ç½®å‘é‡ç»´åº¦
            model_dims = {
                "text-embedding-ada-002": 1536,
                "text-embedding-3-small": 1536,
                "text-embedding-3-large": 3072
            }
            self.vector_dim = model_dims.get(self.model_name, 1536)
            
            logger.info(f"OpenAI APIé…ç½®æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {self.model_name}, å‘é‡ç»´åº¦: {self.vector_dim}")
            
        except ImportError:
            logger.error("æœªæ‰¾åˆ°openaiåº“ï¼Œè¯·å®‰è£…: pip install openai")
            raise
        except Exception as e:
            logger.error(f"é…ç½®OpenAI APIå¤±è´¥: {e}")
            raise
    
    def _load_custom_model(self) -> None:
        """åŠ è½½è‡ªå®šä¹‰æ¨¡å‹"""
        # è¿™é‡Œå¯ä»¥å®ç°åŠ è½½è‡ªå®šä¹‰æ¨¡å‹çš„é€»è¾‘
        # ä¾‹å¦‚åŸºäºå…¶ä»–æ¡†æ¶çš„æ¨¡å‹æˆ–è‡ªå·±è®­ç»ƒçš„æ¨¡å‹
        logger.warning("è‡ªå®šä¹‰æ¨¡å‹åŠ è½½æœªå®ç°ï¼Œè¯·åœ¨å­ç±»ä¸­å®ç°")
        raise NotImplementedError("è‡ªå®šä¹‰æ¨¡å‹åŠ è½½æœªå®ç°")
    
    def encode(self, texts: Union[str, List[str]], 
               batch_size: int = 32, 
               show_progress: bool = False,
               memory_weights: Optional[Union[float, List[float]]] = None) -> np.ndarray:
        """
        å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡
        
        å‚æ•°:
            texts: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            memory_weights: è®°å¿†é‡è¦æ€§æƒé‡ï¼Œç”¨äºæ™ºèƒ½ç¼“å­˜
            
        è¿”å›:
            np.ndarray: æ–‡æœ¬å‘é‡ï¼Œå½¢çŠ¶ä¸º (n, vector_dim) æˆ– (vector_dim,)
        """
        if self.model is None:
            logger.error("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç¼–ç æ–‡æœ¬")
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç¼–ç æ–‡æœ¬")
        
        # ç¡®ä¿è¾“å…¥æ˜¯åˆ—è¡¨
        is_single_text = isinstance(texts, str)
        if is_single_text:
            texts = [texts]
            if memory_weights is not None and not isinstance(memory_weights, list):
                memory_weights = [memory_weights]
        
        # å¤„ç†æƒé‡
        if memory_weights is None:
            memory_weights = [1.0] * len(texts)
        elif isinstance(memory_weights, (int, float)):
            # å•ä¸ªæƒé‡å€¼ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
            memory_weights = [float(memory_weights)] * len(texts)
        elif len(memory_weights) != len(texts):
            logger.warning(f"æƒé‡æ•°é‡({len(memory_weights)})ä¸æ–‡æœ¬æ•°é‡({len(texts)})ä¸åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤æƒé‡")
            memory_weights = [1.0] * len(texts)
        
        # æ£€æŸ¥å¢å¼ºç¼“å­˜
        if self.use_cache and self.cache:
            # å°è¯•ä»ç¼“å­˜è·å–æ‰€æœ‰å‘é‡
            cached_vectors = []
            texts_to_encode = []
            text_indices = []
            weights_to_encode = []
            
            for i, (text, weight) in enumerate(zip(texts, memory_weights)):
                vector = self.cache.get(text, memory_weight=weight)
                if vector is not None:
                    cached_vectors.append((i, vector))
                else:
                    texts_to_encode.append(text)
                    text_indices.append(i)
                    weights_to_encode.append(weight)
            
            # å¦‚æœæ‰€æœ‰å‘é‡éƒ½åœ¨ç¼“å­˜ä¸­
            if len(texts_to_encode) == 0:
                logger.debug(f"æ‰€æœ‰ {len(texts)} ä¸ªæ–‡æœ¬éƒ½ä»ç¼“å­˜ä¸­è·å–")
                # æ„å»ºç»“æœæ•°ç»„
                results = np.zeros((len(texts), self.vector_dim), dtype=np.float32)
                for i, vector in cached_vectors:
                    results[i] = vector
                
                return results[0] if is_single_text else results
            
            # ç¼–ç æœªç¼“å­˜çš„æ–‡æœ¬
            if len(texts_to_encode) > 0:
                logger.debug(f"ä»ç¼“å­˜è·å– {len(cached_vectors)} ä¸ªå‘é‡ï¼Œéœ€è¦ç¼–ç  {len(texts_to_encode)} ä¸ªæ–‡æœ¬")
                new_vectors = self._encode_texts(texts_to_encode, batch_size, show_progress)
                
                # å°†æ–°å‘é‡æ·»åŠ åˆ°ç¼“å­˜
                for text, vector, weight in zip(texts_to_encode, new_vectors, weights_to_encode):
                    self.cache.put(text, vector, memory_weight=weight)
                
                # æ„å»ºå®Œæ•´ç»“æœæ•°ç»„
                results = np.zeros((len(texts), self.vector_dim), dtype=np.float32)
                
                # å¡«å…¥ç¼“å­˜çš„å‘é‡
                for i, vector in cached_vectors:
                    results[i] = vector
                
                # å¡«å…¥æ–°ç¼–ç çš„å‘é‡
                for idx, vector in zip(text_indices, new_vectors):
                    results[idx] = vector
                
                return results[0] if is_single_text else results
        
        # å¦‚æœä¸ä½¿ç”¨ç¼“å­˜ï¼Œç›´æ¥ç¼–ç 
        vectors = self._encode_texts(texts, batch_size, show_progress)
        
        # å¦‚æœä½¿ç”¨ç¼“å­˜ï¼Œä¿å­˜æ–°ç¼–ç çš„å‘é‡
        if self.use_cache and self.cache:
            for text, vector, weight in zip(texts, vectors, memory_weights):
                self.cache.put(text, vector, memory_weight=weight)
        
        return vectors[0] if is_single_text else vectors
    
    def _encode_texts(self, texts: List[str], batch_size: int = 32, 
                     show_progress: bool = False) -> np.ndarray:
        """å®é™…çš„æ–‡æœ¬ç¼–ç é€»è¾‘"""
        if self.model_type == "sentence-transformers":
            return self._encode_with_sentence_transformers(texts, batch_size, show_progress)
        elif self.model_type == "openai":
            return self._encode_with_openai(texts, batch_size)
        elif self.model_type == "custom":
            return self._encode_with_custom_model(texts, batch_size)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
    
    def _encode_with_sentence_transformers(self, texts: List[str], 
                                          batch_size: int = 32, 
                                          show_progress: bool = False) -> np.ndarray:
        """ä½¿ç”¨sentence-transformersç¼–ç æ–‡æœ¬"""
        try:
            # ä½¿ç”¨æ¨¡å‹ç¼–ç 
            start_time = time.time()
            embeddings = self.model.encode(
                texts, 
                batch_size=batch_size, 
                show_progress_bar=show_progress,
                convert_to_numpy=True
            )
            encode_time = time.time() - start_time
            
            logger.debug(f"ä½¿ç”¨sentence-transformersç¼–ç  {len(texts)} ä¸ªæ–‡æœ¬ï¼Œè€—æ—¶: {encode_time:.2f}ç§’")
            return embeddings
            
        except Exception as e:
            logger.error(f"sentence-transformersç¼–ç å¤±è´¥: {e}")
            raise
    
    def _encode_with_openai(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """ä½¿ç”¨OpenAI APIç¼–ç æ–‡æœ¬"""
        try:
            all_embeddings = []
            
            # æ‰¹é‡å¤„ç†
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i+batch_size]
                
                start_time = time.time()
                response = self.model.Embedding.create(
                    input=batch,
                    model=self.model_name
                )
                batch_time = time.time() - start_time
                
                # æå–åµŒå…¥å‘é‡
                batch_embeddings = [item["embedding"] for item in response["data"]]
                all_embeddings.extend(batch_embeddings)
                
                logger.debug(f"OpenAIæ‰¹å¤„ç† {len(batch)} ä¸ªæ–‡æœ¬ï¼Œè€—æ—¶: {batch_time:.2f}ç§’")
                
                # APIé€Ÿç‡é™åˆ¶ï¼Œé¿å…è§¦å‘é™åˆ¶
                if i + batch_size < len(texts):
                    time.sleep(0.5)
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            embeddings = np.array(all_embeddings, dtype=np.float32)
            return embeddings
            
        except Exception as e:
            logger.error(f"OpenAIç¼–ç å¤±è´¥: {e}")
            raise
    
    def _encode_with_custom_model(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹ç¼–ç æ–‡æœ¬"""
        # è¿™é‡Œå®ç°è‡ªå®šä¹‰æ¨¡å‹çš„ç¼–ç é€»è¾‘
        logger.warning("è‡ªå®šä¹‰æ¨¡å‹ç¼–ç æœªå®ç°")
        raise NotImplementedError("è‡ªå®šä¹‰æ¨¡å‹ç¼–ç æœªå®ç°")
    
    def get_vector_dimension(self) -> int:
        """
        è·å–å‘é‡ç»´åº¦
        
        è¿”å›:
            int: å‘é‡ç»´åº¦
        """
        return self.vector_dim
    
    def compute_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        
        å‚æ•°:
            vec1: ç¬¬ä¸€ä¸ªå‘é‡
            vec2: ç¬¬äºŒä¸ªå‘é‡
            
        è¿”å›:
            float: ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        # ç¡®ä¿å‘é‡æ˜¯ä¸€ç»´çš„
        if vec1.ndim > 1:
            vec1 = vec1.flatten()
        if vec2.ndim > 1:
            vec2 = vec2.flatten()
            
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
            
        return dot_product / (norm1 * norm2)
    
    def batch_compute_similarity(self, query_vec: np.ndarray, 
                                vectors: np.ndarray) -> np.ndarray:
        """
        æ‰¹é‡è®¡ç®—æŸ¥è¯¢å‘é‡ä¸å¤šä¸ªå‘é‡çš„ç›¸ä¼¼åº¦
        
        å‚æ•°:
            query_vec: æŸ¥è¯¢å‘é‡ï¼Œå½¢çŠ¶ä¸º (vector_dim,)
            vectors: å‘é‡æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (n, vector_dim)
            
        è¿”å›:
            np.ndarray: ç›¸ä¼¼åº¦åˆ†æ•°æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (n,)
        """
        # ç¡®ä¿æŸ¥è¯¢å‘é‡æ˜¯ä¸€ç»´çš„
        if query_vec.ndim > 1:
            query_vec = query_vec.flatten()
            
        # è®¡ç®—ç‚¹ç§¯
        dot_products = np.dot(vectors, query_vec)
        
        # è®¡ç®—èŒƒæ•°
        query_norm = np.linalg.norm(query_vec)
        vector_norms = np.linalg.norm(vectors, axis=1)
        
        # é¿å…é™¤é›¶
        mask = vector_norms != 0
        similarities = np.zeros_like(vector_norms)
        similarities[mask] = dot_products[mask] / (vector_norms[mask] * query_norm)
        
        return similarities
    
    def search_cached_memories(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """
        åœ¨ç¼“å­˜ä¸­æœç´¢ç›¸å…³è®°å¿†ï¼Œåˆ©ç”¨å…³é”®è¯ç¼“å­˜åŠ é€Ÿ
        
        å‚æ•°:
            query: æœç´¢æŸ¥è¯¢
            limit: æœ€å¤§è¿”å›æ¡æ•°
            
        è¿”å›:
            List[Dict]: åŒ¹é…çš„è®°å¿†åˆ—è¡¨
        """
        if not self.use_cache or not self.cache:
            logger.warning("ç¼“å­˜æœªå¯ç”¨ï¼Œæ— æ³•æœç´¢ç¼“å­˜è®°å¿†")
            return []
        
        return self.cache.search_by_content(query, limit)
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if not self.use_cache or not self.cache:
            return {"cache_enabled": False}
        
        stats = self.cache.get_stats()
        stats["cache_enabled"] = True
        return stats
    
    def clear_cache(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        if self.use_cache and self.cache:
            self.cache.clear_all_cache()
            logger.info("å‘é‡åŒ–å™¨ç¼“å­˜å·²æ¸…ç©º")

# æ¨¡å—æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("æµ‹è¯•æ–‡æœ¬å‘é‡åŒ–æ¨¡å—...")
    
    # å°è¯•åŠ è½½sentence-transformersæ¨¡å‹
    try:
        print("\n1. æµ‹è¯•sentence-transformersæ¨¡å‹")
        vectorizer = TextVectorizer(
            model_type="sentence-transformers",
            model_name="paraphrase-multilingual-MiniLM-L12-v2",
            use_cache=True
        )
        
        # æµ‹è¯•æ–‡æœ¬
        test_texts = [
            "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯å‘é‡åŒ–åŠŸèƒ½",
            "This is a test text for verifying vectorization functionality",
            "è¿™æ˜¯å¦ä¸€ä¸ªç›¸ä¼¼çš„æµ‹è¯•æ–‡æœ¬"
        ]
        
        # ç¼–ç æ–‡æœ¬
        print("ç¼–ç æ–‡æœ¬...")
        vectors = vectorizer.encode(test_texts, show_progress=True)
        
        print(f"å‘é‡å½¢çŠ¶: {vectors.shape}")
        print(f"å‘é‡ç»´åº¦: {vectorizer.get_vector_dimension()}")
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        print("\nè®¡ç®—ç›¸ä¼¼åº¦:")
        for i in range(len(test_texts)):
            for j in range(i+1, len(test_texts)):
                sim = vectorizer.compute_similarity(vectors[i], vectors[j])
                print(f"æ–‡æœ¬ {i+1} å’Œæ–‡æœ¬ {j+1} çš„ç›¸ä¼¼åº¦: {sim:.4f}")
        
        # æµ‹è¯•æ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—
        print("\næ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦:")
        query_vec = vectors[0]
        similarities = vectorizer.batch_compute_similarity(query_vec, vectors)
        for i, sim in enumerate(similarities):
            print(f"æŸ¥è¯¢ä¸æ–‡æœ¬ {i+1} çš„ç›¸ä¼¼åº¦: {sim:.4f}")
        
        # æµ‹è¯•ç¼“å­˜
        print("\næµ‹è¯•ç¼“å­˜:")
        start_time = time.time()
        cached_vectors = vectorizer.encode(test_texts)
        cache_time = time.time() - start_time
        print(f"ä»ç¼“å­˜è·å–å‘é‡è€—æ—¶: {cache_time:.6f}ç§’")
        
        # éªŒè¯å‘é‡ä¸€è‡´æ€§
        is_equal = np.array_equal(vectors, cached_vectors)
        print(f"å‘é‡ä¸€è‡´æ€§: {is_equal}")
        
    except ImportError:
        print("æœªå®‰è£…sentence-transformersï¼Œè·³è¿‡æµ‹è¯•")
    except Exception as e:
        print(f"æµ‹è¯•sentence-transformerså¤±è´¥: {e}")
    
    # å¦‚æœæœ‰OpenAI APIå¯†é’¥ï¼Œæµ‹è¯•OpenAIæ¨¡å‹
    if "OPENAI_API_KEY" in os.environ:
        try:
            print("\n2. æµ‹è¯•OpenAIæ¨¡å‹")
            openai_vectorizer = TextVectorizer(
                model_type="openai",
                model_name="text-embedding-ada-002",
                use_cache=True
            )
            
            # æµ‹è¯•å•ä¸ªæ–‡æœ¬
            test_text = "è¿™æ˜¯ä¸€ä¸ªç®€çŸ­çš„æµ‹è¯•æ–‡æœ¬"
            
            print("ç¼–ç æ–‡æœ¬...")
            vector = openai_vectorizer.encode(test_text)
            
            print(f"å‘é‡å½¢çŠ¶: {vector.shape}")
            print(f"å‘é‡ç»´åº¦: {openai_vectorizer.get_vector_dimension()}")
            
        except ImportError:
            print("æœªå®‰è£…openaiåº“ï¼Œè·³è¿‡æµ‹è¯•")
        except Exception as e:
            print(f"æµ‹è¯•OpenAIæ¨¡å‹å¤±è´¥: {e}")
    else:
        print("\næœªè®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡ï¼Œè·³è¿‡OpenAIæ¨¡å‹æµ‹è¯•")
    
    print("\næµ‹è¯•å®Œæˆ")
