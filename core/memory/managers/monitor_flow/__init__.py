#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è®°å¿†æµç¨‹ç›‘æ§å™¨ (MemoryFlowMonitor)
åˆå¹¶system_stats.pyåˆ°monitoring/æ¨¡å—
èŒè´£ï¼šæ¨ªåˆ‡å…³æ³¨ç‚¹ï¼Œç›‘æ§æ‰€æœ‰æµç¨‹çš„æ€§èƒ½å’ŒçŠ¶æ€
"""

import time
import logging
from typing import Dict, Any, List, Optional
from ...shared.internal import handle_memory_errors, ErrorHandlerMixin

logger = logging.getLogger(__name__)

class MemoryFlowMonitor(ErrorHandlerMixin):
    """è®°å¿†æµç¨‹ç›‘æ§å™¨ - æ¨ªåˆ‡å…³æ³¨ç‚¹ç›‘æ§"""
    
    def __init__(self, components: Dict[str, Any]):
        """
        åˆå§‹åŒ–æµç¨‹ç›‘æ§å™¨
        
        Args:
            components: æ‰€éœ€çš„ç»„ä»¶å­—å…¸
        """
        super().__init__()
        self.db_manager = components.get('db_manager')
        self.unified_cache = components.get('unified_cache')
        self.sync_flow_manager = components.get('sync_flow_manager')
        self.async_flow_manager = components.get('async_flow_manager')
        
        # å¯¼å…¥åŸsystem_statsåŠŸèƒ½
        from .system_stats import SystemStatsManager
        self.system_stats = SystemStatsManager(self.db_manager, self.unified_cache)
        
        # å¯¼å…¥monitoringæ¨¡å—åŠŸèƒ½
        try:
            from .monitoring.pipeline_monitor import PipelineMonitor
            from .monitoring.analytics import PerformanceAnalyzer
            
            self.pipeline_monitor = PipelineMonitor()
            self.performance_analyzer = PerformanceAnalyzer()
        except ImportError:
            self.pipeline_monitor = None
            self.performance_analyzer = None
            
        self.logger = logger
    
    @handle_memory_errors({'error': 'è·å–ç³»ç»Ÿç»Ÿè®¡å¤±è´¥'})
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """
        è·å–ç»¼åˆç³»ç»Ÿç»Ÿè®¡
        åˆå¹¶system_stats.pyçš„åŠŸèƒ½
        
        Returns:
            Dict: å®Œæ•´çš„ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            'timestamp': time.time(),
            'monitor_status': 'active'
        }
        
        try:
            # 1. åŸºç¡€ç³»ç»Ÿç»Ÿè®¡ï¼ˆæ¥è‡ªåŸsystem_stats.pyï¼‰
            basic_stats = self.system_stats.get_memory_statistics()
            stats['memory_overview'] = basic_stats
            
            # 2. æƒé‡åˆ†å¸ƒç»Ÿè®¡
            weight_stats = self.system_stats.get_weight_distribution()
            stats['weight_distribution'] = weight_stats
            
            # 3. æ€§èƒ½ç»Ÿè®¡
            performance_stats = self.system_stats.get_performance_statistics()
            stats['performance_metrics'] = performance_stats
            
            # 4. ä¼šè¯ç»Ÿè®¡
            session_stats = self.system_stats.get_session_statistics()
            stats['session_statistics'] = session_stats
            
            # 5. å¥åº·æŠ¥å‘Š
            health_report = self.system_stats.get_health_report()
            stats['health_status'] = health_report
            
            # 6. æµç¨‹ç›‘æ§ï¼ˆæ–°å¢ï¼‰
            if self.pipeline_monitor:
                pipeline_stats = self.pipeline_monitor.get_pipeline_stats()
                stats['pipeline_monitoring'] = pipeline_stats
            
            # 7. æ€§èƒ½åˆ†æï¼ˆæ–°å¢ï¼‰
            if self.performance_analyzer:
                analysis_report = self.performance_analyzer.get_analysis_report()
                stats['performance_analysis'] = analysis_report
                
            return stats
            
        except Exception as e:
            self.logger.error(f"è·å–ç»¼åˆç»Ÿè®¡å¤±è´¥: {e}")
            return {'error': str(e), 'timestamp': time.time()}
    
    @handle_memory_errors({'error': 'æµç¨‹ç›‘æ§å¤±è´¥'})
    def monitor_flow_execution(self, flow_type: str, operation: str, 
                              start_time: float, end_time: float, 
                              success: bool = True, metadata: Dict = None) -> bool:
        """
        ç›‘æ§æµç¨‹æ‰§è¡Œ
        
        Args:
            flow_type: æµç¨‹ç±»å‹ ('sync', 'async')
            operation: æ“ä½œåç§°
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            success: æ˜¯å¦æˆåŠŸ
            metadata: é¢å¤–å…ƒæ•°æ®
            
        Returns:
            bool: ç›‘æ§è®°å½•æ˜¯å¦æˆåŠŸ
        """
        try:
            execution_time = end_time - start_time
            
            monitor_record = {
                'flow_type': flow_type,
                'operation': operation,
                'execution_time_ms': round(execution_time * 1000, 2),
                'success': success,
                'timestamp': start_time,
                'metadata': metadata or {}
            }
            
            # è®°å½•åˆ°pipelineç›‘æ§å™¨
            if self.pipeline_monitor:
                self.pipeline_monitor.record_operation(monitor_record)
            
            # æ€§èƒ½åˆ†æ
            if self.performance_analyzer:
                self.performance_analyzer.analyze_performance(monitor_record)
            
            # æ—¥å¿—è®°å½•
            status = "æˆåŠŸ" if success else "å¤±è´¥"
            self.logger.debug(f"ğŸ“Š {flow_type}æµç¨‹ç›‘æ§: {operation} {status} ({execution_time*1000:.2f}ms)")
            
            return True
            
        except Exception as e:
            self.logger.error(f"æµç¨‹ç›‘æ§è®°å½•å¤±è´¥: {e}")
            return False
    
    def get_13_step_monitoring(self) -> Dict[str, Any]:
        """
        è·å–13æ­¥æµç¨‹ç›‘æ§è¯¦æƒ…
        
        Returns:
            Dict: 13æ­¥æµç¨‹çš„è¯¦ç»†ç›‘æ§ä¿¡æ¯
        """
        try:
            if not self.pipeline_monitor:
                return {'error': 'Pipelineç›‘æ§å™¨æœªåˆå§‹åŒ–'}
            
            # è·å–æ¯ä¸ªæ­¥éª¤çš„ç›‘æ§æ•°æ®
            step_stats = {}
            
            # åŒæ­¥æµç¨‹ç›‘æ§ (Step 1-9)
            sync_steps = [
                'system_init', 'component_init', 'async_evaluator_init',
                'cache_vectorization', 'faiss_search', 'association_expansion',
                'history_aggregation', 'weight_ranking', 'context_assembly'
            ]
            
            # å¼‚æ­¥æµç¨‹ç›‘æ§ (Step 10-15)
            async_steps = [
                'async_queue_trigger', 'llm_evaluation', 'weight_update',
                'layer_adjustment', 'summary_generation', 'association_creation'
            ]
            
            all_steps = sync_steps + async_steps
            
            for i, step in enumerate(all_steps, 1):
                step_data = self.pipeline_monitor.get_step_stats(step)
                step_stats[f'step_{i:02d}_{step}'] = step_data
            
            return {
                'timestamp': time.time(),
                'total_steps': len(all_steps),
                'sync_steps': len(sync_steps),
                'async_steps': len(async_steps),
                'step_details': step_stats,
                'overall_performance': self._calculate_overall_performance(step_stats)
            }
            
        except Exception as e:
            self.logger.error(f"è·å–13æ­¥ç›‘æ§å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _calculate_overall_performance(self, step_stats: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—æ•´ä½“æ€§èƒ½æŒ‡æ ‡"""
        try:
            total_time = 0
            success_count = 0
            total_count = 0
            
            for step_name, stats in step_stats.items():
                if isinstance(stats, dict) and 'avg_time_ms' in stats:
                    total_time += stats.get('avg_time_ms', 0)
                    if stats.get('success_rate', 0) > 0.8:
                        success_count += 1
                    total_count += 1
            
            return {
                'total_avg_time_ms': round(total_time, 2),
                'overall_success_rate': round(success_count / total_count, 3) if total_count > 0 else 0,
                'performance_grade': self._get_performance_grade(total_time, success_count / total_count if total_count > 0 else 0)
            }
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—æ•´ä½“æ€§èƒ½å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _get_performance_grade(self, total_time: float, success_rate: float) -> str:
        """è·å–æ€§èƒ½ç­‰çº§"""
        if total_time < 200 and success_rate > 0.95:
            return 'A'
        elif total_time < 500 and success_rate > 0.90:
            return 'B'
        elif total_time < 1000 and success_rate > 0.80:
            return 'C'
        else:
            return 'D'
    
    def start_monitoring(self, operation_name: str) -> str:
        """
        å¼€å§‹ç›‘æ§æ“ä½œ
        
        Args:
            operation_name: æ“ä½œåç§°
            
        Returns:
            str: ç›‘æ§ID
        """
        try:
            monitor_id = f"{operation_name}_{int(time.time() * 1000)}"
            start_time = time.time()
            
            # å­˜å‚¨ç›‘æ§ä¼šè¯
            if not hasattr(self, '_active_monitors'):
                self._active_monitors = {}
            
            self._active_monitors[monitor_id] = {
                'operation': operation_name,
                'start_time': start_time,
                'status': 'active'
            }
            
            self.logger.debug(f"ğŸ“Š å¼€å§‹ç›‘æ§: {operation_name} (ID: {monitor_id})")
            return monitor_id
            
        except Exception as e:
            self.logger.error(f"å¼€å§‹ç›‘æ§å¤±è´¥: {e}")
            return f"error_{int(time.time())}"
    
    def end_monitoring(self, operation_name: str, monitor_id: str = None) -> Dict[str, Any]:
        """
        ç»“æŸç›‘æ§æ“ä½œ
        
        Args:
            operation_name: æ“ä½œåç§°
            monitor_id: ç›‘æ§IDï¼Œå¦‚æœä¸æä¾›åˆ™æŸ¥æ‰¾æœ€è¿‘çš„
            
        Returns:
            Dict: ç›‘æ§ç»“æœ
        """
        try:
            end_time = time.time()
            
            if not hasattr(self, '_active_monitors'):
                self._active_monitors = {}
            
            # æŸ¥æ‰¾å¯¹åº”çš„ç›‘æ§ä¼šè¯
            target_monitor = None
            target_id = None
            
            if monitor_id and monitor_id in self._active_monitors:
                target_monitor = self._active_monitors[monitor_id]
                target_id = monitor_id
            else:
                # æŸ¥æ‰¾æœ€è¿‘çš„åŒåæ“ä½œ
                for mid, monitor in self._active_monitors.items():
                    if monitor['operation'] == operation_name and monitor['status'] == 'active':
                        target_monitor = monitor
                        target_id = mid
                        break
            
            if target_monitor:
                # è®¡ç®—æ‰§è¡Œæ—¶é—´
                execution_time = end_time - target_monitor['start_time']
                
                # æ›´æ–°ç›‘æ§çŠ¶æ€
                target_monitor['end_time'] = end_time
                target_monitor['execution_time'] = execution_time
                target_monitor['status'] = 'completed'
                
                # è®°å½•ç›‘æ§ç»“æœ
                self.monitor_flow_execution(
                    flow_type='sync',
                    operation=operation_name,
                    start_time=target_monitor['start_time'],
                    end_time=end_time,
                    success=True
                )
                
                # æ¸…ç†ç›‘æ§ä¼šè¯
                del self._active_monitors[target_id]
                
                result = {
                    'operation': operation_name,
                    'execution_time_ms': round(execution_time * 1000, 2),
                    'status': 'success'
                }
                
                self.logger.debug(f"ğŸ“Š ç»“æŸç›‘æ§: {operation_name} ({result['execution_time_ms']}ms)")
                return result
            else:
                self.logger.warning(f"æœªæ‰¾åˆ°å¯¹åº”çš„ç›‘æ§ä¼šè¯: {operation_name}")
                return {'operation': operation_name, 'status': 'not_found'}
                
        except Exception as e:
            self.logger.error(f"ç»“æŸç›‘æ§å¤±è´¥: {e}")
            return {'operation': operation_name, 'status': 'error', 'error': str(e)}

    def get_real_time_metrics(self) -> Dict[str, Any]:
        """è·å–å®æ—¶æ€§èƒ½æŒ‡æ ‡"""
        try:
            return {
                'timestamp': time.time(),
                'cache_performance': self._get_cache_metrics(),
                'database_performance': self._get_database_metrics(),
                'queue_status': self._get_queue_metrics(),
                'memory_usage': self._get_memory_metrics()
            }
            
        except Exception as e:
            self.logger.error(f"è·å–å®æ—¶æŒ‡æ ‡å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _get_cache_metrics(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜æ€§èƒ½æŒ‡æ ‡"""
        if self.unified_cache:
            return self.unified_cache.get_performance_metrics()
        return {'status': 'unavailable'}
    
    def _get_database_metrics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡"""
        return self.system_stats.get_database_performance()
    
    def _get_queue_metrics(self) -> Dict[str, Any]:
        """è·å–é˜Ÿåˆ—çŠ¶æ€æŒ‡æ ‡"""
        if self.async_flow_manager:
            return self.async_flow_manager.get_queue_status()
        return {'status': 'unavailable'}
    
    def _get_memory_metrics(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ä½¿ç”¨æŒ‡æ ‡"""
        return self.system_stats.get_memory_performance()