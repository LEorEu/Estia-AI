#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æƒ…æ„Ÿåˆ†æå™¨
ä½¿ç”¨ä¸“ä¸šæƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆå¦‚goemotionsï¼‰æ›¿ä»£å…³é”®è¯åŒ¹é…
"""

import logging
from typing import Dict, Any, List, Optional, Tuple

logger = logging.getLogger(__name__)

class EmotionAnalyzer:
    """ä¸“ä¸šæƒ…æ„Ÿåˆ†æå™¨"""
    
    def __init__(self, model_name: str = "goemotions", use_transformers: bool = True):
        """
        åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            use_transformers: æ˜¯å¦ä½¿ç”¨transformersåº“
        """
        self.model_name = model_name
        self.use_transformers = use_transformers
        self.logger = logger
        
        # æƒ…æ„Ÿé…ç½®
        self.emotion_config = {
            'confidence_threshold': 0.5,  # ç½®ä¿¡åº¦é˜ˆå€¼
            'max_text_length': 512,       # æœ€å¤§æ–‡æœ¬é•¿åº¦
            'batch_size': 8,              # æ‰¹å¤„ç†å¤§å°
            'use_cache': True,            # æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        }
        
        # æƒ…æ„Ÿæ ‡ç­¾æ˜ å°„
        self.emotion_mapping = {
            # GoEmotions 27ç§æƒ…æ„Ÿ -> ç®€åŒ–çš„6ç§æƒ…æ„Ÿ
            'admiration': 'positive',
            'amusement': 'positive', 
            'approval': 'positive',
            'caring': 'positive',
            'excitement': 'positive',
            'gratitude': 'positive',
            'joy': 'positive',
            'love': 'positive',
            'optimism': 'positive',
            'pride': 'positive',
            'relief': 'positive',
            
            'anger': 'negative',
            'annoyance': 'negative',
            'disappointment': 'negative',
            'disapproval': 'negative',
            'disgust': 'negative',
            'embarrassment': 'negative',
            'grief': 'negative',
            'remorse': 'negative',
            'sadness': 'negative',
            
            'fear': 'anxious',
            'nervousness': 'anxious',
            
            'confusion': 'confused',
            'curiosity': 'curious',
            'desire': 'desire',
            'realization': 'realization',
            'surprise': 'surprise',
            'neutral': 'neutral'
        }
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = None
        self.tokenizer = None
        self._initialize_model()
    
    def _initialize_model(self):
        """åˆå§‹åŒ–æƒ…æ„Ÿåˆ†ææ¨¡å‹"""
        try:
            if self.use_transformers:
                self._initialize_transformers_model()
            else:
                self._initialize_fallback_model()
                
        except Exception as e:
            self.logger.warning(f"æƒ…æ„Ÿåˆ†ææ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
            self._initialize_fallback_model()
    
    def _initialize_transformers_model(self):
        """åˆå§‹åŒ–transformersæ¨¡å‹"""
        try:
            from transformers import AutoTokenizer, AutoModelForSequenceClassification
            import torch
            
            # ä½¿ç”¨GoEmotionsæ¨¡å‹
            model_path = "j-hartmann/emotion-english-distilroberta-base"
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            self.logger.info(f"âœ… æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            
        except ImportError:
            self.logger.warning("transformersåº“æœªå®‰è£…ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
            raise
        except Exception as e:
            self.logger.error(f"transformersæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _initialize_fallback_model(self):
        """åˆå§‹åŒ–é™çº§æ¨¡å‹ï¼ˆåŸºäºè§„åˆ™ï¼‰"""
        self.model = "rule_based"
        self.tokenizer = None
        self.logger.info("ğŸ”„ ä½¿ç”¨åŸºäºè§„åˆ™çš„æƒ…æ„Ÿåˆ†æé™çº§æ–¹æ¡ˆ")
    
    def analyze_emotion(self, text: str) -> Dict[str, Any]:
        """
        åˆ†æå•ä¸ªæ–‡æœ¬çš„æƒ…æ„Ÿ
        
        Args:
            text: å¾…åˆ†æçš„æ–‡æœ¬
            
        Returns:
            Dict: æƒ…æ„Ÿåˆ†æç»“æœ
        """
        try:
            if not text or len(text.strip()) == 0:
                return self._get_neutral_emotion()
            
            # æ–‡æœ¬é¢„å¤„ç†
            processed_text = self._preprocess_text(text)
            
            if self.model and self.model != "rule_based":
                return self._analyze_with_model(processed_text)
            else:
                return self._analyze_with_rules(processed_text)
                
        except Exception as e:
            self.logger.error(f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            return self._get_error_emotion(str(e))
    
    def analyze_emotions_batch(self, texts: List[str]) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡åˆ†ææƒ…æ„Ÿ
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            List: æƒ…æ„Ÿåˆ†æç»“æœåˆ—è¡¨
        """
        try:
            if not texts:
                return []
            
            # é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬
            processed_texts = [self._preprocess_text(text) for text in texts]
            
            if self.model and self.model != "rule_based":
                return self._analyze_batch_with_model(processed_texts)
            else:
                return [self._analyze_with_rules(text) for text in processed_texts]
                
        except Exception as e:
            self.logger.error(f"æ‰¹é‡æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            return [self._get_error_emotion(str(e)) for _ in texts]
    
    def _analyze_with_model(self, text: str) -> Dict[str, Any]:
        """ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹åˆ†ææƒ…æ„Ÿ"""
        try:
            import torch
            from torch.nn.functional import softmax
            
            # ç¼–ç æ–‡æœ¬
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                max_length=self.emotion_config['max_text_length'],
                padding=True
            )
            
            # æ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs)
                predictions = softmax(outputs.logits, dim=-1)
            
            # è·å–é¢„æµ‹ç»“æœ
            emotion_scores = predictions[0].tolist()
            
            # è·å–æ ‡ç­¾ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´ï¼‰
            emotion_labels = [
                'anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise'
            ]  # è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œå®é™…æ ‡ç­¾éœ€è¦æ ¹æ®æ¨¡å‹ç¡®å®š
            
            # æ„å»ºç»“æœ
            emotions = {}
            for label, score in zip(emotion_labels, emotion_scores):
                emotions[label] = round(score, 4)
            
            # æ‰¾åˆ°æœ€é«˜åˆ†çš„æƒ…æ„Ÿ
            max_emotion = max(emotions.items(), key=lambda x: x[1])
            primary_emotion = max_emotion[0]
            confidence = max_emotion[1]
            
            # æ˜ å°„åˆ°ç®€åŒ–çš„æƒ…æ„Ÿç±»åˆ«
            simplified_emotion = self.emotion_mapping.get(primary_emotion, 'neutral')
            
            return {
                'primary_emotion': primary_emotion,
                'simplified_emotion': simplified_emotion,
                'confidence': confidence,
                'all_emotions': emotions,
                'method': 'transformer_model',
                'model_name': self.model_name
            }
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            return self._analyze_with_rules(text)
    
    def _analyze_batch_with_model(self, texts: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡ä½¿ç”¨æ¨¡å‹åˆ†æ"""
        try:
            import torch
            from torch.nn.functional import softmax
            
            # æ‰¹é‡ç¼–ç 
            inputs = self.tokenizer(
                texts,
                return_tensors="pt",
                truncation=True,
                max_length=self.emotion_config['max_text_length'],
                padding=True
            )
            
            # æ‰¹é‡æ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs)
                predictions = softmax(outputs.logits, dim=-1)
            
            # å¤„ç†ç»“æœ
            results = []
            emotion_labels = [
                'anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise'
            ]
            
            for i, text in enumerate(texts):
                emotion_scores = predictions[i].tolist()
                
                emotions = {}
                for label, score in zip(emotion_labels, emotion_scores):
                    emotions[label] = round(score, 4)
                
                max_emotion = max(emotions.items(), key=lambda x: x[1])
                primary_emotion = max_emotion[0]
                confidence = max_emotion[1]
                simplified_emotion = self.emotion_mapping.get(primary_emotion, 'neutral')
                
                results.append({
                    'primary_emotion': primary_emotion,
                    'simplified_emotion': simplified_emotion,
                    'confidence': confidence,
                    'all_emotions': emotions,
                    'method': 'transformer_model_batch',
                    'model_name': self.model_name
                })
            
            return results
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡æ¨¡å‹åˆ†æå¤±è´¥: {e}")
            return [self._analyze_with_rules(text) for text in texts]
    
    def _analyze_with_rules(self, text: str) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„æƒ…æ„Ÿåˆ†æï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        try:
            text_lower = text.lower()
            
            # æƒ…æ„Ÿå…³é”®è¯è¯å…¸
            emotion_keywords = {
                'positive': [
                    'å¼€å¿ƒ', 'é«˜å…´', 'å…´å¥‹', 'æ„‰å¿«', 'å¿«ä¹', 'æ»¡æ„', 'æ£’', 'å¥½', 'èµ', 'å–œæ¬¢',
                    'çˆ±', 'æ„Ÿè°¢', 'è°¢è°¢', 'ä¼˜ç§€', 'å®Œç¾', 'æˆåŠŸ', 'èƒœåˆ©', 'å¹¸ç¦', 'ç¾å¥½'
                ],
                'negative': [
                    'éš¾è¿‡', 'æ²®ä¸§', 'çƒ¦æ¼', 'éƒé—·', 'å¤±æœ›', 'æ„¤æ€’', 'ç”Ÿæ°”', 'æ¼ç«', 'è®¨åŒ',
                    'ç³Ÿç³•', 'å', 'å·®', 'å¤±è´¥', 'ç—›è‹¦', 'æ‚²ä¼¤', 'ä¸æ»¡', 'æŠ±æ€¨'
                ],
                'anxious': [
                    'ç„¦è™‘', 'æ‹…å¿ƒ', 'ç´§å¼ ', 'å®³æ€•', 'ææƒ§', 'ä¸å®‰', 'å¿§è™‘', 'æƒŠæ…Œ', 'ææ…Œ'
                ],
                'surprise': [
                    'æƒŠè®¶', 'æ„å¤–', 'éœ‡æƒŠ', 'ä¸æ•¢ç›¸ä¿¡', 'æ²¡æƒ³åˆ°', 'ç«Ÿç„¶', 'å±…ç„¶'
                ],
                'confused': [
                    'å›°æƒ‘', 'è¿·æƒ‘', 'ä¸æ‡‚', 'ä¸æ˜ç™½', 'æä¸æ¸…', 'ç–‘æƒ‘', 'çº³é—·'
                ]
            }
            
            # è®¡ç®—å„æƒ…æ„Ÿçš„åŒ¹é…åˆ†æ•°
            scores = {}
            for emotion, keywords in emotion_keywords.items():
                score = sum(1 for keyword in keywords if keyword in text_lower)
                scores[emotion] = score / len(keywords)  # å½’ä¸€åŒ–
            
            # æ·»åŠ ä¸­æ€§æƒ…æ„Ÿ
            scores['neutral'] = 0.5 if all(score == 0 for score in scores.values()) else 0.1
            
            # æ‰¾åˆ°æœ€é«˜åˆ†çš„æƒ…æ„Ÿ
            if scores:
                max_emotion = max(scores.items(), key=lambda x: x[1])
                primary_emotion = max_emotion[0]
                confidence = min(max_emotion[1] * 2, 1.0)  # è°ƒæ•´ç½®ä¿¡åº¦
            else:
                primary_emotion = 'neutral'
                confidence = 0.5
            
            # å¦‚æœç½®ä¿¡åº¦å¤ªä½ï¼Œè®¾ä¸ºä¸­æ€§
            if confidence < self.emotion_config['confidence_threshold']:
                primary_emotion = 'neutral'
                confidence = 0.6
            
            return {
                'primary_emotion': primary_emotion,
                'simplified_emotion': primary_emotion,
                'confidence': round(confidence, 3),
                'all_emotions': {k: round(v, 3) for k, v in scores.items()},
                'method': 'rule_based',
                'model_name': 'keyword_matching'
            }
            
        except Exception as e:
            self.logger.error(f"è§„åˆ™æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            return self._get_neutral_emotion()
    
    def _preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        try:
            # åŸºæœ¬æ¸…ç†
            text = text.strip()
            
            # é™åˆ¶é•¿åº¦
            if len(text) > self.emotion_config['max_text_length']:
                text = text[:self.emotion_config['max_text_length']]
            
            return text
            
        except Exception as e:
            self.logger.error(f"æ–‡æœ¬é¢„å¤„ç†å¤±è´¥: {e}")
            return text
    
    def _get_neutral_emotion(self) -> Dict[str, Any]:
        """è·å–ä¸­æ€§æƒ…æ„Ÿç»“æœ"""
        return {
            'primary_emotion': 'neutral',
            'simplified_emotion': 'neutral',
            'confidence': 0.6,
            'all_emotions': {'neutral': 1.0},
            'method': 'default',
            'model_name': 'fallback'
        }
    
    def _get_error_emotion(self, error_msg: str) -> Dict[str, Any]:
        """è·å–é”™è¯¯æƒ…æ„Ÿç»“æœ"""
        return {
            'primary_emotion': 'neutral',
            'simplified_emotion': 'neutral',
            'confidence': 0.0,
            'all_emotions': {'neutral': 1.0},
            'method': 'error',
            'model_name': 'fallback',
            'error': error_msg
        }
    
    def analyze_emotion_pattern(self, emotion_history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åˆ†ææƒ…æ„Ÿæ¨¡å¼
        
        Args:
            emotion_history: å†å²æƒ…æ„Ÿåˆ†æç»“æœåˆ—è¡¨
            
        Returns:
            Dict: æƒ…æ„Ÿæ¨¡å¼åˆ†æ
        """
        try:
            if not emotion_history:
                return {'pattern': 'no_data', 'description': 'æ²¡æœ‰è¶³å¤Ÿçš„æƒ…æ„Ÿæ•°æ®'}
            
            # ç»Ÿè®¡å„ç§æƒ…æ„Ÿçš„é¢‘ç‡
            emotion_counts = {}
            total_confidence = 0
            
            for emotion_result in emotion_history:
                primary = emotion_result.get('simplified_emotion', 'neutral')
                confidence = emotion_result.get('confidence', 0)
                
                emotion_counts[primary] = emotion_counts.get(primary, 0) + 1
                total_confidence += confidence
            
            total_count = len(emotion_history)
            avg_confidence = total_confidence / total_count if total_count > 0 else 0
            
            # è®¡ç®—æƒ…æ„Ÿåˆ†å¸ƒ
            emotion_distribution = {
                emotion: count / total_count 
                for emotion, count in emotion_counts.items()
            }
            
            # ç¡®å®šä¸»è¦æƒ…æ„Ÿæ¨¡å¼
            dominant_emotion = max(emotion_distribution.items(), key=lambda x: x[1])
            dominant_emotion_name = dominant_emotion[0]
            dominant_ratio = dominant_emotion[1]
            
            # åˆ†ææ¨¡å¼ç±»å‹
            if dominant_ratio > 0.7:
                pattern = 'consistent'  # æƒ…æ„Ÿä¸€è‡´
                description = f'æƒ…æ„Ÿè¾ƒä¸ºä¸€è‡´ï¼Œä¸»è¦è¡¨ç°ä¸º{dominant_emotion_name}'
            elif len(emotion_counts) >= 4:
                pattern = 'diverse'  # æƒ…æ„Ÿå¤šæ ·
                description = 'æƒ…æ„Ÿè¡¨è¾¾ä¸°å¯Œå¤šæ ·'
            elif 'positive' in emotion_counts and 'negative' in emotion_counts:
                pattern = 'mixed'  # æƒ…æ„Ÿæ··åˆ
                description = 'æ­£è´Ÿæƒ…æ„Ÿäº¤æ›¿å‡ºç°'
            else:
                pattern = 'stable'  # æƒ…æ„Ÿç¨³å®š
                description = 'æƒ…æ„ŸçŠ¶æ€ç›¸å¯¹ç¨³å®š'
            
            return {
                'pattern': pattern,
                'description': description,
                'dominant_emotion': dominant_emotion_name,
                'dominant_ratio': round(dominant_ratio, 3),
                'emotion_distribution': emotion_distribution,
                'avg_confidence': round(avg_confidence, 3),
                'sample_count': total_count
            }
            
        except Exception as e:
            self.logger.error(f"æƒ…æ„Ÿæ¨¡å¼åˆ†æå¤±è´¥: {e}")
            return {
                'pattern': 'error',
                'description': f'åˆ†æå¤±è´¥: {str(e)}',
                'sample_count': len(emotion_history) if emotion_history else 0
            }
    
    def get_emotion_insights(self, text: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        è·å–æƒ…æ„Ÿæ´å¯Ÿï¼ˆåŒ…å«æƒ…æ„Ÿåˆ†æå’Œä¸Šä¸‹æ–‡ç†è§£ï¼‰
        
        Args:
            text: å¾…åˆ†ææ–‡æœ¬
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            Dict: æƒ…æ„Ÿæ´å¯Ÿç»“æœ
        """
        try:
            # åŸºç¡€æƒ…æ„Ÿåˆ†æ
            emotion_result = self.analyze_emotion(text)
            
            # æ·»åŠ ä¸Šä¸‹æ–‡æ´å¯Ÿ
            insights = {
                'emotion_analysis': emotion_result,
                'context_insights': {},
                'recommendations': []
            }
            
            if context:
                # åˆ†æä¸Šä¸‹æ–‡ä¸­çš„æƒ…æ„Ÿå˜åŒ–
                if 'previous_emotions' in context:
                    pattern_analysis = self.analyze_emotion_pattern(context['previous_emotions'])
                    insights['context_insights']['emotion_pattern'] = pattern_analysis
                
                # åŸºäºæƒ…æ„Ÿç»™å‡ºå»ºè®®
                primary_emotion = emotion_result['simplified_emotion']
                confidence = emotion_result['confidence']
                
                if primary_emotion == 'negative' and confidence > 0.7:
                    insights['recommendations'].append('æ£€æµ‹åˆ°è¾ƒå¼ºçš„è´Ÿé¢æƒ…æ„Ÿï¼Œå»ºè®®å…³æ³¨ç”¨æˆ·æƒ…ç»ª')
                elif primary_emotion == 'positive' and confidence > 0.7:
                    insights['recommendations'].append('ç”¨æˆ·æƒ…ç»ªç§¯æï¼Œå¯ä»¥ç»§ç»­å½“å‰çš„äº¤æµæ–¹å¼')
                elif primary_emotion == 'anxious' and confidence > 0.6:
                    insights['recommendations'].append('ç”¨æˆ·å¯èƒ½æ„Ÿåˆ°ç„¦è™‘ï¼Œå»ºè®®æä¾›å®‰æ…°å’Œæ”¯æŒ')
            
            return insights
            
        except Exception as e:
            self.logger.error(f"æƒ…æ„Ÿæ´å¯Ÿåˆ†æå¤±è´¥: {e}")
            return {
                'emotion_analysis': self._get_error_emotion(str(e)),
                'context_insights': {},
                'recommendations': ['æƒ…æ„Ÿåˆ†æå‡ºç°é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥ç³»ç»ŸçŠ¶æ€']
            } 